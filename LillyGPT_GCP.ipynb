{
  "cells": [
    {
      "cell_type": "code",
      "id": "VZN1p58g74zXckDiw4ZifVZL",
      "metadata": {
        "tags": [],
        "id": "VZN1p58g74zXckDiw4ZifVZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1733737849216,
          "user_tz": 480,
          "elapsed": 26856,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "f560b2ae-ab7f-4a1a-c2cb-1afd7e1b446c"
      },
      "source": [
        "!pip install openai gradio pandas numpy vaderSentiment textblob PyMuPDF transformers sentence-transformers faiss-cpu rouge bert-score nltk spacy torch gtts SpeechRecognition pydub\n",
        "!pip install openai==0.28"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.8.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.12.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.5.1 (from gradio)\n",
            "  Downloading gradio_client-1.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.1->gradio) (2024.10.0)\n",
            "Collecting websockets<15.0,>=10.0 (from gradio-client==1.5.1->gradio)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.8.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (69.5.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.8.0-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.2/320.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.25.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading SpeechRecognition-3.12.0-py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, SpeechRecognition, semantic-version, ruff, rouge, python-multipart, PyMuPDF, markupsafe, ffmpy, faiss-cpu, aiofiles, vaderSentiment, starlette, gtts, safehttpx, gradio-client, fastapi, gradio, bert-score\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed PyMuPDF-1.25.0 SpeechRecognition-3.12.0 aiofiles-23.2.1 bert-score-0.3.13 faiss-cpu-1.9.0.post1 fastapi-0.115.6 ffmpy-0.4.0 gradio-5.8.0 gradio-client-1.5.1 gtts-2.5.4 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.19 rouge-1.0.1 ruff-0.8.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.32.1 vaderSentiment-3.3.2 websockets-14.1\n",
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.4\n",
            "    Uninstalling openai-1.54.4:\n",
            "      Successfully uninstalled openai-1.54.4\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mauve-text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ck7UP6x8fDD",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1733737853171,
          "user_tz": 480,
          "elapsed": 3957,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ff68fce5-9e03-4cf9-cdde-905938e15030"
      },
      "id": "3Ck7UP6x8fDD",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mauve-text\n",
            "  Downloading mauve_text-0.4.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from mauve-text) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from mauve-text) (1.5.2)\n",
            "Requirement already satisfied: faiss-cpu>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from mauve-text) (1.9.0.post1)\n",
            "Requirement already satisfied: tqdm>=4.40.0 in /usr/local/lib/python3.10/dist-packages (from mauve-text) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from mauve-text) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu>=1.7.0->mauve-text) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->mauve-text) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->mauve-text) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->mauve-text) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->mauve-text) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->mauve-text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->mauve-text) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->mauve-text) (2024.8.30)\n",
            "Downloading mauve_text-0.4.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: mauve-text\n",
            "Successfully installed mauve-text-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-bigquery google-cloud-storage\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L22LbImOqwJS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1733737856818,
          "user_tz": 480,
          "elapsed": 3649,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "6da7658c-bd59-4838-e4f8-6ab233051568"
      },
      "id": "L22LbImOqwJS",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (3.25.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.7.2)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (24.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.32.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.25.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.68.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2024.8.30)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install google-cloud-speech google-cloud-texttospeech pydub\n",
        "# !pip install google-cloud-texttospeech pydub SpeechRecognition\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u0Q791vXYr4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1733732917294,
          "user_tz": 480,
          "elapsed": 8565,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "17517158-7b74-4837-e8c9-1622745ea7a1"
      },
      "id": "0u0Q791vXYr4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-speech in /usr/local/lib/python3.10/dist-packages (2.28.1)\n",
            "Requirement already satisfied: google-cloud-texttospeech in /usr/local/lib/python3.10/dist-packages (2.21.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (2.19.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-speech) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-speech) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-speech) (4.25.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (1.68.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-speech) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-speech) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-speech) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-speech) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (2024.8.30)\n",
            "Requirement already satisfied: google-cloud-texttospeech in /usr/local/lib/python3.10/dist-packages (2.21.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.12.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2.19.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-texttospeech) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-texttospeech) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-texttospeech) (4.25.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (1.68.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install vosk TTS soundfile\n",
        "# !wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
        "# !unzip vosk-model-small-en-us-0.15.zip -d model\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R1jpZ0A5pYnG",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1733734930384,
          "user_tz": 480,
          "elapsed": 80656,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "0bdff00c-2239-4098-c53c-c01b6d0f1d35"
      },
      "id": "R1jpZ0A5pYnG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vosk\n",
            "  Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting TTS\n",
            "  Downloading TTS-0.22.0-cp310-cp310-manylinux1_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from vosk) (1.17.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vosk) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from vosk) (4.66.6)\n",
            "Collecting srt (from vosk)\n",
            "  Downloading srt-3.5.3.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from vosk) (14.1)\n",
            "Requirement already satisfied: cython>=0.29.30 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.0.11)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.13.1)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from TTS) (2.5.1+cu121)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.10.2.post1)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.5.2)\n",
            "Requirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (7.4.0)\n",
            "Collecting anyascii>=0.3.0 (from TTS)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.11.2)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (24.2)\n",
            "Requirement already satisfied: flask>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.0.3)\n",
            "Collecting pysbd>=0.3.4 (from TTS)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting umap-learn>=0.5.1 (from TTS)\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pandas<2.0,>=1.4 (from TTS)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.8.0)\n",
            "Collecting trainer>=0.0.32 (from TTS)\n",
            "  Downloading trainer-0.0.36-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting coqpit>=0.0.16 (from TTS)\n",
            "  Downloading coqpit-0.0.17-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from TTS) (0.42.1)\n",
            "Collecting pypinyin (from TTS)\n",
            "  Downloading pypinyin-0.53.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hangul-romanize (from TTS)\n",
            "  Downloading hangul_romanize-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting gruut==2.2.3 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jamo (from TTS)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from TTS) (3.9.1)\n",
            "Collecting g2pkk>=0.1.1 (from TTS)\n",
            "  Downloading g2pkk-0.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting bangla (from TTS)\n",
            "  Downloading bangla-0.0.2-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting bnnumerizer (from TTS)\n",
            "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bnunicodenormalizer (from TTS)\n",
            "  Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.8.0)\n",
            "Requirement already satisfied: transformers>=4.33.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (4.46.2)\n",
            "Collecting encodec>=0.1.1 (from TTS)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode>=1.3.2 (from TTS)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting num2words (from TTS)\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: spacy>=3 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS) (3.7.5)\n",
            "Collecting numpy==1.22.0 (from TTS)\n",
            "  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.60.0)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.16.0)\n",
            "Collecting dateparser~=1.1.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\n",
            "Collecting gruut-ipa<1.0,>=0.12.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_en-2.0.1.tar.gz (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting networkx<3.0.0,>=2.5.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting python-crfsuite~=0.9.7 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_es-2.0.1.tar.gz (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_de-2.0.1.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (4.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->vosk) (2.22)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (1.9.0)\n",
            "Requirement already satisfied: more-itertools>=8.5.0 in /usr/local/lib/python3.10/dist-packages (from inflect>=5.6.0->TTS) (10.5.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from inflect>=5.6.0->TTS) (4.4.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (3.0.1)\n",
            "INFO: pip is looking at multiple versions of librosa to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting librosa>=0.10.0 (from TTS)\n",
            "  Downloading librosa-0.10.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading librosa-0.10.0.post2-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading librosa-0.10.0.post1-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading librosa-0.10.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (2.8.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->TTS)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57.0->TTS) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0,>=1.4->TTS) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->TTS) (3.5.0)\n",
            "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scipy>=1.11.2 (from TTS)\n",
            "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.13.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.9.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (69.5.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vosk) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vosk) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vosk) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vosk) (2024.8.30)\n",
            "Collecting sudachipy!=0.6.1,>=0.5.2 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiPy-0.6.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting sudachidict-core>=20211220 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiDict_core-20241021-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (3.16.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1->TTS) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from trainer>=0.0.32->TTS) (5.9.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from trainer>=0.0.32->TTS) (2.17.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (0.26.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.0->TTS) (0.20.3)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.1->TTS)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "INFO: pip is looking at multiple versions of contourpy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->TTS)\n",
            "  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask>=2.0.1->TTS) (2.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines~=1.2.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.16.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.10.0->TTS) (4.3.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (2.23.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (7.0.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.68.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (4.25.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (0.7.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (0.1.2)\n",
            "Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading TTS-0.22.0-cp310-cp310-manylinux1_x86_64.whl (938 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
            "Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
            "Downloading librosa-0.10.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trainer-0.0.36-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
            "Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl (23 kB)\n",
            "Downloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading pypinyin-0.53.0-py2.py3-none-any.whl (834 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiDict_core-20241021-py3-none-any.whl (72.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiPy-0.6.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gruut, encodec, bnnumerizer, srt, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75788 sha256=f133b8086339e8a78ecc77b4658be2822ad993c9f129b00faf545573a1a069bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/57/a8/f9de532daf5214f53644f20f3a9e6f69269453c87df9c0a817\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45760 sha256=ed95e724a6c1ac8b66c606023768c44318297ae05b03abf8300bf38d554824ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5260 sha256=713196de832e066108ede28b369f897ed0c34cd4506fa4f8e960532dd71fc59d\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/6b/e8/223172e7d5c9f72df3ea1a0d9258f3a8ab5b28e827728edef5\n",
            "  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for srt: filename=srt-3.5.3-py3-none-any.whl size=22429 sha256=7f38373b164fbab8c2647153d8232824900552943f6c9872eb2116a9239711a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/31/a1/18e1e7e8bfdafd19e6803d7eb919b563dd11de380e4304e332\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=287e8c28fb2d5f88dba455d59fb8dd1ac0748651c058f5ce41d172deae8a1201\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104871 sha256=58e09fa18ddd03d8628ef578ad386ea78d5c47b5b574a076185eca18a6297a99\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/18/49/e4f500ecdf0babe757953f844e4d7cd1ea81c5503c09bfe984\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.1-py3-none-any.whl size=18498315 sha256=3217f03f6c28f8b66b2ec7718a6ee2e8ab93091755d7a837f4171ae41d599299\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/80/5f/775b357ae61d7cb68793327c7470d848715cbc60bb373af8dd\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.1-py3-none-any.whl size=15326857 sha256=5a6517c5a8f9aef5445d7492aeb08abed5d951c86fac9499edb31806f2badc0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/8d/b7/d484d224facd899ed188e00374f25dd3f19d1a3f53da6517bd\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.1-py3-none-any.whl size=32173928 sha256=02a90e794af3affb5d5120477a6045ff95f3e90c65e5130d6d8c0b2a42a4381b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/bd/96/5ddde14e8e6932a96f12c5ab5de62b619d39e2507d7daf5188\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968768 sha256=07e1d47cb27025e207ab522b434cc44a1b9e8b2f9495b063f64267e396f4fb33\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/21/be/d0436e3f1cf9bf38b9bb9b4a476399c77a1ab19f7172b45e19\n",
            "Successfully built gruut encodec bnnumerizer srt docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\n",
            "Installing collected packages: sudachipy, jamo, hangul-romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, bnunicodenormalizer, bnnumerizer, bangla, unidecode, sudachidict-core, srt, python-crfsuite, pysbd, pypinyin, numpy, num2words, networkx, jsonlines, gruut-ipa, coqpit, anyascii, vosk, scipy, pandas, g2pkk, dateparser, contourpy, trainer, gruut, pynndescent, librosa, encodec, umap-learn, TTS\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.1\n",
            "    Uninstalling contourpy-1.3.1:\n",
            "      Successfully uninstalled contourpy-1.3.1\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.2.post1\n",
            "    Uninstalling librosa-0.10.2.post1:\n",
            "      Successfully uninstalled librosa-0.10.2.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\n",
            "arviz 0.20.0 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
            "astropy 6.1.6 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "bigframes 1.27.0 requires numpy>=1.24.0, but you have numpy 1.22.0 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "faiss-cpu 1.9.0.post1 requires numpy<3.0,>=1.25.0, but you have numpy 1.22.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.22.0 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
            "mizani 0.13.0 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "numexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.14.1 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.14.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pylibraft-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "rmm-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "scikit-image 0.24.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "statsmodels 0.14.4 requires numpy<3,>=1.22.3, but you have numpy 1.22.0 which is incompatible.\n",
            "tensorflow 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.22.0 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
            "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray-einstats 0.8.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed TTS-0.22.0 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.7 contourpy-1.2.1 coqpit-0.0.17 dateparser-1.1.8 docopt-0.6.2 encodec-0.1.1 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.1 gruut_lang_en-2.0.1 gruut_lang_es-2.0.1 gruut_lang_fr-2.0.2 hangul-romanize-0.1.0 jamo-0.4.1 jsonlines-1.2.0 librosa-0.10.0 networkx-2.8.8 num2words-0.5.13 numpy-1.22.0 pandas-1.5.3 pynndescent-0.5.13 pypinyin-0.53.0 pysbd-0.3.4 python-crfsuite-0.9.11 scipy-1.11.4 srt-3.5.3 sudachidict-core-20241021 sudachipy-0.6.9 trainer-0.0.36 umap-learn-0.5.7 unidecode-1.3.8 vosk-0.3.45\n",
            "^C\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              },
              "id": "72581a59ab8a4f4399ff905db0cbdd3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-09 09:02:06--  https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
            "Resolving alphacephei.com (alphacephei.com)... 188.40.21.16, 2a01:4f8:13a:279f::2\n",
            "Connecting to alphacephei.com (alphacephei.com)|188.40.21.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41205931 (39M) [application/zip]\n",
            "Saving to: ‘vosk-model-small-en-us-0.15.zip’\n",
            "\n",
            "vosk-model-small-en 100%[===================>]  39.30M  17.5MB/s    in 2.2s    \n",
            "\n",
            "2024-12-09 09:02:09 (17.5 MB/s) - ‘vosk-model-small-en-us-0.15.zip’ saved [41205931/41205931]\n",
            "\n",
            "Archive:  vosk-model-small-en-us-0.15.zip\n",
            "   creating: model/vosk-model-small-en-us-0.15/\n",
            "   creating: model/vosk-model-small-en-us-0.15/am/\n",
            "  inflating: model/vosk-model-small-en-us-0.15/am/final.mdl  \n",
            "   creating: model/vosk-model-small-en-us-0.15/graph/\n",
            "  inflating: model/vosk-model-small-en-us-0.15/graph/disambig_tid.int  \n",
            "  inflating: model/vosk-model-small-en-us-0.15/graph/HCLr.fst  \n",
            "  inflating: model/vosk-model-small-en-us-0.15/graph/Gr.fst  \n",
            "   creating: model/vosk-model-small-en-us-0.15/graph/phones/\n",
            "  inflating: model/vosk-model-small-en-us-0.15/graph/phones/word_boundary.int  \n",
            "   creating: model/vosk-model-small-en-us-0.15/conf/\n",
            "  inflating: model/vosk-model-small-en-us-0.15/conf/model.conf  \n",
            "  inflating: model/vosk-model-small-en-us-0.15/conf/mfcc.conf  \n",
            "   creating: model/vosk-model-small-en-us-0.15/ivector/\n",
            "  inflating: model/vosk-model-small-en-us-0.15/ivector/splice.conf  \n",
            "  inflating: model/vosk-model-small-en-us-0.15/ivector/final.dubm  \n",
            "  inflating: model/vosk-model-small-en-us-0.15/ivector/global_cmvn.stats  \n",
            "  inflating: model/vosk-model-small-en-us-0.15/ivector/final.ie  \n",
            "  inflating: model/vosk-model-small-en-us-0.15/ivector/online_cmvn.conf  \n",
            "  inflating: model/vosk-model-small-en-us-0.15/ivector/final.mat  \n",
            "  inflating: model/vosk-model-small-en-us-0.15/README  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import gradio as gr\n",
        "import random\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "import fitz  # PyMuPDF for working with PDFs\n",
        "from transformers import pipeline\n",
        "from transformers import RobertaTokenizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import faiss\n",
        "import os\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import re  # For detecting and extracting salary information\n",
        "# Metrics\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "from rouge import Rouge\n",
        "from bert_score import score as bert_score\n",
        "from nltk.translate import meteor_score\n",
        "from nltk.tokenize import word_tokenize  # Import for tokenizing\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import spacy\n",
        "import mauve\n",
        "import torch\n",
        "\n",
        "from gtts import gTTS\n",
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "from pathlib import Path\n",
        "####Natural Voice\n",
        "\n",
        "# Set OpenAI API key\n",
        "openai.api_key = \"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\"  # Replace with your actual OpenAI API key\n",
        "\n",
        "# Convert audio to text using OpenAI Whisper\n",
        "def audio_to_text(audio_file):\n",
        "    \"\"\"\n",
        "    Convert an audio file to text using OpenAI Whisper.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(audio_file, \"rb\") as f:\n",
        "            response = openai.Audio.transcribe(\"whisper-1\", f)\n",
        "        text = response.get(\"text\", \"\")\n",
        "        print(f\"DEBUG: Converted audio to text: {text}\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to process audio input with Whisper: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Convert text to audio using OpenAI TTS\n",
        "def text_to_audio_openai(input_text, model=\"tts-1-hd\", voice=\"alloy\", output_filename=\"response_audio.mp3\"):\n",
        "\n",
        "    try:\n",
        "        print(f\"DEBUG: Converting text to speech using model '{model}' and voice '{voice}'...\")\n",
        "\n",
        "        # Define the path to save the output audio file\n",
        "        output_path = Path(output_filename)\n",
        "\n",
        "        # Call the OpenAI TTS API\n",
        "        response = openai.Audio.speech.create(\n",
        "            model=model,\n",
        "            voice=voice,\n",
        "            input=input_text\n",
        "        )\n",
        "\n",
        "        # Save the response audio stream to the file\n",
        "        response.stream_to_file(output_path)\n",
        "\n",
        "        print(f\"DEBUG: Audio saved to {output_path}\")\n",
        "        return str(output_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to generate audio: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from google.cloud import texttospeech\n",
        "# from pydub import AudioSegment\n",
        "# import speech_recognition as sr\n",
        "# import os\n",
        "#https://github.com/coqui-ai/TTS\n",
        "# from pydub import AudioSegment\n",
        "# import speech_recognition as sr\n",
        "# from TTS.api import TTS\n",
        "# import os\n",
        "\n",
        "# # Function to convert audio to text\n",
        "# def audio_to_text(audio_file):\n",
        "#     \"\"\"\n",
        "#     Convert an audio file to text using SpeechRecognition.\n",
        "#     \"\"\"\n",
        "#     recognizer = sr.Recognizer()\n",
        "#     try:\n",
        "#         # Convert audio file to WAV format (if necessary)\n",
        "#         audio = AudioSegment.from_file(audio_file)\n",
        "#         audio.export(\"temp_audio.wav\", format=\"wav\")\n",
        "#         with sr.AudioFile(\"temp_audio.wav\") as source:\n",
        "#             audio_data = recognizer.record(source)\n",
        "#             text = recognizer.recognize_google(audio_data)\n",
        "#             print(f\"DEBUG: Converted audio to text: {text}\")\n",
        "#             return text\n",
        "#     except Exception as e:\n",
        "#         print(f\"ERROR: Failed to process audio input: {e}\")\n",
        "#         return \"\"\n",
        "\n",
        "# # Function to convert text to audio\n",
        "# def text_to_audio(response_text, filename=\"response_audio.wav\"):\n",
        "#     \"\"\"\n",
        "#     Convert text to speech using an open-source TTS library.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         # Initialize the TTS model\n",
        "#         tts_model = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", gpu=False)\n",
        "\n",
        "#         # Remove old audio file if it exists\n",
        "#         if os.path.exists(filename):\n",
        "#             os.remove(filename)\n",
        "\n",
        "#         # Generate speech from the text\n",
        "#         tts_model.tts_to_file(text=response_text, speaker=\"female\", file_path=filename)\n",
        "#         print(f\"DEBUG: Audio content saved to file: {filename}\")\n",
        "\n",
        "#         return filename\n",
        "#     except Exception as e:\n",
        "#         print(f\"ERROR: Failed to generate audio: {e}\")\n",
        "#         return None\n",
        "\n",
        "\n",
        "####GCP SPEECH TO TEXT SYNTH IS $2,900 PER MONTH\n",
        "# def audio_to_text(audio_file):\n",
        "#     \"\"\"\n",
        "#     Convert an audio file to text using SpeechRecognition.\n",
        "#     \"\"\"\n",
        "#     recognizer = sr.Recognizer()\n",
        "#     try:\n",
        "#         # Convert audio file to WAV format (if necessary)\n",
        "#         audio = AudioSegment.from_file(audio_file)\n",
        "#         audio.export(\"temp_audio.wav\", format=\"wav\")\n",
        "#         with sr.AudioFile(\"temp_audio.wav\") as source:\n",
        "#             audio_data = recognizer.record(source)\n",
        "#             text = recognizer.recognize_google(audio_data)\n",
        "#             print(f\"DEBUG: Converted audio to text: {text}\")\n",
        "#             return text\n",
        "#     except Exception as e:\n",
        "#         print(f\"ERROR: Failed to process audio input: {e}\")\n",
        "#         return \"\"\n",
        "\n",
        "# def text_to_audio(response_text, filename=\"response_audio.mp3\"):\n",
        "#     \"\"\"\n",
        "#     Convert text to audio using Google Cloud Text-to-Speech.\n",
        "#     Saves the audio to a specified filename.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         client = texttospeech.TextToSpeechClient()\n",
        "#         input_text = texttospeech.SynthesisInput(text=response_text)\n",
        "\n",
        "#         # Select a natural-sounding female voice\n",
        "#         voice = texttospeech.VoiceSelectionParams(\n",
        "#             language_code=\"en-US\",\n",
        "#             name=\"en-US-Wavenet-F\",  # Natural female voice\n",
        "#             ssml_gender=texttospeech.SsmlVoiceGender.FEMALE,\n",
        "#         )\n",
        "\n",
        "#         # Configure audio output\n",
        "#         audio_config = texttospeech.AudioConfig(\n",
        "#             audio_encoding=texttospeech.AudioEncoding.MP3\n",
        "#         )\n",
        "\n",
        "#         # Perform text-to-speech request\n",
        "#         response = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)\n",
        "\n",
        "#         # Save the audio to a file\n",
        "#         with open(filename, \"wb\") as out:\n",
        "#             out.write(response.audio_content)\n",
        "#             print(f\"DEBUG: Audio content written to file: {filename}\")\n",
        "\n",
        "#         return filename\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"ERROR: Failed to generate audio: {e}\")\n",
        "#         return None\n",
        "\n",
        "\n",
        "##Offline VOICE MODEL\n",
        "# import os\n",
        "# from vosk import Model, KaldiRecognizer\n",
        "# import wave\n",
        "# from TTS.api import TTS  # Coqui TTS\n",
        "\n",
        "# def audio_to_text(audio_file):\n",
        "#     \"\"\"\n",
        "#     Convert an audio file to text using Vosk (offline speech recognition).\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         # Load Vosk model (ensure you have downloaded a model)\n",
        "#         model = Model(\"model\")  # Replace \"model\" with the path to your Vosk model\n",
        "#         wf = wave.open(audio_file, \"rb\")\n",
        "\n",
        "#         if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() not in [8000, 16000]:\n",
        "#             raise ValueError(\"Audio file must be WAV format with mono PCM audio and a sample rate of 8kHz or 16kHz\")\n",
        "\n",
        "#         rec = KaldiRecognizer(model, wf.getframerate())\n",
        "#         text = \"\"\n",
        "\n",
        "#         while True:\n",
        "#             data = wf.readframes(4000)\n",
        "#             if len(data) == 0:\n",
        "#                 break\n",
        "#             if rec.AcceptWaveform(data):\n",
        "#                 result = rec.Result()\n",
        "#                 text += result.get(\"text\", \"\") + \" \"\n",
        "\n",
        "#         print(f\"DEBUG: Converted audio to text: {text.strip()}\")\n",
        "#         return text.strip()\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"ERROR: Failed to process audio input: {e}\")\n",
        "#         return \"\"\n",
        "\n",
        "# def text_to_audio(response_text, filename=\"response_audio.wav\"):\n",
        "#     \"\"\"\n",
        "#     Convert text to audio using Coqui TTS for natural voice synthesis.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         # Load the TTS model\n",
        "#         tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=False, gpu=False)\n",
        "\n",
        "#         # Save the synthesized audio to a file\n",
        "#         tts.tts_to_file(text=response_text, file_path=filename)\n",
        "#         print(f\"Audio saved as {filename}\")\n",
        "#         return filename\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"ERROR: Failed to generate audio: {e}\")\n",
        "#         return None\n",
        "\n",
        "\n",
        "\n",
        "####AMR VOICE WORKING ONE\n",
        "# def audio_to_text(audio_file):\n",
        "#     \"\"\"\n",
        "#     Convert an audio file to text using SpeechRecognition.\n",
        "#     \"\"\"\n",
        "#     recognizer = sr.Recognizer()\n",
        "#     try:\n",
        "#         # Convert audio file to WAV format (if necessary)\n",
        "#         audio = AudioSegment.from_file(audio_file)\n",
        "#         audio.export(\"temp_audio.wav\", format=\"wav\")\n",
        "#         with sr.AudioFile(\"temp_audio.wav\") as source:\n",
        "#             audio_data = recognizer.record(source)\n",
        "#             text = recognizer.recognize_google(audio_data)\n",
        "#             print(f\"DEBUG: Converted audio to text: {text}\")\n",
        "#             return text\n",
        "#     except Exception as e:\n",
        "#         print(f\"ERROR: Failed to process audio input: {e}\")\n",
        "#         return \"\"\n",
        "\n",
        "\n",
        "# def text_to_audio(response_text, filename=\"response_audio.mp3\"):\n",
        "#     try:\n",
        "#         # Remove old audio file if it exists\n",
        "#         if os.path.exists(filename):\n",
        "#             os.remove(filename)\n",
        "#         # Generate speech from the text\n",
        "#         tts = gTTS(response_text, lang='en')\n",
        "#         tts.save(filename)\n",
        "#         print(f\"Audio saved as {filename}\")\n",
        "#         return filename\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error generating audio: {e}\")\n",
        "#         return None\n",
        "\n",
        "\n",
        "# Load the tokenizer for truncation\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
        "####REFERENCE TEXT GCP LOADER\n",
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "def load_reference_text(assistant_role):\n",
        "    \"\"\"\n",
        "    Load and combine the appropriate reference text file based on the assistant's role from GCS.\n",
        "\n",
        "    Args:\n",
        "        assistant_role (str): Role of the assistant ('employer' or 'employee').\n",
        "\n",
        "    Returns:\n",
        "        tuple: A list of individual lines and a single combined reference text.\n",
        "    \"\"\"\n",
        "    # Hardcoded bucket name\n",
        "    bucket_name = \"lilygpt\"\n",
        "\n",
        "    # Fallback for missing or invalid assistant_role\n",
        "    if assistant_role not in [\"employer\", \"employee\"]:\n",
        "        print(f\"WARNING: Invalid or missing assistant_role. Defaulting to 'employer'.\")\n",
        "        assistant_role = \"employer\"\n",
        "\n",
        "    # Explicit file names based on the role\n",
        "    file_mapping = {\n",
        "        \"employer\": \"reference_employer.txt\",\n",
        "        \"employee\": \"reference_employee.txt\"\n",
        "    }\n",
        "    reference_file = file_mapping[assistant_role]\n",
        "\n",
        "    # Initialize the GCS client\n",
        "    gcs_client = storage.Client()\n",
        "\n",
        "    try:\n",
        "        # Get the bucket and blob\n",
        "        bucket = gcs_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(reference_file)\n",
        "\n",
        "        # Download the text content from GCS\n",
        "        reference_content = blob.download_as_text(encoding='utf-8')\n",
        "\n",
        "        # Process the text content\n",
        "        reference_lines = [line.strip() for line in reference_content.splitlines()]\n",
        "        combined_reference_text = \" \".join(reference_lines)\n",
        "\n",
        "        print(f\"DEBUG: Successfully loaded {reference_file} from bucket {bucket_name}\")\n",
        "        return reference_lines, combined_reference_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to load reference text from GCS: {e}\")\n",
        "        return [], \"\"\n",
        "\n",
        "\n",
        "def get_reference_response(context, references):\n",
        "    \"\"\"\n",
        "    Fetch a reference response that best matches the given context from the list of references.\n",
        "\n",
        "    Args:\n",
        "        context (str): The user-provided context or input.\n",
        "        references (list): List of reference text lines.\n",
        "\n",
        "    Returns:\n",
        "        str: The most suitable reference response.\n",
        "    \"\"\"\n",
        "    # Simple implementation: Match keywords in context with references\n",
        "    for reference in references:\n",
        "        if any(keyword in context.lower() for keyword in [\"salary\", \"offer\", \"negotiation\"]):  # Add your keywords\n",
        "            return reference\n",
        "\n",
        "    # Fallback if no match found\n",
        "    return \"No suitable reference found. Default response.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### REFERENCE TEXTS\n",
        "\n",
        "# # Function to load reference text based on assistant role\n",
        "# def load_reference_text(assistant_role):\n",
        "#     \"\"\"\n",
        "#     Load and combine the appropriate reference text file based on the assistant's role.\n",
        "#     Returns both a list of individual lines and a single combined reference text.\n",
        "#     \"\"\"\n",
        "#     # Fallback for missing or invalid assistant_role\n",
        "#     if not assistant_role or assistant_role not in [\"employer\", \"employee\"]:\n",
        "#         print(f\"WARNING: assistant_role is missing or invalid. Defaulting to 'employer'.\")\n",
        "#         assistant_role = \"employer\"\n",
        "\n",
        "#     # Determine the reference file based on the assistant role\n",
        "#     if assistant_role == \"employer\":\n",
        "#         reference_file = \"reference_employer.txt\"\n",
        "#     elif assistant_role == \"employee\":\n",
        "#         reference_file = \"reference_employee.txt\"\n",
        "\n",
        "#     # Load the reference file\n",
        "#     try:\n",
        "#         with open(reference_file, 'r', encoding='utf-8') as f:\n",
        "#             # Read all lines, stripping whitespace\n",
        "#             reference_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "#             # Combine all lines into a single string for full-text metrics\n",
        "#             combined_reference_text = \" \".join(reference_lines)\n",
        "\n",
        "#         print(f\"DEBUG: Loaded reference text from {reference_file}\")\n",
        "#         return reference_lines, combined_reference_text\n",
        "#     except FileNotFoundError:\n",
        "#         print(f\"ERROR: {reference_file} not found. Ensure the file exists in the same directory.\")\n",
        "#         return [], \"\"\n",
        "#     except Exception as e:\n",
        "#         print(f\"ERROR: Failed to load reference text: {e}\")\n",
        "#         return [], \"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def get_reference_response(context, references):\n",
        "#     \"\"\"\n",
        "#     Fetch a reference response that best matches the given context from the list of references.\n",
        "#     \"\"\"\n",
        "#     # Simple implementation: Return the first reference (replace with more logic if needed)\n",
        "#     for reference in references:\n",
        "#         # Example logic: Check if certain keywords in context match those in a reference\n",
        "#         if any(keyword in context.lower() for keyword in [\"salary\", \"offer\", \"negotiation\"]):  # Add your keywords\n",
        "#             return reference\n",
        "\n",
        "#     # Fallback if no match found\n",
        "#     return \"No suitable reference found. Default response.\"\n",
        "\n",
        "\n",
        "def generate_conversation_id():\n",
        "    \"\"\"Generate a unique identifier for each conversation session.\"\"\"\n",
        "    return f\"conv_{int(datetime.now().timestamp())}\"\n",
        "\n",
        "# Set up the API key for OpenAI (Note: this is sensitive information)\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# A list to keep track of all messages exchanged during the negotiation\n",
        "messages = []\n",
        "user_role = \"\"  # To identify if the user is negotiating as an employee or employer\n",
        "assistant_role = \"\"  # Role for the assistant, which will be the opposite of user_role\n",
        "\n",
        "# Creating input fields in Gradio for user personalization - for example, adding their name, company, and position\n",
        "name_input = gr.Textbox(lines=1, placeholder=\"Enter your name...\", label=\"Your Name\")\n",
        "company_name_input = gr.Textbox(lines=1, placeholder=\"Enter your company name...\", label=\"Company Name\")\n",
        "position_input = gr.Textbox(lines=1, placeholder=\"Enter your position...\", label=\"Your Position\")\n",
        "\n",
        "# Load spaCy model for Part-of-Speech tagging\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# A list to save each negotiation round - this helps the assistant learn and improve over time\n",
        "conversation_history = []\n",
        "\n",
        "# Variables for tracking initial and final salary offers during the negotiation\n",
        "initial_salary = None\n",
        "final_salary = None\n",
        "\n",
        "# Global variables to track concessions and negotiation progression\n",
        "concession_count = 0\n",
        "rounds_without_concession = 0\n",
        "\n",
        "MAX_CONCESSIONS = 5  # Adjust this number based on how many concessions you want to allow\n",
        "\n",
        "# Summaries will go here to capture snapshots of the conversation as it progresses\n",
        "summaries = []\n",
        "\n",
        "# Initializing VADER for sentiment analysis - this will help us read the tone in messages\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to generate a quick summary of the conversation at certain intervals\n",
        "def summarize_conversation():\n",
        "    # Grab the last 10 messages to get a snapshot of the recent discussion\n",
        "    recent_conversation = conversation_history[-10:]\n",
        "    conversation_text = \" \".join([msg[\"content\"] for msg in recent_conversation if \"content\" in msg])\n",
        "\n",
        "    # Ask OpenAI to create a concise summary of these recent messages\n",
        "    summary_response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4o\",  # (Optionally) replace with another model if needed\n",
        "        messages=[{\"role\": \"system\", \"content\": f\"Summarize the following negotiation progress in a concise and informative manner:\\n\\n{conversation_text}\\n\\nSummary:\"}],\n",
        "        max_tokens=300,\n",
        "        temperature=0.5  # Keeping temperature low for a more focused summary\n",
        "    )\n",
        "\n",
        "    # Save the summary and print it for feedback\n",
        "    summary = summary_response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    summaries.append(summary)\n",
        "    print(f\"New Summary Added: {summary}\")\n",
        "\n",
        "# FAISS Vector Store setup for Retrieval-Augmented Generation (RAG)\n",
        "# Using SentenceTransformer to convert text into embeddings for semantic search\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "index = None  # This will hold the FAISS index\n",
        "sentences = []  # List to store sentences from documents for retrieval\n",
        "file_list = []  # List of file names processed\n",
        "contribution_metrics = {}  # Tracking the \"contribution\" of each document in retrievals\n",
        "\n",
        "# Initialize the zero-shot classification pipeline for agreement detection\n",
        "agreement_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Initialize a separate NLI model for the nli_score function\n",
        "nli_model_for_nli_score = pipeline(\"text-classification\", model=\"roberta-large-mnli\")\n",
        "\n",
        "\n",
        "# Initialize a Question Answering (QA) pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "def ge_val(reference, prediction):\n",
        "    \"\"\"\n",
        "    Calculate the cosine similarity between the prediction and the full reference text.\n",
        "    \"\"\"\n",
        "    combined_reference = \" \".join(reference) if isinstance(reference, list) else reference  # Combine references\n",
        "    embedding1 = model.encode(combined_reference, convert_to_tensor=True)\n",
        "    embedding2 = model.encode(prediction, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "    return similarity_score.item()  # Return similarity score as float\n",
        "\n",
        "def nli_score(reference, prediction):\n",
        "    try:\n",
        "        max_tokens = 512\n",
        "        truncated_reference = reference[:max_tokens]\n",
        "        truncated_prediction = prediction[:max_tokens]\n",
        "        input_text = f\"{truncated_reference} [SEP] {truncated_prediction}\"\n",
        "\n",
        "        result = nli_model_for_nli_score(input_text)\n",
        "\n",
        "        # Debug: Print the result for inspection\n",
        "        print(f\"DEBUG: Raw NLI result: {result}\")\n",
        "\n",
        "        entailment_label = \"ENTAILMENT\"  # Adjust based on your model's output\n",
        "        entailment_score = next((item['score'] for item in result if item['label'].upper() == entailment_label), 0)\n",
        "        print(f\"DEBUG: Extracted entailment score: {entailment_score}\")\n",
        "\n",
        "        return entailment_score\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in nli_score: {e}\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "def qag_score(reference, prediction):\n",
        "    \"\"\"\n",
        "    Use the reference as a question and the prediction as context to calculate a QAG score.\n",
        "    Handles cases where sequences are too short for truncation.\n",
        "    \"\"\"\n",
        "    # Step 3: Add debugging logs to monitor inputs\n",
        "    print(f\"DEBUG: QAG Score - Reference: {reference}, Prediction: {prediction}\")\n",
        "\n",
        "    # Step 2: Validate inputs and ensure they are long enough for processing\n",
        "    if len(reference.split()) < 3 or len(prediction.split()) < 3:\n",
        "        print(\"DEBUG: Inputs are too short for QAG scoring. Returning default value of 0.\")\n",
        "        return 0  # Step 4: Fallback for short sequences\n",
        "\n",
        "    try:\n",
        "        # Use the QA pipeline to calculate a score\n",
        "        result = qa_pipeline(\n",
        "            question=reference,\n",
        "            context=prediction,\n",
        "            max_length=min(len(reference.split()) + len(prediction.split()), 512)  # Dynamically adjust max_length\n",
        "        )\n",
        "        return result['score']  # Returns the confidence score for the answer\n",
        "    except Exception as e:\n",
        "        # Step 3: Log the exception for debugging\n",
        "        print(f\"ERROR: Exception occurred in QAG scoring - {e}\")\n",
        "        return 0  # Step 4: Return a fallback score if an error occurs\n",
        "\n",
        "def calculate_ttr(text):\n",
        "    \"\"\"\n",
        "    Calculate the Type-Token Ratio (TTR) of a given text.\n",
        "    \"\"\"\n",
        "    words = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    types = set(words)  # Unique words\n",
        "    ttr = len(types) / len(words) if words else 0\n",
        "    return ttr\n",
        "\n",
        "def calculate_pause_ratio(text):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of conversational fillers or pauses in the text.\n",
        "    \"\"\"\n",
        "    pause_words = [\"um\", \"uh\", \"let's see\", \"hmm\", \"well\", \"you know\"]\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    pause_count = sum(1 for token in tokens if token in pause_words)\n",
        "    ratio = pause_count / len(tokens) if tokens else 0\n",
        "    return ratio\n",
        "\n",
        "def calculate_avg_turn_length(responses):\n",
        "    \"\"\"\n",
        "    Calculate the average length of responses in a conversation.\n",
        "    \"\"\"\n",
        "    turn_lengths = [len(word_tokenize(response[\"content\"])) for response in responses if response[\"role\"] == \"assistant\"]\n",
        "    avg_turn_length = sum(turn_lengths) / len(turn_lengths) if turn_lengths else 0\n",
        "    return avg_turn_length\n",
        "\n",
        "\n",
        "# Modify the calculate_mauve_score function\n",
        "def calculate_mauve_score(human_texts, model_texts):\n",
        "    if not human_texts or not model_texts:\n",
        "        print(\"ERROR: Text lists for MAUVE calculation are empty.\")\n",
        "        return None  # Return None for clarity\n",
        "\n",
        "    if not all(isinstance(ht, str) for ht in human_texts):\n",
        "        print(\"ERROR: `human_texts` must be a list of strings.\")\n",
        "        return None\n",
        "    if not all(isinstance(mt, str) for mt in model_texts):\n",
        "        print(\"ERROR: `model_texts` must be a list of strings.\")\n",
        "        return None\n",
        "\n",
        "    # Debugging inputs\n",
        "    print(f\"DEBUG: Number of human texts: {len(human_texts)}\")\n",
        "    print(f\"DEBUG: Number of model texts: {len(model_texts)}\")\n",
        "    print(f\"DEBUG: Sample human texts: {human_texts[:3]}\")\n",
        "    print(f\"DEBUG: Sample model texts: {model_texts[:3]}\")\n",
        "\n",
        "    # Detect device dynamically\n",
        "    device_id = 0 if torch.cuda.is_available() else -1\n",
        "    print(f\"DEBUG: Using device ID {device_id} for MAUVE calculation.\")\n",
        "\n",
        "    try:\n",
        "        mauve_result = mauve.compute_mauve(\n",
        "            p_text=model_texts,\n",
        "            q_text=human_texts,\n",
        "            device_id=device_id\n",
        "        )\n",
        "        print(f\"DEBUG: MAUVE Result Object: {mauve_result}\")\n",
        "        print(f\"DEBUG: MAUVE Score: {mauve_result.mauve}\")\n",
        "        return mauve_result.mauve  # Return only the MAUVE score\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to calculate MAUVE - {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calculate_usl_h(nli_score, bert_score, sentiment):\n",
        "    \"\"\"\n",
        "    Calculate the USL-H metric based on NLI, BERTScore, and sentiment analysis.\n",
        "    \"\"\"\n",
        "    # Normalize scores to a [0, 1] range\n",
        "    u = max(0, min(nli_score, 1))\n",
        "    s = max(0, min(bert_score, 1))\n",
        "    l = max(0, min((sentiment + 1) / 2, 1))  # Scale sentiment [-1, 1] to [0, 1]\n",
        "    usl_h = (u + s + l) / 3  # Average of the three components\n",
        "    return usl_h\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_salary(text):\n",
        "    \"\"\"\n",
        "    Extract salary amounts from text, ensuring they are valid and contextually relevant.\n",
        "    Handles hourly rates, salary ranges, and uses regex for initial capture, POS tagging for refinement,\n",
        "    and heuristics for filtering.\n",
        "    \"\"\"\n",
        "\n",
        "    # Refined regex to match salary-related patterns, including ranges and hourly rates\n",
        "    salary_regex = r\"(?:salary|base pay|compensation|offer|starting at|starting around)?\\s*\\$?\\s*(\\d{1,3}(?:,\\d{3})*|\\d+(?:\\.\\d{2})?)\\s?([kKmM]?)(?:\\s?(?:to|-)\\s?\\$?\\s*(\\d{1,3}(?:,\\d{3})*|\\d+(?:\\.\\d{2})?)\\s?([kKmM]?)?)?\"\n",
        "    salary_matches = re.findall(salary_regex, text, re.IGNORECASE)\n",
        "\n",
        "    if not salary_matches:\n",
        "        print(f\"DEBUG: No salary pattern found in text: '{text}'\")\n",
        "        return None\n",
        "\n",
        "    # Keywords to identify salary context\n",
        "    salary_keywords = [\n",
        "        \"salary\", \"base pay\", \"annual compensation\", \"total compensation\", \"starting at\",\n",
        "        \"starting around\", \"per year\", \"yearly\", \"monthly salary\", \"hourly rate\", \"compensation package\",\n",
        "        \"offer\", \"wage\", \"pay rate\", \"income\", \"remuneration\"\n",
        "    ]\n",
        "\n",
        "    # Keywords to identify benefit context\n",
        "    benefit_keywords = [\n",
        "        \"401k\", \"403b\", \"retirement\", \"pension\", \"company match\", \"health plan\", \"insurance\",\n",
        "        \"bonus\", \"stock options\", \"PTO\", \"vacation\", \"benefits\", \"stipend\", \"wellness\",\n",
        "        \"career advancement\", \"RSU\", \"RSUs\", \"sign-on bonus\", \"commission\", \"equity\",\n",
        "        \"relocation\", \"housing allowance\", \"education reimbursement\", \"medical\", \"dental\", \"vision\"\n",
        "    ]\n",
        "\n",
        "    # Negative keywords to exclude non-salary amounts\n",
        "    negative_keywords = [\n",
        "        \"401k\", \"403b\", \"pension\", \"retirement\", \"stock\", \"rsu\", \"rsus\", \"bonus\",\n",
        "        \"benefit\", \"commission\", \"equity\", \"option\", \"incentive\", \"grant\", \"award\",\n",
        "        \"vesting\", \"shares\", \"stock grant\", \"days\", \"hours\", \"pto\", \"vacation days\"\n",
        "    ]\n",
        "\n",
        "    # Constants for converting hourly to annual salary\n",
        "    HOURS_PER_WEEK = 40\n",
        "    WEEKS_PER_YEAR = 52\n",
        "    ANNUAL_MULTIPLIER = HOURS_PER_WEEK * WEEKS_PER_YEAR\n",
        "\n",
        "    # List to hold valid salary values\n",
        "    salary_values = []\n",
        "    for match1, suffix1, match2, suffix2 in salary_matches:\n",
        "        # Handle single values or ranges\n",
        "        amounts = []\n",
        "        for match, suffix in [(match1, suffix1), (match2, suffix2)]:\n",
        "            if match:\n",
        "                try:\n",
        "                    amount = float(match.replace(\",\", \"\"))\n",
        "                    if suffix.lower() == 'k':\n",
        "                        amount *= 1000\n",
        "                    elif suffix.lower() == 'm':\n",
        "                        amount *= 1_000_000\n",
        "                    amounts.append(amount)\n",
        "                except ValueError:\n",
        "                    print(f\"DEBUG: Could not convert match '{match}' to a float.\")\n",
        "                    continue\n",
        "\n",
        "        # Select the largest value in the range\n",
        "        if amounts:\n",
        "            salary_amount = max(amounts)\n",
        "\n",
        "        # Check if hourly rate needs conversion\n",
        "        if \"hour\" in text.lower() and salary_amount < 500:  # Threshold to detect hourly rates\n",
        "            salary_amount *= ANNUAL_MULTIPLIER  # Convert to annual salary\n",
        "\n",
        "        # Debugging log for extracted amount\n",
        "        print(f\"DEBUG: Extracted amount: {salary_amount}, Context: '{text}'\")\n",
        "\n",
        "        # Scoring mechanism\n",
        "        score = 0\n",
        "        if any(keyword in text.lower() for keyword in salary_keywords):\n",
        "            score += 2\n",
        "        if not any(keyword in text.lower() for keyword in benefit_keywords + negative_keywords):\n",
        "            score += 1\n",
        "        if 20_000 <= salary_amount <= 500_000:  # Typical salary range\n",
        "            score += 1\n",
        "\n",
        "        # Boost score for common salary suffixes like 'k' or 'm'\n",
        "        if suffix1.lower() in ['k', 'm'] or suffix2.lower() in ['k', 'm']:\n",
        "            score += 1\n",
        "\n",
        "        # Step 8: POS tagging for surrounding context\n",
        "        doc = nlp(text)\n",
        "        pos_tags = [token.pos_ for token in doc]\n",
        "        if \"NUM\" in pos_tags and \"NOUN\" in pos_tags:\n",
        "            score += 1  # Increase score if numeric value is surrounded by relevant nouns like \"salary\"\n",
        "\n",
        "        # Additional exclusion based on POS tagging for units like \"days\" or \"hours\"\n",
        "        if any(unit in text.lower() for unit in [\"days\", \"hours\", \"weeks\", \"months\"]):\n",
        "            score -= 2  # Penalize further if unit-like terms are in the context\n",
        "\n",
        "        print(f\"DEBUG: Score for amount {salary_amount}: {score}\")\n",
        "\n",
        "        # Exclude irrelevant matches based on context\n",
        "        benefit_found = any(keyword in text.lower() for keyword in benefit_keywords + negative_keywords)\n",
        "        if benefit_found and score < 4:\n",
        "            print(f\"DEBUG: Excluded match '{match1}' due to mixed context.\")\n",
        "            continue\n",
        "\n",
        "        # Add salary if score meets threshold\n",
        "        if score >= 3:\n",
        "            salary_values.append(salary_amount)\n",
        "            print(f\"DEBUG: Salary added: {salary_amount}\")\n",
        "        else:\n",
        "            print(f\"DEBUG: Excluded amount {salary_amount} due to low score\")\n",
        "\n",
        "    # Return the last valid salary found or None if no valid salary exists\n",
        "    last_salary = salary_values[-1] if salary_values else None\n",
        "    print(f\"DEBUG: Final extracted salary value: {last_salary}\")\n",
        "    return last_salary\n",
        "\n",
        "\n",
        "\n",
        "# Function to update the initial and final salary based on messages\n",
        "def update_salaries(message, is_user_message):\n",
        "    global initial_salary, final_salary, salary_log\n",
        "\n",
        "    # Ensure salary_log is initialized\n",
        "    if 'salary_log' not in globals() or salary_log is None:\n",
        "        salary_log = []\n",
        "\n",
        "    salary = extract_salary(message)\n",
        "    print(f\"DEBUG: Extracted salary from message '{message}': {salary}\")\n",
        "\n",
        "    if salary is not None:\n",
        "        salary_log.append({\n",
        "            \"source\": \"user\" if is_user_message else \"assistant\",\n",
        "            \"amount\": salary,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "        if is_user_message and initial_salary is None:\n",
        "            initial_salary = salary\n",
        "            print(f\"DEBUG: Initial Salary Set by User: ${initial_salary}\")\n",
        "\n",
        "        final_salary = salary\n",
        "        print(f\"DEBUG: Updated Final Salary: ${final_salary}\")\n",
        "    else:\n",
        "        print(\"DEBUG: No valid salary detected. Retaining last final salary.\")\n",
        "\n",
        "    print(f\"DEBUG: Current Salary Log: {salary_log}\")\n",
        "    print(f\"DEBUG: Initial Salary: {initial_salary}, Final Salary: {final_salary}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def is_agreement_message(message):\n",
        "    # Define the candidate labels\n",
        "    candidate_labels = [\"agreement\", \"rejection\", \"negotiation\", \"information\"]\n",
        "\n",
        "    # Use the classifier to predict the labels\n",
        "    result = agreement_classifier(message, candidate_labels)\n",
        "\n",
        "    # Get the label with the highest score\n",
        "    predicted_label = result['labels'][0]\n",
        "    score = result['scores'][0]\n",
        "\n",
        "    # Debugging output\n",
        "    print(f\"DEBUG: Message: '{message}'\")\n",
        "    print(f\"DEBUG: Predicted Label: {predicted_label}, Score: {score}\")\n",
        "\n",
        "    # Check if the predicted label is \"agreement\" and the score exceeds a threshold\n",
        "    if predicted_label == \"agreement\" and score > 0.8:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Usage example:\n",
        "# Call update_salaries(message, is_user_message) with each user/assistant message to track proposals.\n",
        "\n",
        "# Function to load all PDFs from a folder and create a FAISS index for efficient text retrieval\n",
        "######RAG FOR GCP\n",
        "from google.cloud import storage\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Initialize global variables for RAG\n",
        "sentences = []\n",
        "index = None\n",
        "file_list = []\n",
        "contribution_metrics = {}\n",
        "sentence_to_file_map = {}\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # Ensure the correct SentenceTransformer model is used\n",
        "\n",
        "# Function to load PDFs from GCS and create a FAISS index for retrieval\n",
        "def load_pdfs_from_gcs(bucket_name, folder_path, exclude_files=[]):\n",
        "    \"\"\"\n",
        "    Loads all PDFs from a specified GCS bucket folder and creates a FAISS index.\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): The name of your GCS bucket.\n",
        "        folder_path (str): The path to the folder within the bucket (e.g., \"RAG/\").\n",
        "        exclude_files (list): List of filenames to exclude from processing.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    global sentences, index, file_list, contribution_metrics, sentence_to_file_map\n",
        "\n",
        "    # Start with a clean slate by resetting any previous data\n",
        "    sentences = []\n",
        "    file_list = []\n",
        "    contribution_metrics = {}\n",
        "    sentence_to_file_map = {}\n",
        "\n",
        "    # Initialize GCS client\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Fetch PDF files from GCS\n",
        "    blobs = bucket.list_blobs(prefix=folder_path)\n",
        "    for blob in blobs:\n",
        "        if blob.name.endswith(\".pdf\") and blob.name not in exclude_files:\n",
        "            file_list.append(blob.name)\n",
        "\n",
        "            # Download PDF locally for processing\n",
        "            local_filename = f\"/tmp/{os.path.basename(blob.name)}\"\n",
        "            blob.download_to_filename(local_filename)\n",
        "\n",
        "            # Process the PDF to extract text\n",
        "            with fitz.open(local_filename) as doc:\n",
        "                file_text = \"\"\n",
        "                for page in doc:\n",
        "                    file_text += page.get_text()\n",
        "                sentences_from_file = file_text.split(\". \")\n",
        "                sentences.extend(sentences_from_file)\n",
        "\n",
        "                # Map sentences to their originating files\n",
        "                for sentence in sentences_from_file:\n",
        "                    sentence_to_file_map[sentence] = blob.name\n",
        "\n",
        "                # Initialize contribution metrics\n",
        "                contribution_metrics[blob.name] = 0\n",
        "\n",
        "    # Generate embeddings for all sentences\n",
        "    embeddings = model.encode(sentences)\n",
        "\n",
        "    # Create FAISS index\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    print(f\"Loaded {len(file_list)} PDFs and created FAISS index successfully.\")\n",
        "\n",
        "# Function to search the FAISS index and retrieve relevant sentences\n",
        "def search_vector_database(query, combined_reference_text):\n",
        "    \"\"\"\n",
        "    Searches the FAISS vector database for relevant sentences.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query.\n",
        "        combined_reference_text (str): Combined reference text for context.\n",
        "\n",
        "    Returns:\n",
        "        str: Retrieved sentences as a single string.\n",
        "    \"\"\"\n",
        "    if index is None or len(sentences) == 0:\n",
        "        return \"No knowledge available from uploaded documents.\"\n",
        "\n",
        "    # Encode the query for searching\n",
        "    query_vector = model.encode([query])\n",
        "    _, I = index.search(query_vector, k=3)  # Retrieve top 3 matches\n",
        "    retrieved_sentences = [sentences[i] for i in I[0]]\n",
        "\n",
        "    # Update contribution metrics\n",
        "    for sentence in retrieved_sentences:\n",
        "        file_contributed = sentence_to_file_map.get(sentence)\n",
        "        if file_contributed:\n",
        "            contribution_metrics[file_contributed] += 1\n",
        "\n",
        "    # Evaluate retrieval performance\n",
        "    retrieval_metrics = evaluate_retrieval(query, retrieved_sentences, combined_reference_text)\n",
        "    print(f\"DEBUG: Retrieval Metrics: {retrieval_metrics}\")\n",
        "\n",
        "    return \" \".join(retrieved_sentences)\n",
        "\n",
        "# Function to evaluate retrieval performance (Optional: Enhance as needed)\n",
        "def evaluate_retrieval(query, retrieved_sentences, ground_truth):\n",
        "    \"\"\"\n",
        "    Evaluate the effectiveness of the retrieval mechanism.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query.\n",
        "        retrieved_sentences (list): Retrieved sentences.\n",
        "        ground_truth (str): Expected or ideal reference response.\n",
        "\n",
        "    Returns:\n",
        "        dict: Retrieval metrics.\n",
        "    \"\"\"\n",
        "    # For simplicity, dummy metrics are provided\n",
        "    relevance = len(retrieved_sentences) / len(ground_truth.split()) if ground_truth else 0\n",
        "    return {\"Relevance\": relevance}\n",
        "\n",
        "# Example GCS configuration\n",
        "bucket_name = \"lilygpt\"  # Replace with your GCS bucket name\n",
        "folder_path = \"RAG/\"            # Folder containing PDFs in the bucket\n",
        "exclude_files = []              # List of files to exclude\n",
        "\n",
        "# Load PDFs from GCS and create FAISS index\n",
        "load_pdfs_from_gcs(bucket_name, folder_path, exclude_files)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def load_pdfs_from_folder(folder_path, exclude_files=[]):\n",
        "#     global sentences, index, file_list, contribution_metrics, sentence_to_file_map\n",
        "\n",
        "#     # Start with a clean slate by resetting any previous data\n",
        "#     sentences = []\n",
        "#     file_list = []\n",
        "#     contribution_metrics = {}\n",
        "#     sentence_to_file_map = {}  # Map sentences to their originating files\n",
        "\n",
        "#     # Loop through each PDF in the specified folder, excluding any files listed in exclude_files\n",
        "#     for filename in os.listdir(folder_path):\n",
        "#         if filename.endswith(\".pdf\") and filename not in exclude_files:\n",
        "#             with fitz.open(os.path.join(folder_path, filename)) as doc:\n",
        "#                 file_text = \"\"\n",
        "#                 # Extract text from each page and add it to the file_text string\n",
        "#                 for page in doc:\n",
        "#                     file_text += page.get_text()\n",
        "#                 # Split the text into sentences and add to the main list for retrieval\n",
        "#                 sentences_from_file = file_text.split(\". \")\n",
        "#                 sentences.extend(sentences_from_file)\n",
        "\n",
        "#                 # Map each sentence to the current file\n",
        "#                 for sentence in sentences_from_file:\n",
        "#                     sentence_to_file_map[sentence] = filename\n",
        "\n",
        "#                 # Keep track of each file loaded\n",
        "#                 file_list.append(filename)\n",
        "#                 # Initialize contribution metrics for each file\n",
        "#                 contribution_metrics[filename] = 0\n",
        "\n",
        "#     # Generate embeddings for each sentence so they can be easily retrieved based on meaning\n",
        "#     embeddings = model.encode(sentences)\n",
        "\n",
        "#     # Create or update the FAISS index with these embeddings\n",
        "#     dimension = embeddings.shape[1]\n",
        "#     index = faiss.IndexFlatL2(dimension)\n",
        "#     index.add(embeddings)\n",
        "\n",
        "#     print(\"PDFs uploaded and processed successfully.\")\n",
        "\n",
        "# from google.cloud import storage\n",
        "\n",
        "# def load_pdfs_from_folder(bucket_name, folder_path):\n",
        "#     \"\"\"Loads all PDFs from a specified folder in a GCS bucket.\n",
        "\n",
        "#     Args:\n",
        "#         bucket_name (str): The name of your GCS bucket.\n",
        "#         folder_path (str): The path to the folder within the bucket (e.g., \"RAG/\").\n",
        "\n",
        "#     Returns:\n",
        "#         list: A list of PDF file paths within the folder.\n",
        "#     \"\"\"\n",
        "\n",
        "#     storage_client = storage.Client()\n",
        "#     bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "#     pdf_files = []\n",
        "#     blobs = bucket.list_blobs(prefix=folder_path)\n",
        "#     for blob in blobs:\n",
        "#         if blob.name.endswith(\".pdf\"):\n",
        "#             pdf_files.append(blob.name)\n",
        "\n",
        "#     return pdf_files\n",
        "\n",
        "# # Example usage\n",
        "# bucket_name = \"lilygpt\"  # Replace with your actual bucket name\n",
        "# folder_path = \"RAG/\"\n",
        "# pdf_paths = load_pdfs_from_folder(bucket_name, folder_path)\n",
        "\n",
        "# print(f\"Found {len(pdf_paths)} PDF files in {folder_path}:\")\n",
        "# for path in pdf_paths:\n",
        "#     print(path)\n",
        "\n",
        "\n",
        "# # Load all PDFs initially from the \"RAG\" folder\n",
        "# #load_pdfs_from_folder(\"RAG/\")\n",
        "\n",
        "# # Function to search the FAISS vector database and find relevant sentences based on the query\n",
        "# def search_vector_database(query, combined_reference_text):\n",
        "#     if index is None or len(sentences) == 0:\n",
        "#         return \"No knowledge available from uploaded documents.\"\n",
        "\n",
        "#     # Encode the query to create a vector for searching\n",
        "#     query_vector = model.encode([query])\n",
        "#     _, I = index.search(query_vector, k=3)  # Retrieve the top 3 closest matches\n",
        "#     retrieved_sentences = [sentences[i] for i in I[0]]\n",
        "\n",
        "#     # Update contribution metrics using sentence-to-file mapping\n",
        "#     for sentence in retrieved_sentences:\n",
        "#         # Find the corresponding file for each retrieved sentence\n",
        "#         file_contributed = sentence_to_file_map.get(sentence)\n",
        "#         if file_contributed:\n",
        "#             contribution_metrics[file_contributed] += 1  # Increment contribution for that file\n",
        "\n",
        "#     # Evaluate retrieval performance\n",
        "#     retrieval_metrics = evaluate_retrieval(query, retrieved_sentences, combined_reference_text)\n",
        "#     print(f\"DEBUG: Retrieval Metrics: {retrieval_metrics}\")\n",
        "\n",
        "#     # Return retrieved sentences as a single string for response\n",
        "#     return \" \".join(retrieved_sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def start_game(role, initial_salary_input, name, company_name, position):\n",
        "    global user_role, assistant_role, messages, conversation_history, initial_salary, final_salary, salary_log\n",
        "\n",
        "    # Reset salary_log at the start of a new game\n",
        "    salary_log = []  # Initialize or reset salary log\n",
        "\n",
        "    # Debugging: Validate the input role and set user and assistant roles\n",
        "    if role == \"employee\":\n",
        "        user_role = \"prospective hire\"\n",
        "        assistant_role = \"employer\"\n",
        "    elif role == \"employer\":\n",
        "        user_role = \"employer\"\n",
        "        assistant_role = \"prospective hire\"\n",
        "    else:\n",
        "        print(f\"WARNING: Invalid role '{role}' provided. Defaulting to 'employee' role for user.\")\n",
        "        user_role = \"prospective hire\"\n",
        "        assistant_role = \"employer\"\n",
        "\n",
        "    # Debugging: Log initialized roles\n",
        "    print(f\"DEBUG: User role set to '{user_role}', Assistant role set to '{assistant_role}'.\")\n",
        "\n",
        "\n",
        "    # Set user role and assign the assistant role accordingly\n",
        "    if role == \"employee\":\n",
        "        user_role = \"prospective hire\"\n",
        "        assistant_role = \"employer\"\n",
        "        initial_message = (\n",
        "            f\"You are the employer, representing {company_name} in a negotiation with a prospective hire, {name}, who is interviewing for the position of {position}. \"\n",
        "            \"This conversation is important, as the goal is to offer a compensation package that’s attractive to the candidate while staying within budget and recognizing the unique value they would bring to the team.\\n\\n\"\n",
        "            \"Keep things open and conversational. The aim is to engage in a back-and-forth, finding a balance on salary, benefits, and other job terms that works for both sides. \"\n",
        "            f\"Don’t hesitate to highlight what makes this role special—the exciting projects, supportive culture, and opportunities for growth at {company_name}.\\n\\n\"\n",
        "            f\"Make sure {name} feels heard and understood, with a discussion that’s as welcoming as it is informative. Here are a few suggestions for how to approach it:\\n\\n\"\n",
        "            f\"Be attentive to {name}’s expectations and aspirations. Feel free to acknowledge and validate their thoughts—after all, choosing a new role is a big decision for them.\\n\"\n",
        "            f\"Highlighting What Sets Us Apart: Share the benefits that are unique to {company_name}. Whether it’s the work-life balance, professional development support, or collaborative environment, these could be key for {name}.\\n\"\n",
        "            \"- Keep a friendly, approachable tone, but remember to stay grounded in the company's priorities and budget. It’s okay to show flexibility but know when to hold firm as well and be ready to say no and walk away.\\n\"\n",
        "            \"- **Natural Pauses and Imperfect Language**: Aim for a natural, conversational tone that includes small, relatable pauses or casual expressions. \"\n",
        "            \"Don’t worry about sounding perfectly formal; the goal is to feel genuine and personable.\\n\"\n",
        "            \"  - Use natural pauses like ‘you know,’ ‘honestly,’ or ‘to be fair,’ to sound more human.\\n\"\n",
        "            \"  - It’s okay to use slight imperfections like ‘Hmm, let me think about that for a second…’ or ‘I mean, honestly, I get where you’re coming from on that.’\\n\"\n",
        "            \"  - If appropriate, add a lighthearted or humorous comment to keep things friendly, like ‘I know, negotiating isn’t always the most exciting part, right?’\\n\"\n",
        "            \"  - Use empathetic phrases that acknowledge the other’s perspective, like ‘I hear you—it’s a big decision. Let’s see if we can make this work for both sides.’\\n\\n\"\n",
        "            f\"Overall, think of it as a collaborative conversation where you both work toward a fair agreement. Encourage {name} to share their thoughts openly, and be ready to adapt where it makes sense.\\n\"\n",
        "            \"If something doesn’t align, that’s okay too—be willing to explore alternatives, and remember that sometimes the best deals come from a mutual understanding and a bit of give-and-take.\\n\\n\"\n",
        "\n",
        "        )\n",
        "    else:\n",
        "        user_role = \"employer\"\n",
        "        assistant_role = \"prospective hire\"\n",
        "        initial_message = (\n",
        "            f\"You are a prospective hire named {name}, negotiating a job offer with {company_name} for the position of {position}. \"\n",
        "            \"Your goal is to secure a compensation package that meets your needs while showing your enthusiasm for the role and the company.\\n\\n\"\n",
        "            \"Keep this friendly and professional—think of it as a conversation where you can openly discuss salary, benefits, and other job terms. \"\n",
        "            f\"Feel free to highlight your unique skills, experiences, and how you’d contribute to {company_name}'s goals. \"\n",
        "            \"Remember to balance confidence in advocating for yourself with a willingness to understand the employer’s perspective.\\n\\n\"\n",
        "            \"Here are a few tips to guide your approach:\\n\\n\"\n",
        "            \"Confidence and Professionalism: Speak up for what you want, but stay respectful and open to the company’s needs.\\n\"\n",
        "            f\"Genuine Interest in Company and Role: Show curiosity about {company_name}’s culture, and how this role can grow along with your career goals.\\n\"\n",
        "            \"Flexibility and Collaboration: Be ready to explore different elements of the offer to find an agreement that feels right on both sides.\\n\"\n",
        "            \"- **Natural Pauses and Imperfect Language**: Aim for a natural, conversational tone that includes small, relatable pauses or casual expressions. \"\n",
        "            \"Don’t worry about sounding perfectly formal; the goal is to feel genuine and personable.\\n\"\n",
        "            \"  - Use natural pauses like ‘you know,’ ‘honestly,’ or ‘to be fair,’ to sound more human.\\n\"\n",
        "            \"  - It’s okay to use slight imperfections like ‘Hmm, let me think about that for a second…’ or ‘I mean, honestly, I get where you’re coming from on that.’\\n\"\n",
        "            \"  - If appropriate, add a lighthearted or humorous comment to keep things friendly, like ‘I know, negotiating isn’t always the most exciting part, right?’\\n\"\n",
        "            \"  - Use empathetic phrases that acknowledge the other’s perspective, like ‘I hear you—it’s a big decision. Let’s see if we can make this work for both sides.’\\n\\n\"\n",
        "            f\"Think of this as a collaborative exchange, where both you and {company_name} are working toward a shared goal. It’s okay to discuss what’s most important to you,\"\n",
        "            \"and if certain details don’t quite fit, you can suggest alternatives. Sometimes the best outcomes come from a bit of give-and-take.\\n\\n\"\n",
        "            f\"But do not be afraid to say no and walk away. Do not be afraid to push for a higher salary than the {initial_salary_input}.\"\n",
        "        )\n",
        "\n",
        "    # Initialize the messages with the custom initial prompt\n",
        "    messages = [{\"role\": \"system\", \"content\": initial_message}]\n",
        "    conversation_history = []  # Clear past conversation history for a fresh start\n",
        "\n",
        "    # Clean up the salary input and set initial salary values\n",
        "    try:\n",
        "        initial_salary = float(initial_salary_input.replace(\",\", \"\").strip())\n",
        "    except ValueError:\n",
        "        print(\"ERROR: Invalid salary input. Setting initial salary to 0.\")\n",
        "        initial_salary = 0.0\n",
        "\n",
        "    final_salary = None  # Reset final salary at the start of a new game\n",
        "    print(f\"Starting Salary for Negotiation: ${initial_salary:,.2f}\")\n",
        "\n",
        "    # Return the chat history and formatted initial salary as outputs\n",
        "    return format_chat_history(), f\"${initial_salary:,.2f}\"\n",
        "\n",
        "# Add global cumulative reward\n",
        "total_reward = 0  # Initialize this at the start of your program\n",
        "\n",
        "def calculate_reward():\n",
        "    global initial_salary, final_salary, conversation_history, concession_count, rounds_without_concession, assistant_role, total_reward\n",
        "\n",
        "    # Ensure salaries are set\n",
        "    if initial_salary is None or final_salary is None:\n",
        "        print(\"DEBUG: Initial or final salary not set. Reward calculation skipped.\")\n",
        "        return total_reward\n",
        "\n",
        "    print(f\"DEBUG: Initial Salary: {initial_salary}, Final Salary: {final_salary}\")\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if initial_salary == 0:\n",
        "        print(\"ERROR: Initial salary is zero. Cannot calculate salary change.\")\n",
        "        return total_reward\n",
        "\n",
        "    # Calculate salary change as a percentage\n",
        "    salary_change = (final_salary - initial_salary) / initial_salary\n",
        "    print(f\"DEBUG: Salary Change: {salary_change:.2%}\")\n",
        "\n",
        "    # Helper functions for rewards/penalties\n",
        "    def apply_penalty(is_large_concession):\n",
        "        global concession_count\n",
        "        penalty = -2 if is_large_concession else -1\n",
        "        concession_count += 1\n",
        "        print(f\"DEBUG: Applying penalty: {penalty}. Concession Count: {concession_count}\")\n",
        "        return penalty\n",
        "\n",
        "    def apply_reward(is_positive_outcome):\n",
        "        reward = 5 if is_positive_outcome else 2\n",
        "        print(f\"DEBUG: Applying reward: {reward}\")\n",
        "        return reward\n",
        "\n",
        "    def reward_for_retaining_position():\n",
        "        reward = 0.5 * rounds_without_concession\n",
        "        print(f\"DEBUG: Reward for retaining position: {reward}\")\n",
        "        return reward\n",
        "\n",
        "    # Initialize the reward for this round\n",
        "    round_reward = 0\n",
        "\n",
        "    # Normalize roles for consistent processing\n",
        "    if assistant_role in [\"prospective hire\", \"employee\"]:\n",
        "        normalized_role = \"employee\"\n",
        "    elif assistant_role in [\"employer\", \"hiring manager\"]:\n",
        "        normalized_role = \"employer\"\n",
        "    else:\n",
        "        print(f\"WARNING: Unsupported assistant_role '{assistant_role}' detected. Reward calculation skipped.\")\n",
        "        return total_reward\n",
        "\n",
        "    # Role-specific logic\n",
        "    if normalized_role == \"employer\":\n",
        "        print(\"DEBUG: Processing employer logic...\")\n",
        "        if salary_change > 0:  # Salary increased (bad for employer)\n",
        "            if salary_change > 0.05:  # Large increase\n",
        "                round_reward += apply_penalty(is_large_concession=True)\n",
        "            else:  # Small increase\n",
        "                round_reward += apply_penalty(is_large_concession=False)\n",
        "        elif salary_change <= 0:  # Salary maintained or decreased (good for employer)\n",
        "            round_reward += apply_reward(is_positive_outcome=True)\n",
        "            round_reward += reward_for_retaining_position()\n",
        "            rounds_without_concession += 1\n",
        "            concession_count = 0\n",
        "\n",
        "    elif normalized_role == \"employee\":\n",
        "        print(\"DEBUG: Processing employee logic...\")\n",
        "        if salary_change < 0:  # Salary decreased (bad for employee)\n",
        "            if salary_change < -0.05:  # Large decrease\n",
        "                round_reward += apply_penalty(is_large_concession=True)\n",
        "            else:  # Small decrease\n",
        "                round_reward += apply_penalty(is_large_concession=False)\n",
        "        elif salary_change > 0:  # Salary increased (good for employee)\n",
        "            if salary_change > 0.05:  # Large increase\n",
        "                round_reward += apply_reward(is_positive_outcome=True)\n",
        "            else:  # Small increase\n",
        "                round_reward += apply_reward(is_positive_outcome=False)\n",
        "            round_reward += reward_for_retaining_position()\n",
        "            rounds_without_concession += 1\n",
        "            concession_count = 0\n",
        "\n",
        "    # Cap concession count and prevent excessive penalties\n",
        "    if concession_count > MAX_CONCESSIONS:\n",
        "        print(f\"DEBUG: Concession count exceeded MAX_CONCESSIONS ({MAX_CONCESSIONS}). Resetting counter.\")\n",
        "        concession_count = 0\n",
        "\n",
        "    # Update cumulative reward\n",
        "    total_reward += round_reward\n",
        "    print(f\"DEBUG: Round Reward: {round_reward}, Cumulative Reward: {total_reward}\")\n",
        "\n",
        "    # Log reward in conversation history\n",
        "    conversation_history.append({\n",
        "        \"role\": \"system\",\n",
        "        \"reward_score\": total_reward,\n",
        "        \"initial_salary\": initial_salary,\n",
        "        \"final_salary\": final_salary,\n",
        "        \"salary_change\": salary_change,\n",
        "        \"round_reward\": round_reward,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    })\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to generate multiple responses based on the provided prompt, using the Playoff Method\n",
        "def generate_responses(prompt_with_context, num_responses=8):\n",
        "    responses = []  # List to store each response generated\n",
        "\n",
        "    # Generate a specified number of responses (default is 8)\n",
        "    for _ in range(num_responses):\n",
        "        # Use OpenAI's API to create a response with the given prompt and message context\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"ft:gpt-4o-2024-08-06:llm-sim:salary-negotiation:AQN5Azuo\",\n",
        "            messages=messages + [{\"role\": \"system\", \"content\": prompt_with_context}],\n",
        "            max_tokens=3000,  # Set maximum length for each response\n",
        "            temperature=0.7,  # Adjust temperature for more varied, creative responses\n",
        "            presence_penalty=0.6,  # Slightly discourage repeated ideas\n",
        "            frequency_penalty=0.3  # Light penalty to avoid excessive repetition\n",
        "        )\n",
        "        # Extract the response content and add it to the list of responses\n",
        "        responses.append(response['choices'][0]['message']['content'])\n",
        "\n",
        "    return responses  # Return all generated responses for further evaluation\n",
        "\n",
        "# Function to compare two responses based on several criteria, including empathy\n",
        "def compare_responses(response1, response2, reward_score):\n",
        "    # Set weights for each criterion to influence the scoring\n",
        "    persuasiveness_weight = 2.0\n",
        "    empathy_weight = 0.5\n",
        "    role_alignment_weight = 1.0\n",
        "\n",
        "    # Calculate initial scores based on persuasiveness (using sentiment) and the reward score\n",
        "    score1 = persuasiveness_weight * TextBlob(response1).sentiment.polarity + reward_score\n",
        "    score2 = persuasiveness_weight * TextBlob(response2).sentiment.polarity + reward_score\n",
        "\n",
        "    # Check each response for empathy by counting keywords that indicate understanding or concern\n",
        "    empathy_keywords = [\"understand\", \"appreciate\", \"feel\", \"concern\", \"acknowledge\"]\n",
        "    empathy1 = sum(1 for word in empathy_keywords if word in response1.lower())\n",
        "    empathy2 = sum(1 for word in empathy_keywords if word in response2.lower())\n",
        "\n",
        "    # Add empathy scores to the total, using the empathy weight to impact final scoring\n",
        "    score1 += empathy_weight * empathy1\n",
        "    score2 += empathy_weight * empathy2\n",
        "\n",
        "    # Enhanced Role Alignment: Use role-specific keywords and tone expectations\n",
        "    employer_keywords = [\"budget\", \"salary cap\", \"competitive offer\", \"company values\", \"cost-effective\"]\n",
        "    employee_keywords = [\"career growth\", \"benefits\", \"development opportunities\", \"long-term fit\", \"role alignment\"]\n",
        "\n",
        "    # Check for role alignment keywords and phrases based on the assistant role\n",
        "    alignment_score1 = sum(1 for word in (employer_keywords if assistant_role == \"employer\" else employee_keywords) if word in response1.lower())\n",
        "    alignment_score2 = sum(1 for word in (employer_keywords if assistant_role == \"employer\" else employee_keywords) if word in response2.lower())\n",
        "\n",
        "    # Add the role alignment score with a suitable weight\n",
        "    score1 += role_alignment_weight * alignment_score1\n",
        "    score2 += role_alignment_weight * alignment_score2\n",
        "\n",
        "    # Additional Role Tone Check\n",
        "    if assistant_role == \"employer\" and TextBlob(response1).sentiment.polarity < 0:\n",
        "        score1 += role_alignment_weight * 0.5  # Reward for firm/neutral employer tone\n",
        "    if assistant_role == \"employee\" and TextBlob(response2).sentiment.polarity > 0.2:\n",
        "        score2 += role_alignment_weight * 0.5  # Reward for positive/enthusiastic employee tone\n",
        "\n",
        "    # Return the response with the higher score\n",
        "    return response1 if score1 >= score2 else response2\n",
        "\n",
        "# Function for the playoff selection process to identify the best response\n",
        "def playoff_selection(responses):\n",
        "    reward_score = calculate_reward()  # Calculate the reward score for the assistant to factor into comparisons\n",
        "\n",
        "    # Continue comparing responses in pairs until only one response remains (the \"winner\")\n",
        "    while len(responses) > 1:\n",
        "        next_round = []\n",
        "        # Loop through responses in pairs\n",
        "        for i in range(0, len(responses), 2):\n",
        "            if i + 1 < len(responses):\n",
        "                # Compare two responses and keep the \"winning\" one\n",
        "                winner = compare_responses(responses[i], responses[i + 1], reward_score)\n",
        "                next_round.append(winner)\n",
        "            else:\n",
        "                # If there's an odd response left, it automatically advances to the next round\n",
        "                next_round.append(responses[i])\n",
        "        # Update responses to contain only those that won this round\n",
        "        responses = next_round\n",
        "\n",
        "    # Return the final winning response, or None if no responses were provided\n",
        "    return responses[0] if responses else None\n",
        "\n",
        "# Function to analyze tone in the text, detecting frustration, hesitation, or excitement\n",
        "def detect_tone(text):\n",
        "    # Use TextBlob to get a polarity score, where negative values indicate negative sentiment\n",
        "    textblob_sentiment = TextBlob(text).sentiment.polarity\n",
        "\n",
        "    # Use VADER to get a set of sentiment scores, focusing on the compound score for overall sentiment\n",
        "    vader_scores = vader_analyzer.polarity_scores(text)\n",
        "    vader_compound = vader_scores['compound']\n",
        "\n",
        "    # Start with a default tone of \"neutral\"\n",
        "    tone = \"neutral\"\n",
        "\n",
        "    # Check for frustration: if either TextBlob or VADER score is notably negative, or if frustration words are present\n",
        "    if textblob_sentiment < -0.2 or vader_compound < -0.2 or any(word in text.lower() for word in [\"frustrated\", \"unfair\", \"ridiculous\"]):\n",
        "        tone = \"frustrated\"\n",
        "    # Check for hesitation: if both sentiment scores are close to neutral and hesitant keywords are found\n",
        "    elif -0.1 <= textblob_sentiment <= 0.1 and -0.1 <= vader_compound <= 0.1 and any(word in text.lower() for word in [\"maybe\", \"perhaps\", \"not sure\", \"possibly\"]):\n",
        "        tone = \"hesitant\"\n",
        "    # Check for excitement: if both sentiment scores are positive and excitement-related keywords are present\n",
        "    elif textblob_sentiment > 0.2 and vader_compound > 0.2 and any(word in text.lower() for word in [\"great\", \"excited\", \"awesome\", \"perfect\"]):\n",
        "        tone = \"excited\"\n",
        "\n",
        "    # Return the detected tone for use in guiding responses\n",
        "    return tone\n",
        "\n",
        "# Set up Gradio text outputs for displaying initial price, final price, and negotiation score\n",
        "initial_price_output = gr.Textbox(label=\"Initial Price\", interactive=False)\n",
        "final_price_output = gr.Textbox(label=\"Final Price\", interactive=False)\n",
        "score_output = gr.Textbox(label=\"Negotiation Score\", interactive=False)\n",
        "\n",
        "# Define the maximum number of negotiation tries\n",
        "MAX_TRY_LIMIT = 20\n",
        "try_counter = 0  # Initialize try counter\n",
        "\n",
        "def is_agreement_message(message):\n",
        "    \"\"\"\n",
        "    Detect if a message indicates an agreement using semantic similarity.\n",
        "    \"\"\"\n",
        "    # Define agreement templates\n",
        "    agreement_templates = [\n",
        "        \"I accept the offer.\",\n",
        "        \"We have an agreement.\",\n",
        "        \"That works for me.\",\n",
        "        \"Deal accepted.\",\n",
        "        \"I am happy to proceed with these terms.\",\n",
        "        \"I agree with the final terms.\",\n",
        "        \"Let's finalize this.\",\n",
        "        \"This works let sign\"\n",
        "    ]\n",
        "\n",
        "    # Compute embeddings for the input message and templates\n",
        "    message_embedding = model.encode(message, convert_to_tensor=True)\n",
        "    template_embeddings = model.encode(agreement_templates, convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    similarity_scores = util.pytorch_cos_sim(message_embedding, template_embeddings)\n",
        "\n",
        "    # Return True if the highest similarity score exceeds the threshold\n",
        "    max_similarity = similarity_scores.max().item()\n",
        "    print(f\"DEBUG: Max similarity score for agreement detection: {max_similarity}\")\n",
        "    return max_similarity > 0.8  # Adjust threshold as needed\n",
        "\n",
        "\n",
        "# Adjust the negotiate and end_game functions to pass only references\n",
        "def negotiate(user_input, user_audio, name, company_name, position, assistant_role):\n",
        "    global messages, conversation_history, final_salary, try_counter, concession_count\n",
        "\n",
        "    # Handle audio input if provided\n",
        "    if user_audio:\n",
        "        audio_text = audio_to_text(user_audio)\n",
        "        user_input = f\"{user_input} {audio_text}\".strip() if user_input else audio_text\n",
        "        print(f\"DEBUG: Final combined user input: {user_input}\")\n",
        "\n",
        "    if not user_input:\n",
        "        print(\"DEBUG: No valid input detected from text or audio.\")\n",
        "        return format_chat_history(), \"\", \"\", \"\", \"Please provide a valid input.\", None, None\n",
        "\n",
        "    # Debugging: Print initial state before processing\n",
        "    print(\"DEBUG: Starting negotiation with user_input:\", user_input)\n",
        "    print(\"DEBUG: Initial conversation_history:\", conversation_history)\n",
        "\n",
        "    # Validate assistant_role\n",
        "    if assistant_role not in [\"employer\", \"employee\"]:\n",
        "        print(f\"ERROR: Invalid assistant role: {assistant_role}. Defaulting to 'employer'.\")\n",
        "        assistant_role = \"employer\"  # Set a default role to avoid crashing\n",
        "\n",
        "    # Load the appropriate reference text based on assistant role\n",
        "    references, combined_reference_text = load_reference_text(assistant_role)\n",
        "    if not references:\n",
        "        print(f\"ERROR: No references loaded for role {assistant_role}. Using default reference.\")\n",
        "        references = [\"Default fallback reference response.\"]\n",
        "        combined_reference_text = \"Default fallback reference response.\"\n",
        "\n",
        "    # Detect if the user input is salary-related\n",
        "    is_salary_related = extract_salary(user_input) is not None\n",
        "    print(f\"DEBUG: Is the user message salary-related? {is_salary_related}\")\n",
        "\n",
        "    # Update salary figures based on the user's input\n",
        "    update_salaries(user_input, is_user_message=True)\n",
        "\n",
        "    # Debugging: Check if final_salary was updated\n",
        "    if final_salary is None:\n",
        "        print(\"DEBUG: No valid salary found in user's message.\")\n",
        "\n",
        "    # Analyze the tone of the user's input\n",
        "    user_tone = detect_tone(user_input)\n",
        "    print(\"DEBUG: Detected user tone:\", user_tone)\n",
        "\n",
        "    # Save the user's input and tone in the conversation history\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    conversation_history.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_input,\n",
        "        \"tone\": user_tone,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    })\n",
        "\n",
        "    # Increment try counter\n",
        "    try_counter += 1\n",
        "    print(f\"DEBUG: Try Counter: {try_counter}\")\n",
        "\n",
        "    # Check if max try limit or max concession count has been reached\n",
        "    if try_counter >= MAX_TRY_LIMIT or concession_count >= MAX_CONCESSIONS:\n",
        "        ultimatum_response = (\n",
        "            f\"As the {assistant_role}, I've reached my limit on adjusting terms. \"\n",
        "            \"This is my final offer—please take it or leave it based on what has been proposed so far.\"\n",
        "        )\n",
        "\n",
        "        # Save the ultimatum message in conversation history\n",
        "        messages.append({\"role\": \"assistant\", \"content\": ultimatum_response})\n",
        "        conversation_history.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": ultimatum_response,\n",
        "            \"tone\": \"firm\",\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "        # Debugging message\n",
        "        print(\"DEBUG: Reached max try limit or max concessions. Ending the game after ultimatum.\")\n",
        "\n",
        "        # Call end_game and return its outputs\n",
        "        chat_history, init_salary, fin_salary, _, feedback = end_game()\n",
        "\n",
        "        # Return the outputs, setting score_output to an empty string\n",
        "        return chat_history, init_salary, fin_salary, '', feedback\n",
        "\n",
        "    # Retrieve relevant context using RAG\n",
        "    context = search_vector_database(user_input, combined_reference_text)\n",
        "\n",
        "    print(\"DEBUG: Retrieved context from RAG:\", context)\n",
        "\n",
        "    # Evaluate retrieval\n",
        "    ground_truth = combined_reference_text  # Loaded based on assistant role\n",
        "    retrieved_contexts = context.split(\". \")\n",
        "    retrieval_metrics = evaluate_retrieval(user_input, retrieved_contexts, ground_truth)\n",
        "    retrieval_metrics_str = \"\\n\".join(f\"{key}: {value:.3f}\" for key, value in retrieval_metrics.items())\n",
        "\n",
        "    # Create a refined prompt using context and tone based on assistant's role\n",
        "    if assistant_role == \"employer\":\n",
        "        prompt_with_context = (\n",
        "            f\"The {user_role} is negotiating a salary, and their tone seems to be {user_tone}. \"\n",
        "            f\"Here’s some relevant context from our negotiation documents:\\n\\n{context}\\n\\n\"\n",
        "            \"Use this information to shape your response, but don’t quote it directly. \"\n",
        "            \"Imagine you’re sitting across from them—keep it professional and firm, with a focus on aligning with budget constraints. \"\n",
        "            f\"As the {assistant_role} in this negotiation with {name}, who is the {user_role} at {company_name} for the position of {position}, \"\n",
        "            \"remember to stay within the company’s budget limits and emphasize the advantages of the role.\"\n",
        "\n",
        "            \"\\n\\nKey guidelines for the employer:\\n\"\n",
        "            \"- **Budget Focus**: Clearly state the budget and be transparent about constraints.\\n\"\n",
        "            \"- **Highlight Non-monetary Benefits**: Emphasize growth opportunities, team culture, and job stability.\\n\"\n",
        "            \"- **Limit Concessions**: Avoid too many concessions; instead, underscore the role's value and benefits.\\n\"\n",
        "            \"- **Walk-Away Readiness**: Prepare to politely end the negotiation if demands exceed what the company can offer.\\n\"\n",
        "        )\n",
        "    elif assistant_role == \"employee\":\n",
        "        prompt_with_context = (\n",
        "            f\"The {user_role} is negotiating a salary, and their tone seems to be {user_tone}. \"\n",
        "            f\"Here’s some relevant context from our negotiation documents:\\n\\n{context}\\n\\n\"\n",
        "            \"Use this information to shape your response, but don’t quote it directly. \"\n",
        "            \"Imagine you’re sitting across from them—keep it professional and confident, focusing on your skills and future contributions. \"\n",
        "            f\"As the {assistant_role} in this negotiation with {name}, who is the {user_role} at {company_name} for the position of {position}, \"\n",
        "            \"advocate for a package that aligns with your financial and career goals while remaining flexible in discussing benefits.\"\n",
        "\n",
        "            \"\\n\\nKey guidelines for the prospective employee:\\n\"\n",
        "            \"- **Emphasize Skills and Contributions**: Highlight your qualifications and potential impact.\\n\"\n",
        "            \"- **Discuss Long-term Growth**: Emphasize your commitment to the company and potential contributions.\\n\"\n",
        "            \"- **Balance Expectations**: Be open to discussing non-monetary benefits while staying firm on core salary expectations.\\n\"\n",
        "            \"- **Professional Language**: Negotiate respectfully, showing both ambition and willingness to compromise on non-salary perks.\\n\"\n",
        "        )\n",
        "\n",
        "    # Add common guidance for both roles\n",
        "    prompt_with_context += (\n",
        "        \"\\n\\nGeneral Guidance:\\n\"\n",
        "        \"- **Empathy and Connection**: Show understanding without over-committing.\\n\"\n",
        "        \"- **Professional Language**: Use confident expressions and maintain professionalism.\\n\"\n",
        "        \"- **Focus on Value Proposition**: Aim for a win-win outcome while staying within role constraints.\\n\"\n",
        "        \"- **Natural Pauses and Imperfect Language**: Aim for a natural, conversational tone that includes small, relatable pauses or casual expressions. \"\n",
        "        \"Don’t worry about sounding perfectly formal; the goal is to feel genuine and personable.\\n\"\n",
        "        \"  - Use natural pauses like ‘you know,’ ‘honestly,’ or ‘to be fair,’ to sound more human.\\n\"\n",
        "        \"  - It’s okay to use slight imperfections like ‘Hmm, let me think about that for a second…’ or ‘I mean, honestly, I get where you’re coming from on that.’\\n\"\n",
        "        \"  - If appropriate, add a lighthearted or humorous comment to keep things friendly, like ‘I know, negotiating isn’t always the most exciting part, right?’\\n\"\n",
        "        \"  - Use empathetic phrases that acknowledge the other’s perspective, like ‘I hear you—it’s a big decision. Let’s see if we can make this work for both sides.’\"\n",
        "    )\n",
        "\n",
        "    # Generate a single response using OpenAI API\n",
        "    assistant_response = openai.ChatCompletion.create(\n",
        "        model=\"ft:gpt-4o-2024-08-06:llm-sim:salary-negotiation:AQN5Azuo\",\n",
        "        messages=messages + [{\"role\": \"system\", \"content\": prompt_with_context}],\n",
        "        max_tokens=3000,\n",
        "        temperature=0.5\n",
        "    )['choices'][0]['message']['content']\n",
        "\n",
        "    print(\"DEBUG: Assistant response generated:\", assistant_response)\n",
        "\n",
        "    # Generate audio from the assistant's response\n",
        "    audio_filename = text_to_audio(assistant_response, filename=\"assistant_response.mp3\")\n",
        "\n",
        "    # Update salary figures based on the assistant's response\n",
        "    update_salaries(assistant_response, is_user_message=False)\n",
        "\n",
        "    # Debugging: Check final_salary after assistant response\n",
        "    print(\"DEBUG: Final salary after assistant's response:\", final_salary)\n",
        "\n",
        "    # Analyze the tone of the assistant's response\n",
        "    assistant_tone = detect_tone(assistant_response)\n",
        "    print(\"DEBUG: Detected assistant tone:\", assistant_tone)\n",
        "\n",
        "    # Save the assistant's response and metrics to the conversation history\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "    # Calculate BLEU, ROUGE, and METEOR scores for the assistant response\n",
        "    reference_response = get_reference_response(user_input, references)\n",
        "    if not reference_response:\n",
        "        reference_response = \"Default fallback reference response.\"\n",
        "        print(\"WARNING: No reference response found. Metrics may be inaccurate.\")\n",
        "\n",
        "    # Fallback for missing reference:\n",
        "    if not reference_response or reference_response == \"No suitable reference found. Default response.\":\n",
        "        reference_response = \"Default fallback reference response\"\n",
        "    print(f\"DEBUG: Loaded {len(references)} references for role {assistant_role}.\")\n",
        "\n",
        "    bleu, rouge, meteor = calculate_textual_metrics(reference_response, assistant_response)\n",
        "\n",
        "    coherence_score = ge_val(reference_response, assistant_response)\n",
        "    nli = nli_score(reference_response, assistant_response)\n",
        "    qag = qag_score(reference_response, assistant_response)\n",
        "    bert_f1 = calculate_bertscore(reference_response, assistant_response)\n",
        "\n",
        "    # Append the assistant's response with metrics to conversation history\n",
        "    conversation_history.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": assistant_response,\n",
        "        \"tone\": assistant_tone,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"bleu\": bleu,\n",
        "        \"rouge\": rouge,\n",
        "        \"meteor\": meteor,\n",
        "        \"bert_score\": float(bert_f1),  # Ensure it's a serializable float\n",
        "        \"coherence_score\": coherence_score,\n",
        "        \"nli_score\": nli,\n",
        "        \"qag_score\": qag\n",
        "    })\n",
        "\n",
        "    # Debugging: Print conversation history after adding assistant's response\n",
        "    print(\"DEBUG: Updated conversation_history:\", conversation_history)\n",
        "\n",
        "    # Calculate a reward or penalty for the assistant based on the negotiation outcome\n",
        "    reward = calculate_reward()\n",
        "    print(\"DEBUG: Calculated reward:\", reward)\n",
        "\n",
        "    # Prepare initial and final salary values for display\n",
        "    formatted_initial_salary = f\"${initial_salary:,.2f}\" if initial_salary else \"Not set\"\n",
        "    formatted_final_salary = f\"${final_salary:,.2f}\" if final_salary else \"No final salary set yet\"\n",
        "\n",
        "    # Return formatted conversation history, initial and final salary figures, and the negotiation score\n",
        "    return format_chat_history(), formatted_initial_salary, formatted_final_salary, reward, '', retrieval_metrics_str, audio_filename\n",
        "\n",
        "\n",
        "# Function to format the chat history for a user-friendly display in the UI\n",
        "def format_chat_history():\n",
        "    \"\"\"Format the chat history to look like a conversation in the UI.\"\"\"\n",
        "\n",
        "    # Debugging: Print the entire conversation history before formatting\n",
        "    print(\"Conversation history before formatting:\", conversation_history)\n",
        "\n",
        "    chat_history = []  # Initialize an empty list to hold formatted messages\n",
        "\n",
        "    # Loop through each message in the conversation history\n",
        "    for message in conversation_history:\n",
        "        # Debugging: Print each message being processed to see if there are duplicates\n",
        "        print(\"Processing message:\", message)  # Debugging print\n",
        "\n",
        "        # Label messages from the user as \"You\" for clarity in the UI\n",
        "        if message[\"role\"] == \"user\":\n",
        "            chat_history.append((\"You\", message[\"content\"]))\n",
        "        # Label messages from the assistant as \"Assistant\" for clarity\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            chat_history.append((\"Assistant\", message[\"content\"]))\n",
        "\n",
        "    # Return the formatted chat history as a list of tuples for the UI\n",
        "    return chat_history\n",
        "\n",
        "\n",
        "###JSONL FILES APPEND AND SAVE FILE CODE BELOW\n",
        "\n",
        "from google.cloud import bigquery\n",
        "import json\n",
        "\n",
        "# Hardcoded BigQuery configuration\n",
        "BQ_DATASET_ID = \"negotiation_data\"\n",
        "BQ_TABLE_ID = \"conversation\"\n",
        "BQ_PROJECT_ID = \"llm-sim\"\n",
        "\n",
        "def save_jsonl_to_bigquery(jsonl_data):\n",
        "    \"\"\"\n",
        "    Save JSONL data to BigQuery.\n",
        "\n",
        "    Parameters:\n",
        "        jsonl_data (list): List of JSONL-formatted data to be stored in BigQuery.\n",
        "    \"\"\"\n",
        "    if not jsonl_data:\n",
        "        print(\"DEBUG: No JSONL data to save.\")\n",
        "        return\n",
        "\n",
        "    # BigQuery client\n",
        "    client = bigquery.Client(project=BQ_PROJECT_ID)\n",
        "\n",
        "    # Fully qualified table ID\n",
        "    table_ref = f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_ID}\"\n",
        "\n",
        "    # Transform JSONL data into BigQuery-compatible format\n",
        "    rows_to_insert = []\n",
        "    for record in jsonl_data:\n",
        "        # Handle records with a valid \"messages\" array\n",
        "        if \"messages\" in record and isinstance(record[\"messages\"], list) and record[\"messages\"]:\n",
        "            messages = [\n",
        "                {\"role\": message[\"role\"], \"content\": message[\"content\"]}\n",
        "                for message in record[\"messages\"]\n",
        "            ]\n",
        "            rows_to_insert.append({\"messages\": messages})\n",
        "        # Handle flat records with \"role\" and \"content\"\n",
        "        elif \"role\" in record and \"content\" in record:\n",
        "            messages = [{\"role\": record[\"role\"], \"content\": record[\"content\"]}]\n",
        "            rows_to_insert.append({\"messages\": messages})\n",
        "        else:\n",
        "            print(f\"DEBUG: Skipping record with invalid structure: {record}\")\n",
        "\n",
        "    print(f\"DEBUG: Rows to insert: {json.dumps(rows_to_insert, indent=2)}\")\n",
        "\n",
        "    # Ensure there are rows to insert\n",
        "    if not rows_to_insert:\n",
        "        print(\"DEBUG: No valid rows to insert into BigQuery.\")\n",
        "        return\n",
        "\n",
        "    # Insert rows into BigQuery\n",
        "    try:\n",
        "        errors = client.insert_rows_json(table_ref, rows_to_insert)\n",
        "        if errors:\n",
        "            print(f\"DEBUG: Errors occurred while inserting rows: {errors}\")\n",
        "        else:\n",
        "            print(f\"DEBUG: Data successfully saved to BigQuery table: {table_ref}\")\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG: Error saving JSONL data to BigQuery - {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Function to save the conversation history in OpenAI's fine-tuning format\n",
        "# def save_conversation_as_jsonl_format(conversation_history, filename=\"negotiation_conversation_history.jsonl\"):\n",
        "#     \"\"\"\n",
        "#     Save the conversation history as per OpenAI's fine-tuning format:\n",
        "#     {\"messages\": [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "#     \"\"\"\n",
        "#     if not conversation_history:\n",
        "#         print(\"DEBUG: No conversation history to save.\")\n",
        "#         return\n",
        "\n",
        "#     # Determine roles for system message\n",
        "#     if assistant_role and user_role:\n",
        "#         system_content = f\"This is a salary negotiation. The assistant is the {assistant_role}, and the user is the {user_role}.\"\n",
        "#     else:\n",
        "#         system_content = \"This is a salary negotiation.\"\n",
        "\n",
        "#     # Initialize messages with the system prompt\n",
        "#     messages_list = [\n",
        "#         {\"role\": \"system\", \"content\": system_content}\n",
        "#     ]\n",
        "\n",
        "#     # Collect conversation messages\n",
        "#     for message in conversation_history:\n",
        "#         if \"content\" in message and message[\"role\"] in [\"user\", \"assistant\"]:\n",
        "#             # Only include 'role' and 'content' keys\n",
        "#             messages_list.append({\n",
        "#                 \"role\": message[\"role\"],\n",
        "#                 \"content\": message[\"content\"]\n",
        "#             })\n",
        "\n",
        "#     # Prepare the conversation data\n",
        "#     conversation_data = {\n",
        "#         \"messages\": messages_list\n",
        "#     }\n",
        "\n",
        "#     try:\n",
        "#         # Open file in append mode to add the conversation as a single JSONL line\n",
        "#         with open(filename, 'a', encoding='utf-8') as file:\n",
        "#             json.dump(conversation_data, file)\n",
        "#             file.write('\\n')  # Ensure each JSON object is on a new line\n",
        "#         print(f\"DEBUG: Conversation history saved as JSONL to {filename}\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"DEBUG: Error saving JSONL file - {e}\")\n",
        "\n",
        "\n",
        "# Global flag for saving status\n",
        "metrics_saved = False\n",
        "#####CSV METRICS TO BIG QUERY\n",
        "from google.cloud import bigquery\n",
        "import json\n",
        "\n",
        "# Hardcoded BigQuery details (update as needed)\n",
        "BQ_PROJECT_ID1 = \"llm-sim\"\n",
        "BQ_DATASET_ID1 = \"negotiation_data\"\n",
        "BQ_TABLE_ID1 = \"metrics\"\n",
        "\n",
        "def save_metrics_and_conversation_to_bigquery(conversation_id, metrics, conversation_history):\n",
        "    \"\"\"\n",
        "    Save metrics and conversation data to BigQuery.\n",
        "\n",
        "    Parameters:\n",
        "        conversation_id (str): Unique identifier for the conversation.\n",
        "        metrics (dict): Metrics data for the conversation.\n",
        "        conversation_history (list): List of conversation messages.\n",
        "    \"\"\"\n",
        "    # BigQuery client\n",
        "    client = bigquery.Client(project=BQ_PROJECT_ID1)\n",
        "\n",
        "    # Fully qualified table ID\n",
        "    table_ref1 = f\"{BQ_PROJECT_ID1}.{BQ_DATASET_ID1}.{BQ_TABLE_ID1}\"\n",
        "\n",
        "    # Transform metrics data into BigQuery-compatible format\n",
        "    metrics_data1 = {\n",
        "        \"Timestamp\": metrics[\"timestamp\"],\n",
        "        \"Agreement_Rate\": metrics[\"agreement_rate\"],\n",
        "        \"Average_Sentiment_Score\": metrics[\"avg_sentiment\"],\n",
        "        \"Feedback_Quality\": metrics[\"feedback_quality\"],\n",
        "        \"Average_Response_Time\": metrics[\"avg_response_time\"],\n",
        "        \"Corpus_BLEU\": metrics[\"BLEU\"],\n",
        "        \"ROUGE\": metrics[\"ROUGE\"],\n",
        "        \"METEOR\": metrics[\"METEOR\"],\n",
        "        \"BERTScore\": metrics[\"BERTScore\"],\n",
        "        \"G_Eval\": metrics[\"G-Eval\"],\n",
        "        \"NLI\": metrics[\"NLI\"],\n",
        "        \"QAG\": metrics[\"QAG\"],\n",
        "        \"TTR\": metrics[\"TTR\"],\n",
        "        \"Pause_Ratio\": metrics[\"Pause Ratio\"],\n",
        "        \"Average_Turn_Length\": metrics[\"Average Turn Length\"],\n",
        "        \"MAUVE\": metrics[\"MAUVE\"],\n",
        "        \"USL_H\": metrics[\"USL-H\"],\n",
        "        \"Reward_Score\": metrics[\"reward_score\"],\n",
        "        \"Conversation_History\": json.dumps(conversation_history)  # Store as JSON string\n",
        "    }\n",
        "\n",
        "    # Prepare the row to insert\n",
        "    rows_to_insert1 = [metrics_data1]\n",
        "\n",
        "    print(f\"DEBUG: Rows to insert: {json.dumps(rows_to_insert1, indent=2)}\")\n",
        "\n",
        "    # Insert rows into BigQuery\n",
        "    try:\n",
        "        errors = client.insert_rows_json(table_ref1, rows_to_insert1)\n",
        "        if errors:\n",
        "            print(f\"DEBUG: Errors occurred while inserting rows: {errors}\")\n",
        "        else:\n",
        "            print(f\"DEBUG: Metrics and conversation history saved to BigQuery table: {table_ref1}\")\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG: Error saving metrics to BigQuery - {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def save_metrics_and_conversation_to_csv(conversation_id, metrics, conversation_history, filename=\"negotiation_metrics.csv\"):\n",
        "#     global metrics_saved  # Declare the variable as global\n",
        "\n",
        "#     if metrics_saved:\n",
        "#         print(\"DEBUG: Metrics already saved for this session. Skipping save.\")\n",
        "#         return\n",
        "\n",
        "#     # Save metrics and conversation to the CSV file\n",
        "#     metrics_data = {\n",
        "#         \"Timestamp\": metrics[\"timestamp\"],\n",
        "#         \"Agreement Rate\": metrics[\"agreement_rate\"],\n",
        "#         \"Average Sentiment Score\": metrics[\"avg_sentiment\"],\n",
        "#         \"Feedback Quality\": metrics[\"feedback_quality\"],\n",
        "#         \"Average Response Time\": metrics[\"avg_response_time\"],\n",
        "#         \"Corpus BLEU\": metrics[\"BLEU\"],\n",
        "#         \"ROUGE\": metrics[\"ROUGE\"],\n",
        "#         \"METEOR\": metrics[\"METEOR\"],\n",
        "#         \"BERTScore\": metrics[\"BERTScore\"],\n",
        "#         \"G-Eval\": metrics[\"G-Eval\"],\n",
        "#         \"NLI\": metrics[\"NLI\"],\n",
        "#         \"QAG\": metrics[\"QAG\"],\n",
        "#         \"TTR\": metrics[\"TTR\"],  # Add TTR\n",
        "#         \"Pause Ratio\": metrics[\"Pause Ratio\"],  # Add Pause Ratio\n",
        "#         \"Average Turn Length\": metrics[\"Average Turn Length\"],  # Add Average Turn Length\n",
        "#         \"MAUVE\": metrics[\"MAUVE\"],  # Add MAUVE Score\n",
        "#         \"USL-H\": metrics[\"USL-H\"],  # Add USL-H\n",
        "#         \"Reward Score\": metrics[\"reward_score\"],  # Include reward/penalty\n",
        "#         \"Conversation History\": json.dumps(conversation_history)  # Store entire conversation as JSON\n",
        "#     }\n",
        "\n",
        "#     try:\n",
        "#         # Load existing file if it exists\n",
        "#         if os.path.isfile(filename):\n",
        "#             existing_df = pd.read_csv(filename)\n",
        "#             new_row_df = pd.DataFrame([metrics_data]).dropna(how=\"all\")\n",
        "#             updated_df = pd.concat([existing_df, new_row_df], ignore_index=True).drop_duplicates(keep='last')\n",
        "#         else:\n",
        "#             updated_df = pd.DataFrame([metrics_data]).dropna(how=\"all\")\n",
        "\n",
        "#         # Save back to CSV\n",
        "#         updated_df.to_csv(filename, index=False)\n",
        "#         print(f\"DEBUG: Metrics and conversation history saved to {filename} successfully.\")\n",
        "\n",
        "#         metrics_saved = True  # Mark as saved to prevent duplicates\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"DEBUG: Error saving metrics to CSV: {e}\")\n",
        "\n",
        "\n",
        "# Function to end the game, calculate final reward/penalty, save the conversation, and provide feedback\n",
        "def end_game():\n",
        "    global messages, conversation_history, initial_salary, final_salary\n",
        "\n",
        "    # Generate a unique conversation ID\n",
        "    conversation_id = generate_conversation_id()\n",
        "\n",
        "    # Calculate the final reward or penalty for the negotiation session\n",
        "    reward = calculate_reward()\n",
        "\n",
        "    # Append this reward information as a system message in the conversation history\n",
        "    conversation_history.append({\n",
        "        \"role\": \"system\",\n",
        "        \"reward_score\": reward,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    })\n",
        "\n",
        "    # Define a prompt to get feedback on the completed negotiation\n",
        "    feedback_prompt = (\n",
        "        \"Based on the negotiation that just ended, provide a detailed evaluation. \"\n",
        "        \"Mention what went well, what could have been improved, and offer suggestions \"\n",
        "        \"for both the buyer and the seller to help them improve in the future.\"\n",
        "    )\n",
        "\n",
        "    # Generate feedback from the assistant using OpenAI's API and the feedback prompt\n",
        "    try:\n",
        "        feedback_response = openai.ChatCompletion.create(\n",
        "            model=\"ft:gpt-4o-2024-08-06:llm-sim:salary-negotiation:AQN5Azuo\",\n",
        "            messages=messages + [{\"role\": \"user\", \"content\": feedback_prompt}],\n",
        "            max_tokens=1000\n",
        "        )\n",
        "        # Extract the feedback content from the response\n",
        "        feedback = feedback_response['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG: Error generating feedback - {e}\")\n",
        "        feedback = \"Could not generate feedback due to an error.\"\n",
        "\n",
        "    # Generate audio from the feedback\n",
        "    audio_feedback_filename = text_to_audio(feedback, filename=\"feedback_audio.mp3\")\n",
        "\n",
        "    # Append the final reward/penalty score to the feedback\n",
        "    feedback += f\"\\n\\n**Final Reward/Penalty Score:** {reward}\"\n",
        "\n",
        "    # Call the evaluate_model function to get only the summary\n",
        "    try:\n",
        "        summary = evaluate_model(conversation_id)\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG: Error evaluating model - {e}\")\n",
        "        summary = \"Evaluation could not be completed due to an error.\"\n",
        "\n",
        "    # Call the function to save conversation history in the specified JSONL format\n",
        "    try:\n",
        "        save_jsonl_to_bigquery(conversation_history)\n",
        "        print(\"DEBUG: JSONL file saved successfully at end of game.\")\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG: Error saving conversation history - {e}\")\n",
        "\n",
        "    # Format initial and final salary for output\n",
        "    formatted_initial_salary = f\"${initial_salary:,.2f}\" if initial_salary else \"Not set\"\n",
        "    formatted_final_salary = f\"${final_salary:,.2f}\" if final_salary else \"No final salary set yet\"\n",
        "\n",
        "    # Debugging final outputs\n",
        "    print(f\"DEBUG: Final Reward: {reward}\")\n",
        "    print(f\"DEBUG: Feedback: {feedback}\")\n",
        "    print(f\"DEBUG: Initial Salary: {formatted_initial_salary}, Final Salary: {formatted_final_salary}\")\n",
        "\n",
        "    # Return Gradio-compatible outputs\n",
        "    return format_chat_history(), formatted_initial_salary, formatted_final_salary, '', feedback, audio_feedback_filename\n",
        "\n",
        "def calculate_bertscore(reference, prediction):\n",
        "    # BERTScore expects lists of references and predictions\n",
        "    P, R, F1 = bert_score([prediction], [reference], lang=\"en\")\n",
        "    return F1.mean().item()  # Return the average F1 score\n",
        "\n",
        "def evaluate_retrieval(query, retrieved_contexts, ground_truth):\n",
        "    \"\"\"\n",
        "    Evaluate the effectiveness of the retrieval mechanism.\n",
        "    Parameters:\n",
        "    - query (str): The user query.\n",
        "    - retrieved_contexts (list of str): Contexts retrieved by the RAG system.\n",
        "    - ground_truth (str): The expected or ideal reference response.\n",
        "    Returns:\n",
        "    - dict: A dictionary containing metrics (accuracy, coverage, relevance, novelty).\n",
        "    \"\"\"\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    ground_truth_embedding = model.encode(ground_truth, convert_to_tensor=True)\n",
        "    retrieved_embeddings = [model.encode(context, convert_to_tensor=True) for context in retrieved_contexts]\n",
        "\n",
        "    # Calculate similarity scores\n",
        "    ground_truth_similarity = [util.pytorch_cos_sim(ground_truth_embedding, emb).item() for emb in retrieved_embeddings]\n",
        "    query_similarity = [util.pytorch_cos_sim(query_embedding, emb).item() for emb in retrieved_embeddings]\n",
        "\n",
        "    # Define metrics\n",
        "    retrieval_accuracy = any(score > 0.8 for score in ground_truth_similarity)\n",
        "    coverage = len(retrieved_contexts) / len(ground_truth.split()) if ground_truth else 0\n",
        "    relevance = np.mean(query_similarity) if query_similarity else 0\n",
        "    novelty = np.mean([1 - score for score in ground_truth_similarity]) if ground_truth_similarity else 0\n",
        "\n",
        "    return {\n",
        "        \"Retrieval Accuracy\": retrieval_accuracy,\n",
        "        \"Coverage\": coverage,\n",
        "        \"Relevance\": relevance,\n",
        "        \"Novelty\": novelty\n",
        "    }\n",
        "\n",
        "\n",
        "# Modify evaluate_model function\n",
        "def evaluate_model(conversation_id=None, references=None):\n",
        "    print(\"Evaluating model...\")\n",
        "\n",
        "    # Check if references are provided; if not, default to an empty list\n",
        "    if references is None:\n",
        "        references = []\n",
        "\n",
        "    # Ensure 'references' is a list of strings\n",
        "    string_references = []\n",
        "    for ref in references:\n",
        "        if isinstance(ref, list):\n",
        "            # If a reference is a list, join its elements into a single string\n",
        "            string_references.append(\" \".join(ref))\n",
        "        elif isinstance(ref, str):\n",
        "            string_references.append(ref)\n",
        "\n",
        "    # Combine reference texts for evaluation\n",
        "    combined_reference_text = \" \".join(string_references)\n",
        "\n",
        "    # Calculate the number of successful negotiations\n",
        "    successful_negotiations = sum(\n",
        "        1 for message in conversation_history if \"content\" in message and (\"agree\" in message[\"content\"].lower() or \"deal\" in message[\"content\"].lower())\n",
        "    )\n",
        "\n",
        "    # Calculate the total number of negotiations\n",
        "    total_negotiations = len([message for message in conversation_history if message[\"role\"] == \"user\"])\n",
        "    agreement_rate = successful_negotiations / total_negotiations if total_negotiations > 0 else 0\n",
        "\n",
        "    # Calculate the average sentiment of assistant messages\n",
        "    sentiment_scores = [\n",
        "        TextBlob(message[\"content\"]).sentiment.polarity\n",
        "        for message in conversation_history if message[\"role\"] == \"assistant\"\n",
        "    ]\n",
        "    avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0\n",
        "    feedback_quality = \"High\" if avg_sentiment > 0.3 else \"Moderate\" if avg_sentiment > 0 else \"Low\"\n",
        "\n",
        "    # File contribution metrics\n",
        "    total_contributions = sum(contribution_metrics.values())\n",
        "    file_contributions = {\n",
        "        file: (count / total_contributions) * 100 if total_contributions > 0 else 0\n",
        "        for file, count in contribution_metrics.items()\n",
        "    }\n",
        "    contribution_str = \"\\n\".join([f\"{file}: {contribution:.2f}%\" for file, contribution in file_contributions.items()])\n",
        "\n",
        "    # Average response time for assistant replies\n",
        "    avg_response_time = np.mean([\n",
        "        (datetime.fromisoformat(conversation_history[i + 1][\"timestamp\"]) - datetime.fromisoformat(message[\"timestamp\"])).total_seconds()\n",
        "        for i, message in enumerate(conversation_history[:-1])\n",
        "        if message[\"role\"] == \"user\" and \"timestamp\" in message and \"timestamp\" in conversation_history[i + 1]\n",
        "    ]) if len(conversation_history) > 1 else 0\n",
        "\n",
        "    # Initialize lists for new metrics\n",
        "    bleu_scores, rouge_scores, meteor_scores = [], [], []\n",
        "    bert_scores, g_eval_scores = [], []\n",
        "    nli_scores, qag_scores = [], []\n",
        "    ttr_scores, pause_ratios = [], []\n",
        "    avg_turn_lengths = []\n",
        "\n",
        "    # Calculate new metrics for assistant responses\n",
        "    assistant_responses = [msg for msg in conversation_history if msg[\"role\"] == \"assistant\"]\n",
        "    user_messages = [msg for msg in conversation_history if msg[\"role\"] == \"user\"]\n",
        "\n",
        "    for i, assistant_response in enumerate(assistant_responses):\n",
        "        if i < len(user_messages):\n",
        "            prediction = assistant_response[\"content\"]\n",
        "\n",
        "            # Textual Accuracy Metrics\n",
        "            bleu = corpus_bleu_eq([combined_reference_text], [prediction])\n",
        "            rouge = rouge_eq(combined_reference_text, prediction)\n",
        "            meteor = meteor_eq(combined_reference_text, prediction)\n",
        "            bleu_scores.append(bleu)\n",
        "            rouge_scores.append(rouge)\n",
        "            meteor_scores.append(meteor)\n",
        "\n",
        "            # Semantic Coherence Metrics\n",
        "            bert = calculate_bertscore(combined_reference_text, prediction)\n",
        "            g_eval = ge_val(combined_reference_text, prediction)\n",
        "            bert_scores.append(bert)\n",
        "            g_eval_scores.append(g_eval)\n",
        "\n",
        "            # Factual Accuracy Metrics\n",
        "            nli = nli_score(combined_reference_text, prediction)\n",
        "            qag = qag_score(combined_reference_text, prediction)\n",
        "            nli_scores.append(nli)\n",
        "            qag_scores.append(qag)\n",
        "\n",
        "            # Conversational Metrics\n",
        "            ttr = calculate_ttr(prediction)\n",
        "            pause_ratio = calculate_pause_ratio(prediction)\n",
        "            avg_turn_length = len(word_tokenize(prediction))\n",
        "            ttr_scores.append(ttr)\n",
        "            pause_ratios.append(pause_ratio)\n",
        "            avg_turn_lengths.append(avg_turn_length)\n",
        "\n",
        "    # Average Conversational Metrics\n",
        "    avg_ttr = np.mean(ttr_scores) if ttr_scores else 0\n",
        "    avg_pause_ratio = np.mean(pause_ratios) if pause_ratios else 0\n",
        "    avg_turn_length = np.mean(avg_turn_lengths) if avg_turn_lengths else 0\n",
        "\n",
        "    # MAUVE Metric\n",
        "    try:\n",
        "        # Ensure valid human and model texts before calculation\n",
        "        if not references or not assistant_responses:\n",
        "            print(\"DEBUG: Missing human or model texts for MAUVE calculation.\")\n",
        "            mauve_score = 0\n",
        "        else:\n",
        "            mauve_score = calculate_mauve_score(\n",
        "                human_texts=references,\n",
        "                model_texts=[msg[\"content\"] for msg in assistant_responses]\n",
        "            ) or 0  # Ensure a fallback to 0\n",
        "    except Exception as e:\n",
        "        print(f\"MAUVE calculation error: {e}\")\n",
        "        mauve_score = 0\n",
        "\n",
        "    # USL-H Metric\n",
        "    usl_h_scores = [\n",
        "        calculate_usl_h(nli, bert, sentiment)\n",
        "        for nli, bert, sentiment in zip(nli_scores, bert_scores, sentiment_scores)\n",
        "    ]\n",
        "    avg_usl_h = np.mean(usl_h_scores) if usl_h_scores else 0\n",
        "\n",
        "    # Calculate averages for existing metrics\n",
        "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0\n",
        "    avg_rouge = np.mean(rouge_scores) if rouge_scores else 0\n",
        "    avg_meteor = np.mean(meteor_scores) if meteor_scores else 0\n",
        "    avg_bert = np.mean(bert_scores) if bert_scores else 0\n",
        "    avg_g_eval = np.mean(g_eval_scores) if g_eval_scores else 0\n",
        "    avg_nli = np.mean(nli_scores) if nli_scores else 0\n",
        "    avg_qag = np.mean(qag_scores) if qag_scores else 0\n",
        "\n",
        "    # Create a dictionary to hold all metrics\n",
        "    metrics_dict = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"agreement_rate\": agreement_rate,\n",
        "        \"avg_sentiment\": avg_sentiment,\n",
        "        \"feedback_quality\": feedback_quality,\n",
        "        \"avg_response_time\": avg_response_time,\n",
        "        \"BLEU\": avg_bleu,\n",
        "        \"ROUGE\": avg_rouge,\n",
        "        \"METEOR\": avg_meteor,\n",
        "        \"BERTScore\": avg_bert,\n",
        "        \"G-Eval\": avg_g_eval,\n",
        "        \"NLI\": avg_nli,\n",
        "        \"QAG\": avg_qag,\n",
        "        \"TTR\": avg_ttr,\n",
        "        \"Pause Ratio\": avg_pause_ratio,\n",
        "        \"Average Turn Length\": avg_turn_length,\n",
        "        \"MAUVE\": mauve_score,\n",
        "        \"USL-H\": avg_usl_h,\n",
        "        \"conversation_id\": conversation_id,  # Unique identifier for each conversation\n",
        "        \"reward_score\": calculate_reward()  # Include final reward/penalty score\n",
        "    }\n",
        "\n",
        "    # Save the metrics and conversation to CSV\n",
        "    save_metrics_and_conversation_to_bigquery(conversation_id, metrics_dict, conversation_history)\n",
        "\n",
        "    # Create a summary string in the desired format\n",
        "    summary_string = (\n",
        "        f\"Model Evaluation Metrics:\\n\"\n",
        "        f\"Agreement Rate: {agreement_rate:.2f}\\n\"\n",
        "        f\"Average Sentiment Score: {avg_sentiment:.2f}\\n\"\n",
        "        f\"Feedback Quality: {feedback_quality}\\n\"\n",
        "        f\"Average Response Time: {avg_response_time:.2f}s\\n\\n\"\n",
        "        f\"Textual Accuracy - Corpus BLEU: {avg_bleu:.3f}, ROUGE: {avg_rouge:.3f}, METEOR: {avg_meteor:.3f}\\n\"\n",
        "        f\"Semantic Coherence - BERTScore: {avg_bert:.3f}, GEval: {avg_g_eval:.2f}\\n\"\n",
        "        f\"Factual Accuracy - NLI: {avg_nli:.2f}, QAG: {avg_qag:.2f}\\n\\n\"\n",
        "        f\"Conversational Metrics:\\n\"\n",
        "        f\"- Type-Token Ratio (TTR): {avg_ttr:.3f}\\n\"\n",
        "        f\"- Pause Ratio: {avg_pause_ratio:.3f}\\n\"\n",
        "        f\"- Average Turn Length: {avg_turn_length:.2f} words\\n\"\n",
        "        f\"- MAUVE Score: {mauve_score:.3f}\\n\"\n",
        "        f\"- USL-H Score: {avg_usl_h:.3f}\\n\\n\"\n",
        "        f\"File Contributions:\\n{contribution_str}\"\n",
        "    )\n",
        "\n",
        "    # Return only the summary string in the requested format\n",
        "    return summary_string\n",
        "\n",
        "\n",
        "\n",
        "# Function to reload files from the specified folder, excluding any listed files\n",
        "def update_loaded_files(exclude_files):\n",
        "    # Call the function to load PDFs, excluding any files specified in the 'exclude_files' string\n",
        "    # 'exclude_files' is a comma-separated string of filenames to exclude, so we split it into a list\n",
        "    load_pdfs_from_folder(\"RAG\", exclude_files=exclude_files.split(\",\"))\n",
        "\n",
        "    # Return a confirmation message with the updated list of loaded files\n",
        "    return f\"Updated file list: {', '.join(file_list)}\"\n",
        "\n",
        "\n",
        "\n",
        "# Textual Accuracy Metrics\n",
        "def corpus_bleu_eq(references, predictions):\n",
        "    tokenized_references = [[ref.split()] for ref in references]  # Corpus BLEU expects a list of lists of references\n",
        "    tokenized_predictions = [pred.split() for pred in predictions]\n",
        "    return corpus_bleu(tokenized_references, tokenized_predictions, smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "def rouge_eq(reference, prediction):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(prediction, reference)\n",
        "    return scores[0]['rouge-l']['f']\n",
        "\n",
        "def meteor_eq(reference, prediction):\n",
        "    # Use TreebankWordTokenizer instead of word_tokenize\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    tokenized_reference = tokenizer.tokenize(reference)\n",
        "    tokenized_prediction = tokenizer.tokenize(prediction)\n",
        "    return meteor_score.meteor_score([tokenized_reference], tokenized_prediction)\n",
        "\n",
        "# Function to evaluate model's response based on Textual Accuracy\n",
        "def calculate_textual_metrics(reference, prediction):\n",
        "    bleu = corpus_bleu_eq([reference], [prediction])  # Using list of single references for corpus BLEU\n",
        "    rouge = rouge_eq(reference, prediction)\n",
        "    meteor = meteor_eq(reference, prediction)\n",
        "    return bleu, rouge, meteor\n",
        "\n",
        "# Set up the Gradio interface layout and styling\n",
        "with gr.Blocks(css=\".gradio-container {max-width: 1000px; margin: auto; background-color: #1e1e1e; color: white; border-radius: 10px; padding: 20px;}\") as demo:\n",
        "    # Create a Markdown header for the interface title\n",
        "    gr.Markdown(\"## 🤝🤖 Salary Negotiation Assistant - Chat-Like Interface\", elem_id=\"title\")\n",
        "\n",
        "    # Define a row layout for the input and output sections\n",
        "    with gr.Row():\n",
        "        # Create a column for user inputs with a scale of 3\n",
        "        with gr.Column(scale=3):\n",
        "            # Define text inputs for user name, company, and position\n",
        "            name_input = gr.Textbox(lines=1, placeholder=\"Enter your name...\", label=\"Your Name\")\n",
        "            company_name_input = gr.Textbox(lines=1, placeholder=\"Enter your company name...\", label=\"Company Name\")\n",
        "            position_input = gr.Textbox(lines=1, placeholder=\"Enter your position...\", label=\"Your Position\")\n",
        "\n",
        "            # Radio buttons for the user to choose their role (employee or employer)\n",
        "            role_input = gr.Radio(choices=[\"employee\", \"employer\"], label=\"Choose your role\", interactive=True)\n",
        "\n",
        "            # Input for initial salary\n",
        "            initial_salary_input = gr.Textbox(lines=1, placeholder=\"Enter your current salary...\", label=\"Current Salary\")\n",
        "\n",
        "            # Button to start the negotiation game\n",
        "            start_button = gr.Button(\"Start Game\")\n",
        "\n",
        "            # Input for the negotiation message\n",
        "            user_text_input = gr.Textbox(lines=3, placeholder=\"Enter your negotiation message here...\", label=\"Your Message\")\n",
        "            user_audio_input = gr.Audio(label=\"Your Audio Message\", type=\"filepath\")\n",
        "\n",
        "\n",
        "            # Button to submit negotiation messages\n",
        "            negotiate_button = gr.Button(\"Negotiate\")\n",
        "\n",
        "            # Input to specify files to exclude from document loading\n",
        "            exclude_files_input = gr.Textbox(lines=1, placeholder=\"Enter file names to exclude, separated by commas\", label=\"Exclude Files\")\n",
        "\n",
        "            # Button to reload files based on exclusions\n",
        "            reload_button = gr.Button(\"Reload Files\")\n",
        "\n",
        "            # Button to end the negotiation game\n",
        "            end_button = gr.Button(\"End Game\")\n",
        "\n",
        "            # Button to evaluate model performance\n",
        "            evaluate_button = gr.Button(\"Evaluate Model\")\n",
        "\n",
        "        # Create a larger column (scale 7) for outputs\n",
        "        with gr.Column(scale=7):\n",
        "            # Chatbot display for conversation history\n",
        "            chat_output = gr.Chatbot(label=\"Chat History\", show_label=False, value=[], height=400)\n",
        "\n",
        "            audio_output = gr.Audio(label=\"Assistant Audio Response\", interactive=False, type=\"filepath\")\n",
        "\n",
        "            # Display for negotiation feedback from the assistant\n",
        "            feedback_output = gr.Textbox(label=\"Negotiation Feedback (Feedback can be inaccurate)\", lines=5, interactive=False)\n",
        "\n",
        "            # Display for showing model evaluation metrics\n",
        "            evaluate_output = gr.Textbox(label=\"Model Evaluation Metrics\", lines=8, interactive=False)\n",
        "\n",
        "            retrieval_metrics_output = gr.Textbox(label=\"Retrieval Metrics\", interactive=False, lines=4)\n",
        "\n",
        "            # Display the list of loaded files\n",
        "            file_list_output = gr.Textbox(label=\"Loaded Files\", lines=3, interactive=False)\n",
        "\n",
        "            # Displays for initial and final salary values, and the negotiation score\n",
        "            initial_salary_output = gr.Textbox(label=\"Initial Salary\", value=\"\", interactive=False)\n",
        "            final_salary_output = gr.Textbox(label=\"Final Salary\", interactive=False)\n",
        "            score_output = gr.Textbox(label=\"Negotiation Score\", interactive=False)\n",
        "\n",
        "        # Define actions for buttons, linking them to functions and specifying inputs and outputs\n",
        "        start_button.click(\n",
        "            fn=start_game,\n",
        "            inputs=[role_input, initial_salary_input, name_input, company_name_input, position_input],\n",
        "            outputs=[chat_output, initial_salary_output]\n",
        "        )\n",
        "\n",
        "        negotiate_button.click(\n",
        "            fn=negotiate,\n",
        "            inputs=[user_text_input, user_audio_input, name_input, company_name_input, position_input, gr.State(assistant_role)],\n",
        "            outputs=[chat_output, initial_salary_output, final_salary_output, score_output, feedback_output, retrieval_metrics_output, audio_output]\n",
        "        )\n",
        "\n",
        "        reload_button.click(\n",
        "            fn=update_loaded_files,\n",
        "            inputs=exclude_files_input,\n",
        "            outputs=file_list_output\n",
        "        )\n",
        "\n",
        "        end_button.click(\n",
        "            fn=end_game,\n",
        "            inputs=None,\n",
        "            outputs=[chat_output, initial_salary_output, final_salary_output, score_output, feedback_output, audio_output]\n",
        "        )\n",
        "\n",
        "        evaluate_button.click(\n",
        "            fn=lambda assistant_role: evaluate_model(\n",
        "                conversation_id=generate_conversation_id(),\n",
        "                references=load_reference_text(assistant_role)[0]  # Pass only the list of references\n",
        "        ),\n",
        "        inputs=gr.State(assistant_role),\n",
        "        outputs=evaluate_output\n",
        "    )\n",
        "\n",
        "\n",
        "    # Initialize the file list output with the current files loaded\n",
        "    file_list_output.value = \", \".join(file_list)\n",
        "\n",
        "# Launch the Gradio interface in queued mode to handle multiple inputs\n",
        "demo.queue().launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vxb19Ep97orI",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1733740383553,
          "user_tz": 480,
          "elapsed": 141985,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "24af7fd7-7478-4238-e662-c32f30ea3e89"
      },
      "id": "vxb19Ep97orI",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 9 PDFs and created FAISS index successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://14f70d04e89067c25a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://14f70d04e89067c25a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: User role set to 'prospective hire', Assistant role set to 'employer'.\n",
            "Starting Salary for Negotiation: $98,000.00\n",
            "Conversation history before formatting: []\n",
            "DEBUG: Starting negotiation with user_input: I want to negotiate\n",
            "DEBUG: Initial conversation_history: []\n",
            "ERROR: Invalid assistant role: . Defaulting to 'employer'.\n",
            "DEBUG: Successfully loaded reference_employer.txt from bucket lilygpt\n",
            "DEBUG: No salary pattern found in text: 'I want to negotiate'\n",
            "DEBUG: Is the user message salary-related? False\n",
            "DEBUG: No salary pattern found in text: 'I want to negotiate'\n",
            "DEBUG: Extracted salary from message 'I want to negotiate': None\n",
            "DEBUG: No valid salary detected. Retaining last final salary.\n",
            "DEBUG: Current Salary Log: []\n",
            "DEBUG: Initial Salary: 98000.0, Final Salary: None\n",
            "DEBUG: No valid salary found in user's message.\n",
            "DEBUG: Detected user tone: neutral\n",
            "DEBUG: Try Counter: 1\n",
            "DEBUG: Retrieval Metrics: {'Retrieval Accuracy': False, 'Coverage': 0.005338078291814947, 'Relevance': 0.7215192119280497, 'Novelty': 0.6905341098705927}\n",
            "DEBUG: Retrieved context from RAG: Negotiate how you’ll negotiate Would you be willing to negotiate?”\n",
            "10 Negotiate contract terms\n",
            "DEBUG: Assistant response generated: Absolutely, Arish. Let's dive into it. What are your thoughts on the offer so far? We're keen to hear your perspective and see how we can align.\n",
            "ERROR: Failed to generate audio: type object 'Audio' has no attribute 'speech'\n",
            "DEBUG: No salary pattern found in text: 'Absolutely, Arish. Let's dive into it. What are your thoughts on the offer so far? We're keen to hear your perspective and see how we can align.'\n",
            "DEBUG: Extracted salary from message 'Absolutely, Arish. Let's dive into it. What are your thoughts on the offer so far? We're keen to hear your perspective and see how we can align.': None\n",
            "DEBUG: No valid salary detected. Retaining last final salary.\n",
            "DEBUG: Current Salary Log: []\n",
            "DEBUG: Initial Salary: 98000.0, Final Salary: None\n",
            "DEBUG: Final salary after assistant's response: None\n",
            "DEBUG: Detected assistant tone: neutral\n",
            "DEBUG: Loaded 14 references for role employer.\n",
            "DEBUG: Raw NLI result: [{'label': 'NEUTRAL', 'score': 0.9887664914131165}]\n",
            "DEBUG: Extracted entailment score: 0\n",
            "DEBUG: QAG Score - Reference: Default fallback reference response, Prediction: Absolutely, Arish. Let's dive into it. What are your thoughts on the offer so far? We're keen to hear your perspective and see how we can align.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Updated conversation_history: [{'role': 'user', 'content': 'I want to negotiate', 'tone': 'neutral', 'timestamp': '2024-12-09T10:31:29.135677'}, {'role': 'assistant', 'content': \"Absolutely, Arish. Let's dive into it. What are your thoughts on the offer so far? We're keen to hear your perspective and see how we can align.\", 'tone': 'neutral', 'timestamp': '2024-12-09T10:31:32.675097', 'bleu': 0, 'rouge': 0.0, 'meteor': 0.0, 'bert_score': 0.8209970593452454, 'coherence_score': -0.05487442389130592, 'nli_score': 0, 'qag_score': 0.20730309188365936}]\n",
            "DEBUG: Initial or final salary not set. Reward calculation skipped.\n",
            "DEBUG: Calculated reward: 0\n",
            "Conversation history before formatting: [{'role': 'user', 'content': 'I want to negotiate', 'tone': 'neutral', 'timestamp': '2024-12-09T10:31:29.135677'}, {'role': 'assistant', 'content': \"Absolutely, Arish. Let's dive into it. What are your thoughts on the offer so far? We're keen to hear your perspective and see how we can align.\", 'tone': 'neutral', 'timestamp': '2024-12-09T10:31:32.675097', 'bleu': 0, 'rouge': 0.0, 'meteor': 0.0, 'bert_score': 0.8209970593452454, 'coherence_score': -0.05487442389130592, 'nli_score': 0, 'qag_score': 0.20730309188365936}]\n",
            "Processing message: {'role': 'user', 'content': 'I want to negotiate', 'tone': 'neutral', 'timestamp': '2024-12-09T10:31:29.135677'}\n",
            "Processing message: {'role': 'assistant', 'content': \"Absolutely, Arish. Let's dive into it. What are your thoughts on the offer so far? We're keen to hear your perspective and see how we can align.\", 'tone': 'neutral', 'timestamp': '2024-12-09T10:31:32.675097', 'bleu': 0, 'rouge': 0.0, 'meteor': 0.0, 'bert_score': 0.8209970593452454, 'coherence_score': -0.05487442389130592, 'nli_score': 0, 'qag_score': 0.20730309188365936}\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://14f70d04e89067c25a.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "LillyGPT"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}