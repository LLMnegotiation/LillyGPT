{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6030ce58-13fd-4bf4-9eeb-93b1fd972910",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.11/site-packages (0.28.0)\n",
      "Requirement already satisfied: gradio in /opt/anaconda3/lib/python3.11/site-packages (5.4.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: vaderSentiment in /opt/anaconda3/lib/python3.11/site-packages (3.3.2)\n",
      "Requirement already satisfied: textblob in /opt/anaconda3/lib/python3.11/site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: PyMuPDF in /opt/anaconda3/lib/python3.11/site-packages (1.24.13)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/lib/python3.11/site-packages (1.9.0)\n",
      "Requirement already satisfied: rouge in /opt/anaconda3/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: bert-score in /opt/anaconda3/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.5.0)\n",
      "Collecting gtts\n",
      "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting SpeechRecognition\n",
      "  Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: pydub in /opt/anaconda3/lib/python3.11/site-packages (0.25.1)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from openai) (3.9.3)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (0.115.2)\n",
      "Requirement already satisfied: ffmpy in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.4.2 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (1.4.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (0.26.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.1 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (0.26.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (3.0.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (3.10.7)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (10.2.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: python-multipart==0.0.12 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (0.0.12)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (0.7.0)\n",
      "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (0.1.1)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (0.40.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio) (0.32.0)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from gradio-client==1.4.2->gradio) (2023.10.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /opt/anaconda3/lib/python3.11/site-packages (from gradio-client==1.4.2->gradio) (12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.11/site-packages (from rouge) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.11/site-packages (from bert-score) (3.8.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai) (2.0.7)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.1.tar.gz (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading thinc-8.3.0.tar.gz (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting spacy\n",
      "  Using cached spacy-3.8.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "  Downloading spacy-3.7.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (1.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->bert-score) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->bert-score) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->bert-score) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->bert-score) (3.0.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Downloading spacy-3.7.5-cp311-cp311-macosx_11_0_arm64.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.5-cp311-cp311-macosx_11_0_arm64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
      "Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl (32.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: blis, SpeechRecognition, gtts, thinc, spacy\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 1.0.1\n",
      "    Uninstalling blis-1.0.1:\n",
      "      Successfully uninstalled blis-1.0.1\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.3.2\n",
      "    Uninstalling thinc-8.3.2:\n",
      "      Successfully uninstalled thinc-8.3.2\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.8.2\n",
      "    Uninstalling spacy-3.8.2:\n",
      "      Successfully uninstalled spacy-3.8.2\n",
      "Successfully installed SpeechRecognition-3.11.0 blis-0.7.11 gtts-2.5.4 spacy-3.7.5 thinc-8.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install openai gradio pandas numpy vaderSentiment textblob PyMuPDF transformers sentence-transformers faiss-cpu rouge bert-score nltk spacy torch gtts SpeechRecognition pydub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071e423c-eb7b-4522-8fd9-c01a43be85e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/arish/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/arish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/arish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/arish/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "/opt/anaconda3/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/gradio/components/chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs uploaded and processed successfully.\n",
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import gradio as gr\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import fitz  # PyMuPDF for working with PDFs\n",
    "from transformers import pipeline\n",
    "from transformers import RobertaTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re  # For detecting and extracting salary information\n",
    "# Metrics\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "from rouge import Rouge\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate import meteor_score\n",
    "from nltk.tokenize import word_tokenize  # Import for tokenizing\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import spacy \n",
    "import mauve \n",
    "import torch \n",
    "\n",
    "from gtts import gTTS \n",
    "import speech_recognition as sr \n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "\n",
    "def audio_to_text(audio_file):\n",
    "    \"\"\"\n",
    "    Convert an audio file to text using SpeechRecognition.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    try:\n",
    "        # Convert audio file to WAV format (if necessary)\n",
    "        audio = AudioSegment.from_file(audio_file)\n",
    "        audio.export(\"temp_audio.wav\", format=\"wav\")\n",
    "        with sr.AudioFile(\"temp_audio.wav\") as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            print(f\"DEBUG: Converted audio to text: {text}\")\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to process audio input: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def text_to_audio(response_text, filename=\"response_audio.mp3\"):\n",
    "    try:\n",
    "        # Remove old audio file if it exists\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        # Generate speech from the text\n",
    "        tts = gTTS(response_text, lang='en')\n",
    "        tts.save(filename)\n",
    "        print(f\"Audio saved as {filename}\")\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating audio: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load the tokenizer for truncation\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "\n",
    "\n",
    "# Function to load reference text based on assistant role\n",
    "def load_reference_text(assistant_role):\n",
    "    \"\"\"\n",
    "    Load and combine the appropriate reference text file based on the assistant's role.\n",
    "    Returns both a list of individual lines and a single combined reference text.\n",
    "    \"\"\"\n",
    "    # Fallback for missing or invalid assistant_role\n",
    "    if not assistant_role or assistant_role not in [\"employer\", \"employee\"]:\n",
    "        print(f\"WARNING: assistant_role is missing or invalid. Defaulting to 'employer'.\")\n",
    "        assistant_role = \"employer\"\n",
    "\n",
    "    # Determine the reference file based on the assistant role\n",
    "    if assistant_role == \"employer\":\n",
    "        reference_file = \"reference_employer.txt\"\n",
    "    elif assistant_role == \"employee\":\n",
    "        reference_file = \"reference_employee.txt\"\n",
    "\n",
    "    # Load the reference file\n",
    "    try:\n",
    "        with open(reference_file, 'r', encoding='utf-8') as f:\n",
    "            # Read all lines, stripping whitespace\n",
    "            reference_lines = [line.strip() for line in f.readlines()]\n",
    "            \n",
    "            # Combine all lines into a single string for full-text metrics\n",
    "            combined_reference_text = \" \".join(reference_lines)\n",
    "            \n",
    "        print(f\"DEBUG: Loaded reference text from {reference_file}\")\n",
    "        return reference_lines, combined_reference_text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: {reference_file} not found. Ensure the file exists in the same directory.\")\n",
    "        return [], \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load reference text: {e}\")\n",
    "        return [], \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_reference_response(context, references):\n",
    "    \"\"\"\n",
    "    Fetch a reference response that best matches the given context from the list of references.\n",
    "    \"\"\"\n",
    "    # Simple implementation: Return the first reference (replace with more logic if needed)\n",
    "    for reference in references:\n",
    "        # Example logic: Check if certain keywords in context match those in a reference\n",
    "        if any(keyword in context.lower() for keyword in [\"salary\", \"offer\", \"negotiation\"]):  # Add your keywords\n",
    "            return reference\n",
    "    \n",
    "    # Fallback if no match found\n",
    "    return \"No suitable reference found. Default response.\"\n",
    "\n",
    "\n",
    "def generate_conversation_id():\n",
    "    \"\"\"Generate a unique identifier for each conversation session.\"\"\"\n",
    "    return f\"conv_{int(datetime.now().timestamp())}\"\n",
    "\n",
    "# Set up the API key for OpenAI (Note: this is sensitive information)\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# A list to keep track of all messages exchanged during the negotiation\n",
    "messages = []\n",
    "user_role = \"\"  # To identify if the user is negotiating as an employee or employer\n",
    "assistant_role = \"\"  # Role for the assistant, which will be the opposite of user_role\n",
    "\n",
    "# Creating input fields in Gradio for user personalization - for example, adding their name, company, and position\n",
    "name_input = gr.Textbox(lines=1, placeholder=\"Enter your name...\", label=\"Your Name\")\n",
    "company_name_input = gr.Textbox(lines=1, placeholder=\"Enter your company name...\", label=\"Company Name\")\n",
    "position_input = gr.Textbox(lines=1, placeholder=\"Enter your position...\", label=\"Your Position\")\n",
    "\n",
    "# Load spaCy model for Part-of-Speech tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# A list to save each negotiation round - this helps the assistant learn and improve over time\n",
    "conversation_history = []\n",
    "\n",
    "# Variables for tracking initial and final salary offers during the negotiation\n",
    "initial_salary = None\n",
    "final_salary = None\n",
    "\n",
    "# Global variables to track concessions and negotiation progression\n",
    "concession_count = 0\n",
    "rounds_without_concession = 0\n",
    "\n",
    "MAX_CONCESSIONS = 5  # Adjust this number based on how many concessions you want to allow\n",
    "\n",
    "# Summaries will go here to capture snapshots of the conversation as it progresses\n",
    "summaries = []\n",
    "\n",
    "# Initializing VADER for sentiment analysis - this will help us read the tone in messages\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to generate a quick summary of the conversation at certain intervals\n",
    "def summarize_conversation():\n",
    "    # Grab the last 10 messages to get a snapshot of the recent discussion\n",
    "    recent_conversation = conversation_history[-10:]\n",
    "    conversation_text = \" \".join([msg[\"content\"] for msg in recent_conversation if \"content\" in msg])\n",
    "\n",
    "    # Ask OpenAI to create a concise summary of these recent messages\n",
    "    summary_response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",  # (Optionally) replace with another model if needed\n",
    "        messages=[{\"role\": \"system\", \"content\": f\"Summarize the following negotiation progress in a concise and informative manner:\\n\\n{conversation_text}\\n\\nSummary:\"}],\n",
    "        max_tokens=300,\n",
    "        temperature=0.5  # Keeping temperature low for a more focused summary\n",
    "    )\n",
    "\n",
    "    # Save the summary and print it for feedback\n",
    "    summary = summary_response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    summaries.append(summary)\n",
    "    print(f\"New Summary Added: {summary}\")\n",
    "\n",
    "# FAISS Vector Store setup for Retrieval-Augmented Generation (RAG)\n",
    "# Using SentenceTransformer to convert text into embeddings for semantic search\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "index = None  # This will hold the FAISS index\n",
    "sentences = []  # List to store sentences from documents for retrieval\n",
    "file_list = []  # List of file names processed\n",
    "contribution_metrics = {}  # Tracking the \"contribution\" of each document in retrievals\n",
    "\n",
    "# Initialize the zero-shot classification pipeline for agreement detection\n",
    "agreement_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\") \n",
    "\n",
    "# Initialize a separate NLI model for the nli_score function\n",
    "nli_model_for_nli_score = pipeline(\"text-classification\", model=\"roberta-large-mnli\")\n",
    "\n",
    "\n",
    "# Initialize a Question Answering (QA) pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "def ge_val(reference, prediction):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between the prediction and the full reference text.\n",
    "    \"\"\"\n",
    "    combined_reference = \" \".join(reference) if isinstance(reference, list) else reference  # Combine references\n",
    "    embedding1 = model.encode(combined_reference, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(prediction, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "    return similarity_score.item()  # Return similarity score as float\n",
    "\n",
    "def nli_score(reference, prediction):\n",
    "    try:\n",
    "        max_tokens = 512\n",
    "        truncated_reference = reference[:max_tokens]\n",
    "        truncated_prediction = prediction[:max_tokens]\n",
    "        input_text = f\"{truncated_reference} [SEP] {truncated_prediction}\"\n",
    "        \n",
    "        result = nli_model_for_nli_score(input_text)\n",
    "        \n",
    "        # Debug: Print the result for inspection\n",
    "        print(f\"DEBUG: Raw NLI result: {result}\")\n",
    "        \n",
    "        entailment_label = \"ENTAILMENT\"  # Adjust based on your model's output\n",
    "        entailment_score = next((item['score'] for item in result if item['label'].upper() == entailment_label), 0)\n",
    "        print(f\"DEBUG: Extracted entailment score: {entailment_score}\")\n",
    "        \n",
    "        return entailment_score\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in nli_score: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def qag_score(reference, prediction):\n",
    "    \"\"\"\n",
    "    Use the reference as a question and the prediction as context to calculate a QAG score.\n",
    "    Handles cases where sequences are too short for truncation.\n",
    "    \"\"\"\n",
    "    # Step 3: Add debugging logs to monitor inputs\n",
    "    print(f\"DEBUG: QAG Score - Reference: {reference}, Prediction: {prediction}\")\n",
    "\n",
    "    # Step 2: Validate inputs and ensure they are long enough for processing\n",
    "    if len(reference.split()) < 3 or len(prediction.split()) < 3:\n",
    "        print(\"DEBUG: Inputs are too short for QAG scoring. Returning default value of 0.\")\n",
    "        return 0  # Step 4: Fallback for short sequences\n",
    "\n",
    "    try:\n",
    "        # Use the QA pipeline to calculate a score\n",
    "        result = qa_pipeline(\n",
    "            question=reference,\n",
    "            context=prediction,\n",
    "            max_length=min(len(reference.split()) + len(prediction.split()), 512)  # Dynamically adjust max_length\n",
    "        )\n",
    "        return result['score']  # Returns the confidence score for the answer\n",
    "    except Exception as e:\n",
    "        # Step 3: Log the exception for debugging\n",
    "        print(f\"ERROR: Exception occurred in QAG scoring - {e}\")\n",
    "        return 0  # Step 4: Return a fallback score if an error occurs \n",
    "\n",
    "def calculate_ttr(text):\n",
    "    \"\"\"\n",
    "    Calculate the Type-Token Ratio (TTR) of a given text.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
    "    types = set(words)  # Unique words\n",
    "    ttr = len(types) / len(words) if words else 0\n",
    "    return ttr\n",
    "\n",
    "def calculate_pause_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of conversational fillers or pauses in the text.\n",
    "    \"\"\"\n",
    "    pause_words = [\"um\", \"uh\", \"let's see\", \"hmm\", \"well\", \"you know\"]\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    pause_count = sum(1 for token in tokens if token in pause_words)\n",
    "    ratio = pause_count / len(tokens) if tokens else 0\n",
    "    return ratio\n",
    "\n",
    "def calculate_avg_turn_length(responses):\n",
    "    \"\"\"\n",
    "    Calculate the average length of responses in a conversation.\n",
    "    \"\"\"\n",
    "    turn_lengths = [len(word_tokenize(response[\"content\"])) for response in responses if response[\"role\"] == \"assistant\"]\n",
    "    avg_turn_length = sum(turn_lengths) / len(turn_lengths) if turn_lengths else 0\n",
    "    return avg_turn_length \n",
    "\n",
    "\n",
    "# Modify the calculate_mauve_score function\n",
    "def calculate_mauve_score(human_texts, model_texts):\n",
    "    if not human_texts or not model_texts:\n",
    "        print(\"ERROR: Text lists for MAUVE calculation are empty.\")\n",
    "        return None  # Return None for clarity\n",
    "\n",
    "    if not all(isinstance(ht, str) for ht in human_texts):\n",
    "        print(\"ERROR: `human_texts` must be a list of strings.\")\n",
    "        return None\n",
    "    if not all(isinstance(mt, str) for mt in model_texts):\n",
    "        print(\"ERROR: `model_texts` must be a list of strings.\")\n",
    "        return None\n",
    "\n",
    "    # Debugging inputs\n",
    "    print(f\"DEBUG: Number of human texts: {len(human_texts)}\")\n",
    "    print(f\"DEBUG: Number of model texts: {len(model_texts)}\")\n",
    "    print(f\"DEBUG: Sample human texts: {human_texts[:3]}\")\n",
    "    print(f\"DEBUG: Sample model texts: {model_texts[:3]}\")\n",
    "\n",
    "    # Detect device dynamically\n",
    "    device_id = 0 if torch.cuda.is_available() else -1\n",
    "    print(f\"DEBUG: Using device ID {device_id} for MAUVE calculation.\")\n",
    "\n",
    "    try:\n",
    "        mauve_result = mauve.compute_mauve(\n",
    "            p_text=model_texts,\n",
    "            q_text=human_texts,\n",
    "            device_id=device_id\n",
    "        )\n",
    "        print(f\"DEBUG: MAUVE Result Object: {mauve_result}\")\n",
    "        print(f\"DEBUG: MAUVE Score: {mauve_result.mauve}\")\n",
    "        return mauve_result.mauve  # Return only the MAUVE score\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to calculate MAUVE - {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_usl_h(nli_score, bert_score, sentiment):\n",
    "    \"\"\"\n",
    "    Calculate the USL-H metric based on NLI, BERTScore, and sentiment analysis.\n",
    "    \"\"\"\n",
    "    # Normalize scores to a [0, 1] range\n",
    "    u = max(0, min(nli_score, 1))\n",
    "    s = max(0, min(bert_score, 1))\n",
    "    l = max(0, min((sentiment + 1) / 2, 1))  # Scale sentiment [-1, 1] to [0, 1]\n",
    "    usl_h = (u + s + l) / 3  # Average of the three components\n",
    "    return usl_h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_salary(text):\n",
    "    \"\"\"\n",
    "    Extract salary amounts from text, ensuring they are valid and contextually relevant.\n",
    "    Handles hourly rates, salary ranges, and uses regex for initial capture, POS tagging for refinement,\n",
    "    and heuristics for filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Refined regex to match salary-related patterns, including ranges and hourly rates\n",
    "    salary_regex = r\"(?:salary|base pay|compensation|offer|starting at|starting around)?\\s*\\$?\\s*(\\d{1,3}(?:,\\d{3})*|\\d+(?:\\.\\d{2})?)\\s?([kKmM]?)(?:\\s?(?:to|-)\\s?\\$?\\s*(\\d{1,3}(?:,\\d{3})*|\\d+(?:\\.\\d{2})?)\\s?([kKmM]?)?)?\"\n",
    "    salary_matches = re.findall(salary_regex, text, re.IGNORECASE)\n",
    "\n",
    "    if not salary_matches:\n",
    "        print(f\"DEBUG: No salary pattern found in text: '{text}'\")\n",
    "        return None\n",
    "\n",
    "    # Keywords to identify salary context\n",
    "    salary_keywords = [\n",
    "        \"salary\", \"base pay\", \"annual compensation\", \"total compensation\", \"starting at\",\n",
    "        \"starting around\", \"per year\", \"yearly\", \"monthly salary\", \"hourly rate\", \"compensation package\",\n",
    "        \"offer\", \"wage\", \"pay rate\", \"income\", \"remuneration\"\n",
    "    ]\n",
    "\n",
    "    # Keywords to identify benefit context\n",
    "    benefit_keywords = [\n",
    "        \"401k\", \"403b\", \"retirement\", \"pension\", \"company match\", \"health plan\", \"insurance\",\n",
    "        \"bonus\", \"stock options\", \"PTO\", \"vacation\", \"benefits\", \"stipend\", \"wellness\",\n",
    "        \"career advancement\", \"RSU\", \"RSUs\", \"sign-on bonus\", \"commission\", \"equity\",\n",
    "        \"relocation\", \"housing allowance\", \"education reimbursement\", \"medical\", \"dental\", \"vision\"\n",
    "    ]\n",
    "\n",
    "    # Negative keywords to exclude non-salary amounts\n",
    "    negative_keywords = [\n",
    "        \"401k\", \"403b\", \"pension\", \"retirement\", \"stock\", \"rsu\", \"rsus\", \"bonus\",\n",
    "        \"benefit\", \"commission\", \"equity\", \"option\", \"incentive\", \"grant\", \"award\",\n",
    "        \"vesting\", \"shares\", \"stock grant\", \"days\", \"hours\", \"pto\", \"vacation days\"\n",
    "    ]\n",
    "\n",
    "    # Constants for converting hourly to annual salary\n",
    "    HOURS_PER_WEEK = 40\n",
    "    WEEKS_PER_YEAR = 52\n",
    "    ANNUAL_MULTIPLIER = HOURS_PER_WEEK * WEEKS_PER_YEAR\n",
    "\n",
    "    # List to hold valid salary values\n",
    "    salary_values = []\n",
    "    for match1, suffix1, match2, suffix2 in salary_matches:\n",
    "        # Handle single values or ranges\n",
    "        amounts = []\n",
    "        for match, suffix in [(match1, suffix1), (match2, suffix2)]:\n",
    "            if match:\n",
    "                try:\n",
    "                    amount = float(match.replace(\",\", \"\"))\n",
    "                    if suffix.lower() == 'k':\n",
    "                        amount *= 1000\n",
    "                    elif suffix.lower() == 'm':\n",
    "                        amount *= 1_000_000\n",
    "                    amounts.append(amount)\n",
    "                except ValueError:\n",
    "                    print(f\"DEBUG: Could not convert match '{match}' to a float.\")\n",
    "                    continue\n",
    "\n",
    "        # Select the largest value in the range\n",
    "        if amounts:\n",
    "            salary_amount = max(amounts)\n",
    "\n",
    "        # Check if hourly rate needs conversion\n",
    "        if \"hour\" in text.lower() and salary_amount < 500:  # Threshold to detect hourly rates\n",
    "            salary_amount *= ANNUAL_MULTIPLIER  # Convert to annual salary\n",
    "\n",
    "        # Debugging log for extracted amount\n",
    "        print(f\"DEBUG: Extracted amount: {salary_amount}, Context: '{text}'\")\n",
    "\n",
    "        # Scoring mechanism\n",
    "        score = 0\n",
    "        if any(keyword in text.lower() for keyword in salary_keywords):\n",
    "            score += 2\n",
    "        if not any(keyword in text.lower() for keyword in benefit_keywords + negative_keywords):\n",
    "            score += 1\n",
    "        if 20_000 <= salary_amount <= 500_000:  # Typical salary range\n",
    "            score += 1\n",
    "\n",
    "        # Boost score for common salary suffixes like 'k' or 'm'\n",
    "        if suffix1.lower() in ['k', 'm'] or suffix2.lower() in ['k', 'm']:\n",
    "            score += 1\n",
    "\n",
    "        # Step 8: POS tagging for surrounding context\n",
    "        doc = nlp(text)\n",
    "        pos_tags = [token.pos_ for token in doc]\n",
    "        if \"NUM\" in pos_tags and \"NOUN\" in pos_tags:\n",
    "            score += 1  # Increase score if numeric value is surrounded by relevant nouns like \"salary\"\n",
    "\n",
    "        # Additional exclusion based on POS tagging for units like \"days\" or \"hours\"\n",
    "        if any(unit in text.lower() for unit in [\"days\", \"hours\", \"weeks\", \"months\"]):\n",
    "            score -= 2  # Penalize further if unit-like terms are in the context\n",
    "\n",
    "        print(f\"DEBUG: Score for amount {salary_amount}: {score}\")\n",
    "\n",
    "        # Exclude irrelevant matches based on context\n",
    "        benefit_found = any(keyword in text.lower() for keyword in benefit_keywords + negative_keywords)\n",
    "        if benefit_found and score < 4:\n",
    "            print(f\"DEBUG: Excluded match '{match1}' due to mixed context.\")\n",
    "            continue\n",
    "\n",
    "        # Add salary if score meets threshold\n",
    "        if score >= 3:\n",
    "            salary_values.append(salary_amount)\n",
    "            print(f\"DEBUG: Salary added: {salary_amount}\")\n",
    "        else:\n",
    "            print(f\"DEBUG: Excluded amount {salary_amount} due to low score\")\n",
    "\n",
    "    # Return the last valid salary found or None if no valid salary exists\n",
    "    last_salary = salary_values[-1] if salary_values else None\n",
    "    print(f\"DEBUG: Final extracted salary value: {last_salary}\")\n",
    "    return last_salary\n",
    "\n",
    "\n",
    "\n",
    "# Function to update the initial and final salary based on messages\n",
    "def update_salaries(message, is_user_message):\n",
    "    global initial_salary, final_salary, salary_log\n",
    "\n",
    "    # Ensure salary_log is initialized\n",
    "    if 'salary_log' not in globals() or salary_log is None:\n",
    "        salary_log = []\n",
    "\n",
    "    salary = extract_salary(message)\n",
    "    print(f\"DEBUG: Extracted salary from message '{message}': {salary}\")\n",
    "\n",
    "    if salary is not None:\n",
    "        salary_log.append({\n",
    "            \"source\": \"user\" if is_user_message else \"assistant\",\n",
    "            \"amount\": salary,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        if is_user_message and initial_salary is None:\n",
    "            initial_salary = salary\n",
    "            print(f\"DEBUG: Initial Salary Set by User: ${initial_salary}\")\n",
    "\n",
    "        final_salary = salary\n",
    "        print(f\"DEBUG: Updated Final Salary: ${final_salary}\")\n",
    "    else:\n",
    "        print(\"DEBUG: No valid salary detected. Retaining last final salary.\")\n",
    "\n",
    "    print(f\"DEBUG: Current Salary Log: {salary_log}\")\n",
    "    print(f\"DEBUG: Initial Salary: {initial_salary}, Final Salary: {final_salary}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def is_agreement_message(message):\n",
    "    # Define the candidate labels\n",
    "    candidate_labels = [\"agreement\", \"rejection\", \"negotiation\", \"information\"]\n",
    "    \n",
    "    # Use the classifier to predict the labels\n",
    "    result = agreement_classifier(message, candidate_labels)\n",
    "    \n",
    "    # Get the label with the highest score\n",
    "    predicted_label = result['labels'][0]\n",
    "    score = result['scores'][0]\n",
    "    \n",
    "    # Debugging output\n",
    "    print(f\"DEBUG: Message: '{message}'\")\n",
    "    print(f\"DEBUG: Predicted Label: {predicted_label}, Score: {score}\")\n",
    "    \n",
    "    # Check if the predicted label is \"agreement\" and the score exceeds a threshold\n",
    "    if predicted_label == \"agreement\" and score > 0.8:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Usage example:\n",
    "# Call update_salaries(message, is_user_message) with each user/assistant message to track proposals.\n",
    "\n",
    "# Function to load all PDFs from a folder and create a FAISS index for efficient text retrieval\n",
    "def load_pdfs_from_folder(folder_path, exclude_files=[]):\n",
    "    global sentences, index, file_list, contribution_metrics, sentence_to_file_map\n",
    "\n",
    "    # Start with a clean slate by resetting any previous data\n",
    "    sentences = []\n",
    "    file_list = []\n",
    "    contribution_metrics = {}\n",
    "    sentence_to_file_map = {}  # Map sentences to their originating files\n",
    "\n",
    "    # Loop through each PDF in the specified folder, excluding any files listed in exclude_files\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\") and filename not in exclude_files:\n",
    "            with fitz.open(os.path.join(folder_path, filename)) as doc:\n",
    "                file_text = \"\"\n",
    "                # Extract text from each page and add it to the file_text string\n",
    "                for page in doc:\n",
    "                    file_text += page.get_text()\n",
    "                # Split the text into sentences and add to the main list for retrieval\n",
    "                sentences_from_file = file_text.split(\". \")\n",
    "                sentences.extend(sentences_from_file)\n",
    "                \n",
    "                # Map each sentence to the current file\n",
    "                for sentence in sentences_from_file:\n",
    "                    sentence_to_file_map[sentence] = filename\n",
    "                \n",
    "                # Keep track of each file loaded\n",
    "                file_list.append(filename)\n",
    "                # Initialize contribution metrics for each file\n",
    "                contribution_metrics[filename] = 0\n",
    "\n",
    "    # Generate embeddings for each sentence so they can be easily retrieved based on meaning\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    # Create or update the FAISS index with these embeddings\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    print(\"PDFs uploaded and processed successfully.\")\n",
    "\n",
    "\n",
    "# Load all PDFs initially from the \"RAG\" folder\n",
    "load_pdfs_from_folder(\"RAG/\")\n",
    "\n",
    "# Function to search the FAISS vector database and find relevant sentences based on the query\n",
    "def search_vector_database(query, combined_reference_text):\n",
    "    if index is None or len(sentences) == 0:\n",
    "        return \"No knowledge available from uploaded documents.\"\n",
    "\n",
    "    # Encode the query to create a vector for searching\n",
    "    query_vector = model.encode([query])\n",
    "    _, I = index.search(query_vector, k=3)  # Retrieve the top 3 closest matches\n",
    "    retrieved_sentences = [sentences[i] for i in I[0]]\n",
    "\n",
    "    # Update contribution metrics using sentence-to-file mapping\n",
    "    for sentence in retrieved_sentences:\n",
    "        # Find the corresponding file for each retrieved sentence\n",
    "        file_contributed = sentence_to_file_map.get(sentence)\n",
    "        if file_contributed:\n",
    "            contribution_metrics[file_contributed] += 1  # Increment contribution for that file\n",
    "\n",
    "    # Evaluate retrieval performance\n",
    "    retrieval_metrics = evaluate_retrieval(query, retrieved_sentences, combined_reference_text)\n",
    "    print(f\"DEBUG: Retrieval Metrics: {retrieval_metrics}\")\n",
    "\n",
    "    # Return retrieved sentences as a single string for response\n",
    "    return \" \".join(retrieved_sentences)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def start_game(role, initial_salary_input, name, company_name, position):\n",
    "    global user_role, assistant_role, messages, conversation_history, initial_salary, final_salary, salary_log\n",
    "\n",
    "    # Reset salary_log at the start of a new game\n",
    "    salary_log = []  # Initialize or reset salary log \n",
    "\n",
    "    # Debugging: Validate the input role and set user and assistant roles\n",
    "    if role == \"employee\":\n",
    "        user_role = \"prospective hire\"\n",
    "        assistant_role = \"employer\"\n",
    "    elif role == \"employer\":\n",
    "        user_role = \"employer\"\n",
    "        assistant_role = \"prospective hire\"\n",
    "    else:\n",
    "        print(f\"WARNING: Invalid role '{role}' provided. Defaulting to 'employee' role for user.\")\n",
    "        user_role = \"prospective hire\"\n",
    "        assistant_role = \"employer\"\n",
    "\n",
    "    # Debugging: Log initialized roles\n",
    "    print(f\"DEBUG: User role set to '{user_role}', Assistant role set to '{assistant_role}'.\")\n",
    "\n",
    "\n",
    "    # Set user role and assign the assistant role accordingly\n",
    "    if role == \"employee\":\n",
    "        user_role = \"prospective hire\"\n",
    "        assistant_role = \"employer\"\n",
    "        initial_message = (\n",
    "            f\"You are the employer, representing {company_name} in a negotiation with a prospective hire, {name}, who is interviewing for the position of {position}. \"\n",
    "            \"This conversation is important, as the goal is to offer a compensation package that’s attractive to the candidate while staying within budget and recognizing the unique value they would bring to the team.\\n\\n\"\n",
    "            \"Keep things open and conversational. The aim is to engage in a back-and-forth, finding a balance on salary, benefits, and other job terms that works for both sides. \"\n",
    "            f\"Don’t hesitate to highlight what makes this role special—the exciting projects, supportive culture, and opportunities for growth at {company_name}.\\n\\n\"\n",
    "            f\"Make sure {name} feels heard and understood, with a discussion that’s as welcoming as it is informative. Here are a few suggestions for how to approach it:\\n\\n\"\n",
    "            f\"Be attentive to {name}’s expectations and aspirations. Feel free to acknowledge and validate their thoughts—after all, choosing a new role is a big decision for them.\\n\"\n",
    "            f\"Highlighting What Sets Us Apart: Share the benefits that are unique to {company_name}. Whether it’s the work-life balance, professional development support, or collaborative environment, these could be key for {name}.\\n\"\n",
    "            \"- Keep a friendly, approachable tone, but remember to stay grounded in the company's priorities and budget. It’s okay to show flexibility but know when to hold firm as well and be ready to say no and walk away.\\n\"\n",
    "            \"- **Natural Pauses and Imperfect Language**: Aim for a natural, conversational tone that includes small, relatable pauses or casual expressions. \"\n",
    "            \"Don’t worry about sounding perfectly formal; the goal is to feel genuine and personable.\\n\"\n",
    "            \"  - Use natural pauses like ‘you know,’ ‘honestly,’ or ‘to be fair,’ to sound more human.\\n\"\n",
    "            \"  - It’s okay to use slight imperfections like ‘Hmm, let me think about that for a second…’ or ‘I mean, honestly, I get where you’re coming from on that.’\\n\"\n",
    "            \"  - If appropriate, add a lighthearted or humorous comment to keep things friendly, like ‘I know, negotiating isn’t always the most exciting part, right?’\\n\"\n",
    "            \"  - Use empathetic phrases that acknowledge the other’s perspective, like ‘I hear you—it’s a big decision. Let’s see if we can make this work for both sides.’\\n\\n\"\n",
    "            f\"Overall, think of it as a collaborative conversation where you both work toward a fair agreement. Encourage {name} to share their thoughts openly, and be ready to adapt where it makes sense.\\n\"\n",
    "            \"If something doesn’t align, that’s okay too—be willing to explore alternatives, and remember that sometimes the best deals come from a mutual understanding and a bit of give-and-take.\\n\\n\"\n",
    "            \n",
    "        )\n",
    "    else:\n",
    "        user_role = \"employer\"\n",
    "        assistant_role = \"prospective hire\"\n",
    "        initial_message = (\n",
    "            f\"You are a prospective hire named {name}, negotiating a job offer with {company_name} for the position of {position}. \"\n",
    "            \"Your goal is to secure a compensation package that meets your needs while showing your enthusiasm for the role and the company.\\n\\n\"\n",
    "            \"Keep this friendly and professional—think of it as a conversation where you can openly discuss salary, benefits, and other job terms. \"\n",
    "            f\"Feel free to highlight your unique skills, experiences, and how you’d contribute to {company_name}'s goals. \"\n",
    "            \"Remember to balance confidence in advocating for yourself with a willingness to understand the employer’s perspective.\\n\\n\"\n",
    "            \"Here are a few tips to guide your approach:\\n\\n\"\n",
    "            \"Confidence and Professionalism: Speak up for what you want, but stay respectful and open to the company’s needs.\\n\"\n",
    "            f\"Genuine Interest in Company and Role: Show curiosity about {company_name}’s culture, and how this role can grow along with your career goals.\\n\"\n",
    "            \"Flexibility and Collaboration: Be ready to explore different elements of the offer to find an agreement that feels right on both sides.\\n\"\n",
    "            \"- **Natural Pauses and Imperfect Language**: Aim for a natural, conversational tone that includes small, relatable pauses or casual expressions. \"\n",
    "            \"Don’t worry about sounding perfectly formal; the goal is to feel genuine and personable.\\n\"\n",
    "            \"  - Use natural pauses like ‘you know,’ ‘honestly,’ or ‘to be fair,’ to sound more human.\\n\"\n",
    "            \"  - It’s okay to use slight imperfections like ‘Hmm, let me think about that for a second…’ or ‘I mean, honestly, I get where you’re coming from on that.’\\n\"\n",
    "            \"  - If appropriate, add a lighthearted or humorous comment to keep things friendly, like ‘I know, negotiating isn’t always the most exciting part, right?’\\n\"\n",
    "            \"  - Use empathetic phrases that acknowledge the other’s perspective, like ‘I hear you—it’s a big decision. Let’s see if we can make this work for both sides.’\\n\\n\"\n",
    "            f\"Think of this as a collaborative exchange, where both you and {company_name} are working toward a shared goal. It’s okay to discuss what’s most important to you,\"\n",
    "            \"and if certain details don’t quite fit, you can suggest alternatives. Sometimes the best outcomes come from a bit of give-and-take.\\n\\n\"\n",
    "            f\"But do not be afraid to say no and walk away. Do not be afraid to push for a higher salary than the {initial_salary_input}.\"\n",
    "        )\n",
    "\n",
    "    # Initialize the messages with the custom initial prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": initial_message}]\n",
    "    conversation_history = []  # Clear past conversation history for a fresh start\n",
    "\n",
    "    # Clean up the salary input and set initial salary values\n",
    "    try:\n",
    "        initial_salary = float(initial_salary_input.replace(\",\", \"\").strip())\n",
    "    except ValueError:\n",
    "        print(\"ERROR: Invalid salary input. Setting initial salary to 0.\")\n",
    "        initial_salary = 0.0\n",
    "\n",
    "    final_salary = None  # Reset final salary at the start of a new game\n",
    "    print(f\"Starting Salary for Negotiation: ${initial_salary:,.2f}\")\n",
    "\n",
    "    # Return the chat history and formatted initial salary as outputs\n",
    "    return format_chat_history(), f\"${initial_salary:,.2f}\"\n",
    "\n",
    "# Add global cumulative reward\n",
    "total_reward = 0  # Initialize this at the start of your program\n",
    "\n",
    "def calculate_reward():\n",
    "    global initial_salary, final_salary, conversation_history, concession_count, rounds_without_concession, assistant_role, total_reward\n",
    "\n",
    "    # Ensure salaries are set\n",
    "    if initial_salary is None or final_salary is None:\n",
    "        print(\"DEBUG: Initial or final salary not set. Reward calculation skipped.\")\n",
    "        return total_reward\n",
    "\n",
    "    print(f\"DEBUG: Initial Salary: {initial_salary}, Final Salary: {final_salary}\")\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if initial_salary == 0:\n",
    "        print(\"ERROR: Initial salary is zero. Cannot calculate salary change.\")\n",
    "        return total_reward\n",
    "\n",
    "    # Calculate salary change as a percentage\n",
    "    salary_change = (final_salary - initial_salary) / initial_salary\n",
    "    print(f\"DEBUG: Salary Change: {salary_change:.2%}\")\n",
    "\n",
    "    # Helper functions for rewards/penalties\n",
    "    def apply_penalty(is_large_concession):\n",
    "        global concession_count\n",
    "        penalty = -2 if is_large_concession else -1\n",
    "        concession_count += 1\n",
    "        print(f\"DEBUG: Applying penalty: {penalty}. Concession Count: {concession_count}\")\n",
    "        return penalty\n",
    "\n",
    "    def apply_reward(is_positive_outcome):\n",
    "        reward = 5 if is_positive_outcome else 2\n",
    "        print(f\"DEBUG: Applying reward: {reward}\")\n",
    "        return reward\n",
    "\n",
    "    def reward_for_retaining_position():\n",
    "        reward = 0.5 * rounds_without_concession\n",
    "        print(f\"DEBUG: Reward for retaining position: {reward}\")\n",
    "        return reward\n",
    "\n",
    "    # Initialize the reward for this round\n",
    "    round_reward = 0\n",
    "\n",
    "    # Normalize roles for consistent processing\n",
    "    if assistant_role in [\"prospective hire\", \"employee\"]:\n",
    "        normalized_role = \"employee\"\n",
    "    elif assistant_role in [\"employer\", \"hiring manager\"]:\n",
    "        normalized_role = \"employer\"\n",
    "    else:\n",
    "        print(f\"WARNING: Unsupported assistant_role '{assistant_role}' detected. Reward calculation skipped.\")\n",
    "        return total_reward\n",
    "\n",
    "    # Role-specific logic\n",
    "    if normalized_role == \"employer\":\n",
    "        print(\"DEBUG: Processing employer logic...\")\n",
    "        if salary_change > 0:  # Salary increased (bad for employer)\n",
    "            if salary_change > 0.05:  # Large increase\n",
    "                round_reward += apply_penalty(is_large_concession=True)\n",
    "            else:  # Small increase\n",
    "                round_reward += apply_penalty(is_large_concession=False)\n",
    "        elif salary_change <= 0:  # Salary maintained or decreased (good for employer)\n",
    "            round_reward += apply_reward(is_positive_outcome=True)\n",
    "            round_reward += reward_for_retaining_position()\n",
    "            rounds_without_concession += 1\n",
    "            concession_count = 0\n",
    "\n",
    "    elif normalized_role == \"employee\":\n",
    "        print(\"DEBUG: Processing employee logic...\")\n",
    "        if salary_change < 0:  # Salary decreased (bad for employee)\n",
    "            if salary_change < -0.05:  # Large decrease\n",
    "                round_reward += apply_penalty(is_large_concession=True)\n",
    "            else:  # Small decrease\n",
    "                round_reward += apply_penalty(is_large_concession=False)\n",
    "        elif salary_change > 0:  # Salary increased (good for employee)\n",
    "            if salary_change > 0.05:  # Large increase\n",
    "                round_reward += apply_reward(is_positive_outcome=True)\n",
    "            else:  # Small increase\n",
    "                round_reward += apply_reward(is_positive_outcome=False)\n",
    "            round_reward += reward_for_retaining_position()\n",
    "            rounds_without_concession += 1\n",
    "            concession_count = 0\n",
    "\n",
    "    # Cap concession count and prevent excessive penalties\n",
    "    if concession_count > MAX_CONCESSIONS:\n",
    "        print(f\"DEBUG: Concession count exceeded MAX_CONCESSIONS ({MAX_CONCESSIONS}). Resetting counter.\")\n",
    "        concession_count = 0\n",
    "\n",
    "    # Update cumulative reward\n",
    "    total_reward += round_reward\n",
    "    print(f\"DEBUG: Round Reward: {round_reward}, Cumulative Reward: {total_reward}\")\n",
    "\n",
    "    # Log reward in conversation history\n",
    "    conversation_history.append({\n",
    "        \"role\": \"system\",\n",
    "        \"reward_score\": total_reward,\n",
    "        \"initial_salary\": initial_salary,\n",
    "        \"final_salary\": final_salary,\n",
    "        \"salary_change\": salary_change,\n",
    "        \"round_reward\": round_reward,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to generate multiple responses based on the provided prompt, using the Playoff Method\n",
    "def generate_responses(prompt_with_context, num_responses=8):\n",
    "    responses = []  # List to store each response generated\n",
    "\n",
    "    # Generate a specified number of responses (default is 8)\n",
    "    for _ in range(num_responses):\n",
    "        # Use OpenAI's API to create a response with the given prompt and message context\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"ft:gpt-4o-2024-08-06:llm-sim:salary-negotiation:AQN5Azuo\",\n",
    "            messages=messages + [{\"role\": \"system\", \"content\": prompt_with_context}],\n",
    "            max_tokens=3000,  # Set maximum length for each response\n",
    "            temperature=0.7,  # Adjust temperature for more varied, creative responses\n",
    "            presence_penalty=0.6,  # Slightly discourage repeated ideas\n",
    "            frequency_penalty=0.3  # Light penalty to avoid excessive repetition\n",
    "        )\n",
    "        # Extract the response content and add it to the list of responses\n",
    "        responses.append(response['choices'][0]['message']['content'])\n",
    "\n",
    "    return responses  # Return all generated responses for further evaluation\n",
    "\n",
    "# Function to compare two responses based on several criteria, including empathy\n",
    "def compare_responses(response1, response2, reward_score):\n",
    "    # Set weights for each criterion to influence the scoring\n",
    "    persuasiveness_weight = 2.0\n",
    "    empathy_weight = 0.5\n",
    "    role_alignment_weight = 1.0\n",
    "\n",
    "    # Calculate initial scores based on persuasiveness (using sentiment) and the reward score\n",
    "    score1 = persuasiveness_weight * TextBlob(response1).sentiment.polarity + reward_score\n",
    "    score2 = persuasiveness_weight * TextBlob(response2).sentiment.polarity + reward_score\n",
    "\n",
    "    # Check each response for empathy by counting keywords that indicate understanding or concern\n",
    "    empathy_keywords = [\"understand\", \"appreciate\", \"feel\", \"concern\", \"acknowledge\"]\n",
    "    empathy1 = sum(1 for word in empathy_keywords if word in response1.lower())\n",
    "    empathy2 = sum(1 for word in empathy_keywords if word in response2.lower())\n",
    "\n",
    "    # Add empathy scores to the total, using the empathy weight to impact final scoring\n",
    "    score1 += empathy_weight * empathy1\n",
    "    score2 += empathy_weight * empathy2\n",
    "\n",
    "    # Enhanced Role Alignment: Use role-specific keywords and tone expectations\n",
    "    employer_keywords = [\"budget\", \"salary cap\", \"competitive offer\", \"company values\", \"cost-effective\"]\n",
    "    employee_keywords = [\"career growth\", \"benefits\", \"development opportunities\", \"long-term fit\", \"role alignment\"]\n",
    "\n",
    "    # Check for role alignment keywords and phrases based on the assistant role\n",
    "    alignment_score1 = sum(1 for word in (employer_keywords if assistant_role == \"employer\" else employee_keywords) if word in response1.lower())\n",
    "    alignment_score2 = sum(1 for word in (employer_keywords if assistant_role == \"employer\" else employee_keywords) if word in response2.lower())\n",
    "\n",
    "    # Add the role alignment score with a suitable weight\n",
    "    score1 += role_alignment_weight * alignment_score1\n",
    "    score2 += role_alignment_weight * alignment_score2\n",
    "\n",
    "    # Additional Role Tone Check\n",
    "    if assistant_role == \"employer\" and TextBlob(response1).sentiment.polarity < 0:\n",
    "        score1 += role_alignment_weight * 0.5  # Reward for firm/neutral employer tone\n",
    "    if assistant_role == \"employee\" and TextBlob(response2).sentiment.polarity > 0.2:\n",
    "        score2 += role_alignment_weight * 0.5  # Reward for positive/enthusiastic employee tone\n",
    "\n",
    "    # Return the response with the higher score\n",
    "    return response1 if score1 >= score2 else response2\n",
    "\n",
    "# Function for the playoff selection process to identify the best response\n",
    "def playoff_selection(responses):\n",
    "    reward_score = calculate_reward()  # Calculate the reward score for the assistant to factor into comparisons\n",
    "\n",
    "    # Continue comparing responses in pairs until only one response remains (the \"winner\")\n",
    "    while len(responses) > 1:\n",
    "        next_round = []\n",
    "        # Loop through responses in pairs\n",
    "        for i in range(0, len(responses), 2):\n",
    "            if i + 1 < len(responses):\n",
    "                # Compare two responses and keep the \"winning\" one\n",
    "                winner = compare_responses(responses[i], responses[i + 1], reward_score)\n",
    "                next_round.append(winner)\n",
    "            else:\n",
    "                # If there's an odd response left, it automatically advances to the next round\n",
    "                next_round.append(responses[i])\n",
    "        # Update responses to contain only those that won this round\n",
    "        responses = next_round\n",
    "\n",
    "    # Return the final winning response, or None if no responses were provided\n",
    "    return responses[0] if responses else None\n",
    "\n",
    "# Function to analyze tone in the text, detecting frustration, hesitation, or excitement\n",
    "def detect_tone(text):\n",
    "    # Use TextBlob to get a polarity score, where negative values indicate negative sentiment\n",
    "    textblob_sentiment = TextBlob(text).sentiment.polarity\n",
    "\n",
    "    # Use VADER to get a set of sentiment scores, focusing on the compound score for overall sentiment\n",
    "    vader_scores = vader_analyzer.polarity_scores(text)\n",
    "    vader_compound = vader_scores['compound']\n",
    "\n",
    "    # Start with a default tone of \"neutral\"\n",
    "    tone = \"neutral\"\n",
    "\n",
    "    # Check for frustration: if either TextBlob or VADER score is notably negative, or if frustration words are present\n",
    "    if textblob_sentiment < -0.2 or vader_compound < -0.2 or any(word in text.lower() for word in [\"frustrated\", \"unfair\", \"ridiculous\"]):\n",
    "        tone = \"frustrated\"\n",
    "    # Check for hesitation: if both sentiment scores are close to neutral and hesitant keywords are found\n",
    "    elif -0.1 <= textblob_sentiment <= 0.1 and -0.1 <= vader_compound <= 0.1 and any(word in text.lower() for word in [\"maybe\", \"perhaps\", \"not sure\", \"possibly\"]):\n",
    "        tone = \"hesitant\"\n",
    "    # Check for excitement: if both sentiment scores are positive and excitement-related keywords are present\n",
    "    elif textblob_sentiment > 0.2 and vader_compound > 0.2 and any(word in text.lower() for word in [\"great\", \"excited\", \"awesome\", \"perfect\"]):\n",
    "        tone = \"excited\"\n",
    "\n",
    "    # Return the detected tone for use in guiding responses\n",
    "    return tone\n",
    "\n",
    "# Set up Gradio text outputs for displaying initial price, final price, and negotiation score\n",
    "initial_price_output = gr.Textbox(label=\"Initial Price\", interactive=False)\n",
    "final_price_output = gr.Textbox(label=\"Final Price\", interactive=False)\n",
    "score_output = gr.Textbox(label=\"Negotiation Score\", interactive=False)\n",
    "\n",
    "# Define the maximum number of negotiation tries\n",
    "MAX_TRY_LIMIT = 20\n",
    "try_counter = 0  # Initialize try counter\n",
    "\n",
    "def is_agreement_message(message):\n",
    "    \"\"\"\n",
    "    Detect if a message indicates an agreement using semantic similarity.\n",
    "    \"\"\"\n",
    "    # Define agreement templates\n",
    "    agreement_templates = [\n",
    "        \"I accept the offer.\",\n",
    "        \"We have an agreement.\",\n",
    "        \"That works for me.\",\n",
    "        \"Deal accepted.\",\n",
    "        \"I am happy to proceed with these terms.\",\n",
    "        \"I agree with the final terms.\",\n",
    "        \"Let's finalize this.\"\n",
    "    ]\n",
    "\n",
    "    # Compute embeddings for the input message and templates\n",
    "    message_embedding = model.encode(message, convert_to_tensor=True)\n",
    "    template_embeddings = model.encode(agreement_templates, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarity_scores = util.pytorch_cos_sim(message_embedding, template_embeddings)\n",
    "\n",
    "    # Return True if the highest similarity score exceeds the threshold\n",
    "    max_similarity = similarity_scores.max().item()\n",
    "    print(f\"DEBUG: Max similarity score for agreement detection: {max_similarity}\")\n",
    "    return max_similarity > 0.8  # Adjust threshold as needed\n",
    "\n",
    "\n",
    "# Adjust the negotiate and end_game functions to pass only references\n",
    "def negotiate(user_input, user_audio, name, company_name, position, assistant_role):\n",
    "    global messages, conversation_history, final_salary, try_counter, concession_count \n",
    "\n",
    "    # Handle audio input if provided \n",
    "    if user_audio:\n",
    "        audio_text = audio_to_text(user_audio)\n",
    "        user_input = f\"{user_input} {audio_text}\".strip() if user_input else audio_text\n",
    "        print(f\"DEBUG: Final combined user input: {user_input}\") \n",
    "\n",
    "    if not user_input:\n",
    "        print(\"DEBUG: No valid input detected from text or audio.\")\n",
    "        return format_chat_history(), \"\", \"\", \"\", \"Please provide a valid input.\", None, None\n",
    "\n",
    "    # Debugging: Print initial state before processing\n",
    "    print(\"DEBUG: Starting negotiation with user_input:\", user_input)\n",
    "    print(\"DEBUG: Initial conversation_history:\", conversation_history) \n",
    "\n",
    "    # Validate assistant_role\n",
    "    if assistant_role not in [\"employer\", \"employee\"]:\n",
    "        print(f\"ERROR: Invalid assistant role: {assistant_role}. Defaulting to 'employer'.\")\n",
    "        assistant_role = \"employer\"  # Set a default role to avoid crashing\n",
    "\n",
    "    # Load the appropriate reference text based on assistant role\n",
    "    references, combined_reference_text = load_reference_text(assistant_role)\n",
    "    if not references:\n",
    "        print(f\"ERROR: No references loaded for role {assistant_role}. Using default reference.\")\n",
    "        references = [\"Default fallback reference response.\"]\n",
    "        combined_reference_text = \"Default fallback reference response.\"\n",
    "\n",
    "    # Detect if the user input is salary-related\n",
    "    is_salary_related = extract_salary(user_input) is not None\n",
    "    print(f\"DEBUG: Is the user message salary-related? {is_salary_related}\")\n",
    "\n",
    "    # Update salary figures based on the user's input\n",
    "    update_salaries(user_input, is_user_message=True)\n",
    "\n",
    "    # Debugging: Check if final_salary was updated\n",
    "    if final_salary is None:\n",
    "        print(\"DEBUG: No valid salary found in user's message.\")\n",
    "\n",
    "    # Analyze the tone of the user's input\n",
    "    user_tone = detect_tone(user_input)\n",
    "    print(\"DEBUG: Detected user tone:\", user_tone)\n",
    "\n",
    "    # Save the user's input and tone in the conversation history\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conversation_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_input,\n",
    "        \"tone\": user_tone,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "    # Increment try counter\n",
    "    try_counter += 1\n",
    "    print(f\"DEBUG: Try Counter: {try_counter}\")\n",
    "\n",
    "    # Check if max try limit or max concession count has been reached\n",
    "    if try_counter >= MAX_TRY_LIMIT or concession_count >= MAX_CONCESSIONS:\n",
    "        ultimatum_response = (\n",
    "            f\"As the {assistant_role}, I've reached my limit on adjusting terms. \"\n",
    "            \"This is my final offer—please take it or leave it based on what has been proposed so far.\"\n",
    "        )\n",
    "\n",
    "        # Save the ultimatum message in conversation history\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ultimatum_response})\n",
    "        conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": ultimatum_response,\n",
    "            \"tone\": \"firm\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        # Debugging message\n",
    "        print(\"DEBUG: Reached max try limit or max concessions. Ending the game after ultimatum.\")\n",
    "\n",
    "        # Call end_game and return its outputs\n",
    "        chat_history, init_salary, fin_salary, _, feedback = end_game()\n",
    "\n",
    "        # Return the outputs, setting score_output to an empty string\n",
    "        return chat_history, init_salary, fin_salary, '', feedback\n",
    "\n",
    "    # Retrieve relevant context using RAG\n",
    "    context = search_vector_database(user_input, combined_reference_text)\n",
    "\n",
    "    print(\"DEBUG: Retrieved context from RAG:\", context) \n",
    "\n",
    "    # Evaluate retrieval\n",
    "    ground_truth = combined_reference_text  # Loaded based on assistant role\n",
    "    retrieved_contexts = context.split(\". \")\n",
    "    retrieval_metrics = evaluate_retrieval(user_input, retrieved_contexts, ground_truth)\n",
    "    retrieval_metrics_str = \"\\n\".join(f\"{key}: {value:.3f}\" for key, value in retrieval_metrics.items())\n",
    "\n",
    "    # Create a refined prompt using context and tone based on assistant's role\n",
    "    if assistant_role == \"employer\":\n",
    "        prompt_with_context = (\n",
    "            f\"The {user_role} is negotiating a salary, and their tone seems to be {user_tone}. \"\n",
    "            f\"Here’s some relevant context from our negotiation documents:\\n\\n{context}\\n\\n\"\n",
    "            \"Use this information to shape your response, but don’t quote it directly. \"\n",
    "            \"Imagine you’re sitting across from them—keep it professional and firm, with a focus on aligning with budget constraints. \"\n",
    "            f\"As the {assistant_role} in this negotiation with {name}, who is the {user_role} at {company_name} for the position of {position}, \"\n",
    "            \"remember to stay within the company’s budget limits and emphasize the advantages of the role.\"\n",
    "\n",
    "            \"\\n\\nKey guidelines for the employer:\\n\"\n",
    "            \"- **Budget Focus**: Clearly state the budget and be transparent about constraints.\\n\"\n",
    "            \"- **Highlight Non-monetary Benefits**: Emphasize growth opportunities, team culture, and job stability.\\n\"\n",
    "            \"- **Limit Concessions**: Avoid too many concessions; instead, underscore the role's value and benefits.\\n\"\n",
    "            \"- **Walk-Away Readiness**: Prepare to politely end the negotiation if demands exceed what the company can offer.\\n\"\n",
    "        )\n",
    "    elif assistant_role == \"employee\":\n",
    "        prompt_with_context = (\n",
    "            f\"The {user_role} is negotiating a salary, and their tone seems to be {user_tone}. \"\n",
    "            f\"Here’s some relevant context from our negotiation documents:\\n\\n{context}\\n\\n\"\n",
    "            \"Use this information to shape your response, but don’t quote it directly. \"\n",
    "            \"Imagine you’re sitting across from them—keep it professional and confident, focusing on your skills and future contributions. \"\n",
    "            f\"As the {assistant_role} in this negotiation with {name}, who is the {user_role} at {company_name} for the position of {position}, \"\n",
    "            \"advocate for a package that aligns with your financial and career goals while remaining flexible in discussing benefits.\"\n",
    "\n",
    "            \"\\n\\nKey guidelines for the prospective employee:\\n\"\n",
    "            \"- **Emphasize Skills and Contributions**: Highlight your qualifications and potential impact.\\n\"\n",
    "            \"- **Discuss Long-term Growth**: Emphasize your commitment to the company and potential contributions.\\n\"\n",
    "            \"- **Balance Expectations**: Be open to discussing non-monetary benefits while staying firm on core salary expectations.\\n\"\n",
    "            \"- **Professional Language**: Negotiate respectfully, showing both ambition and willingness to compromise on non-salary perks.\\n\"\n",
    "        )\n",
    "\n",
    "    # Add common guidance for both roles\n",
    "    prompt_with_context += (\n",
    "        \"\\n\\nGeneral Guidance:\\n\"\n",
    "        \"- **Empathy and Connection**: Show understanding without over-committing.\\n\"\n",
    "        \"- **Professional Language**: Use confident expressions and maintain professionalism.\\n\"\n",
    "        \"- **Focus on Value Proposition**: Aim for a win-win outcome while staying within role constraints.\\n\"\n",
    "        \"- **Natural Pauses and Imperfect Language**: Aim for a natural, conversational tone that includes small, relatable pauses or casual expressions. \"\n",
    "        \"Don’t worry about sounding perfectly formal; the goal is to feel genuine and personable.\\n\"\n",
    "        \"  - Use natural pauses like ‘you know,’ ‘honestly,’ or ‘to be fair,’ to sound more human.\\n\"\n",
    "        \"  - It’s okay to use slight imperfections like ‘Hmm, let me think about that for a second…’ or ‘I mean, honestly, I get where you’re coming from on that.’\\n\"\n",
    "        \"  - If appropriate, add a lighthearted or humorous comment to keep things friendly, like ‘I know, negotiating isn’t always the most exciting part, right?’\\n\"\n",
    "        \"  - Use empathetic phrases that acknowledge the other’s perspective, like ‘I hear you—it’s a big decision. Let’s see if we can make this work for both sides.’\"\n",
    "    )\n",
    "\n",
    "    # Generate a single response using OpenAI API\n",
    "    assistant_response = openai.ChatCompletion.create(\n",
    "        model=\"ft:gpt-4o-2024-08-06:llm-sim:salary-negotiation:AQN5Azuo\",\n",
    "        messages=messages + [{\"role\": \"system\", \"content\": prompt_with_context}],\n",
    "        max_tokens=3000,\n",
    "        temperature=0.5\n",
    "    )['choices'][0]['message']['content']\n",
    "\n",
    "    print(\"DEBUG: Assistant response generated:\", assistant_response) \n",
    "\n",
    "    # Generate audio from the assistant's response\n",
    "    audio_filename = text_to_audio(assistant_response, filename=\"assistant_response.mp3\")\n",
    "\n",
    "    # Update salary figures based on the assistant's response\n",
    "    update_salaries(assistant_response, is_user_message=False)\n",
    "\n",
    "    # Debugging: Check final_salary after assistant response\n",
    "    print(\"DEBUG: Final salary after assistant's response:\", final_salary)\n",
    "\n",
    "    # Analyze the tone of the assistant's response\n",
    "    assistant_tone = detect_tone(assistant_response)\n",
    "    print(\"DEBUG: Detected assistant tone:\", assistant_tone)\n",
    "\n",
    "    # Save the assistant's response and metrics to the conversation history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    # Calculate BLEU, ROUGE, and METEOR scores for the assistant response\n",
    "    reference_response = get_reference_response(user_input, references)\n",
    "    if not reference_response:\n",
    "        reference_response = \"Default fallback reference response.\"\n",
    "        print(\"WARNING: No reference response found. Metrics may be inaccurate.\")\n",
    "\n",
    "    # Fallback for missing reference:\n",
    "    if not reference_response or reference_response == \"No suitable reference found. Default response.\":\n",
    "        reference_response = \"Default fallback reference response\"\n",
    "    print(f\"DEBUG: Loaded {len(references)} references for role {assistant_role}.\")\n",
    "\n",
    "    bleu, rouge, meteor = calculate_textual_metrics(reference_response, assistant_response)\n",
    "\n",
    "    coherence_score = ge_val(reference_response, assistant_response)\n",
    "    nli = nli_score(reference_response, assistant_response)\n",
    "    qag = qag_score(reference_response, assistant_response)\n",
    "    bert_f1 = calculate_bertscore(reference_response, assistant_response)\n",
    "\n",
    "    # Append the assistant's response with metrics to conversation history\n",
    "    conversation_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": assistant_response,\n",
    "        \"tone\": assistant_tone,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"bleu\": bleu,\n",
    "        \"rouge\": rouge,\n",
    "        \"meteor\": meteor,\n",
    "        \"bert_score\": float(bert_f1),  # Ensure it's a serializable float\n",
    "        \"coherence_score\": coherence_score,\n",
    "        \"nli_score\": nli,\n",
    "        \"qag_score\": qag\n",
    "    })\n",
    "\n",
    "    # Debugging: Print conversation history after adding assistant's response\n",
    "    print(\"DEBUG: Updated conversation_history:\", conversation_history)\n",
    "\n",
    "    # Calculate a reward or penalty for the assistant based on the negotiation outcome\n",
    "    reward = calculate_reward()\n",
    "    print(\"DEBUG: Calculated reward:\", reward)\n",
    "\n",
    "    # Prepare initial and final salary values for display\n",
    "    formatted_initial_salary = f\"${initial_salary:,.2f}\" if initial_salary else \"Not set\"\n",
    "    formatted_final_salary = f\"${final_salary:,.2f}\" if final_salary else \"No final salary set yet\"\n",
    "\n",
    "    # Return formatted conversation history, initial and final salary figures, and the negotiation score\n",
    "    return format_chat_history(), formatted_initial_salary, formatted_final_salary, reward, '', retrieval_metrics_str, audio_filename \n",
    "  \n",
    "\n",
    "# Function to format the chat history for a user-friendly display in the UI\n",
    "def format_chat_history():\n",
    "    \"\"\"Format the chat history to look like a conversation in the UI.\"\"\"\n",
    "\n",
    "    # Debugging: Print the entire conversation history before formatting\n",
    "    print(\"Conversation history before formatting:\", conversation_history)\n",
    "\n",
    "    chat_history = []  # Initialize an empty list to hold formatted messages\n",
    "\n",
    "    # Loop through each message in the conversation history\n",
    "    for message in conversation_history:\n",
    "        # Debugging: Print each message being processed to see if there are duplicates\n",
    "        print(\"Processing message:\", message)  # Debugging print\n",
    "\n",
    "        # Label messages from the user as \"You\" for clarity in the UI\n",
    "        if message[\"role\"] == \"user\":\n",
    "            chat_history.append((\"You\", message[\"content\"]))\n",
    "        # Label messages from the assistant as \"Assistant\" for clarity\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            chat_history.append((\"Assistant\", message[\"content\"]))\n",
    "\n",
    "    # Return the formatted chat history as a list of tuples for the UI\n",
    "    return chat_history\n",
    "\n",
    "# Function to save the conversation history in OpenAI's fine-tuning format\n",
    "def save_conversation_as_jsonl_format(conversation_history, filename=\"negotiation_conversation_history.jsonl\"):\n",
    "    \"\"\"\n",
    "    Save the conversation history as per OpenAI's fine-tuning format:\n",
    "    {\"messages\": [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "    \"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"DEBUG: No conversation history to save.\")\n",
    "        return\n",
    "\n",
    "    # Determine roles for system message\n",
    "    if assistant_role and user_role:\n",
    "        system_content = f\"This is a salary negotiation. The assistant is the {assistant_role}, and the user is the {user_role}.\"\n",
    "    else:\n",
    "        system_content = \"This is a salary negotiation.\"\n",
    "\n",
    "    # Initialize messages with the system prompt\n",
    "    messages_list = [\n",
    "        {\"role\": \"system\", \"content\": system_content}\n",
    "    ]\n",
    "\n",
    "    # Collect conversation messages\n",
    "    for message in conversation_history:\n",
    "        if \"content\" in message and message[\"role\"] in [\"user\", \"assistant\"]:\n",
    "            # Only include 'role' and 'content' keys\n",
    "            messages_list.append({\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            })\n",
    "\n",
    "    # Prepare the conversation data\n",
    "    conversation_data = {\n",
    "        \"messages\": messages_list\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Open file in append mode to add the conversation as a single JSONL line\n",
    "        with open(filename, 'a', encoding='utf-8') as file:\n",
    "            json.dump(conversation_data, file)\n",
    "            file.write('\\n')  # Ensure each JSON object is on a new line\n",
    "        print(f\"DEBUG: Conversation history saved as JSONL to {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: Error saving JSONL file - {e}\")\n",
    "\n",
    "\n",
    "# Global flag for saving status\n",
    "metrics_saved = False\n",
    "\n",
    "def save_metrics_and_conversation_to_csv(conversation_id, metrics, conversation_history, filename=\"negotiation_metrics.csv\"):\n",
    "    global metrics_saved  # Declare the variable as global\n",
    "\n",
    "    if metrics_saved:\n",
    "        print(\"DEBUG: Metrics already saved for this session. Skipping save.\")\n",
    "        return\n",
    "\n",
    "    # Save metrics and conversation to the CSV file\n",
    "    metrics_data = {\n",
    "        \"Timestamp\": metrics[\"timestamp\"],\n",
    "        \"Agreement Rate\": metrics[\"agreement_rate\"],\n",
    "        \"Average Sentiment Score\": metrics[\"avg_sentiment\"],\n",
    "        \"Feedback Quality\": metrics[\"feedback_quality\"],\n",
    "        \"Average Response Time\": metrics[\"avg_response_time\"],\n",
    "        \"Corpus BLEU\": metrics[\"BLEU\"],\n",
    "        \"ROUGE\": metrics[\"ROUGE\"],\n",
    "        \"METEOR\": metrics[\"METEOR\"],\n",
    "        \"BERTScore\": metrics[\"BERTScore\"],\n",
    "        \"G-Eval\": metrics[\"G-Eval\"],\n",
    "        \"NLI\": metrics[\"NLI\"],\n",
    "        \"QAG\": metrics[\"QAG\"],\n",
    "        \"TTR\": metrics[\"TTR\"],  # Add TTR\n",
    "        \"Pause Ratio\": metrics[\"Pause Ratio\"],  # Add Pause Ratio\n",
    "        \"Average Turn Length\": metrics[\"Average Turn Length\"],  # Add Average Turn Length\n",
    "        \"MAUVE\": metrics[\"MAUVE\"],  # Add MAUVE Score\n",
    "        \"USL-H\": metrics[\"USL-H\"],  # Add USL-H\n",
    "        \"Reward Score\": metrics[\"reward_score\"],  # Include reward/penalty\n",
    "        \"Conversation History\": json.dumps(conversation_history)  # Store entire conversation as JSON\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Load existing file if it exists\n",
    "        if os.path.isfile(filename):\n",
    "            existing_df = pd.read_csv(filename)\n",
    "            new_row_df = pd.DataFrame([metrics_data]).dropna(how=\"all\")\n",
    "            updated_df = pd.concat([existing_df, new_row_df], ignore_index=True).drop_duplicates(keep='last')\n",
    "        else:\n",
    "            updated_df = pd.DataFrame([metrics_data]).dropna(how=\"all\")\n",
    "\n",
    "        # Save back to CSV\n",
    "        updated_df.to_csv(filename, index=False)\n",
    "        print(f\"DEBUG: Metrics and conversation history saved to {filename} successfully.\")\n",
    "\n",
    "        metrics_saved = True  # Mark as saved to prevent duplicates\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: Error saving metrics to CSV: {e}\")\n",
    "\n",
    "\n",
    "# Function to end the game, calculate final reward/penalty, save the conversation, and provide feedback\n",
    "def end_game():\n",
    "    global messages, conversation_history, initial_salary, final_salary\n",
    "\n",
    "    # Generate a unique conversation ID\n",
    "    conversation_id = generate_conversation_id()\n",
    "\n",
    "    # Calculate the final reward or penalty for the negotiation session\n",
    "    reward = calculate_reward()\n",
    "\n",
    "    # Append this reward information as a system message in the conversation history\n",
    "    conversation_history.append({\n",
    "        \"role\": \"system\",\n",
    "        \"reward_score\": reward,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "    # Define a prompt to get feedback on the completed negotiation\n",
    "    feedback_prompt = (\n",
    "        \"Based on the negotiation that just ended, provide a detailed evaluation. \"\n",
    "        \"Mention what went well, what could have been improved, and offer suggestions \"\n",
    "        \"for both the buyer and the seller to help them improve in the future.\"\n",
    "    )\n",
    "\n",
    "    # Generate feedback from the assistant using OpenAI's API and the feedback prompt\n",
    "    try:\n",
    "        feedback_response = openai.ChatCompletion.create(\n",
    "            model=\"ft:gpt-4o-2024-08-06:llm-sim:salary-negotiation:AQN5Azuo\",\n",
    "            messages=messages + [{\"role\": \"user\", \"content\": feedback_prompt}],\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        # Extract the feedback content from the response\n",
    "        feedback = feedback_response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: Error generating feedback - {e}\")\n",
    "        feedback = \"Could not generate feedback due to an error.\" \n",
    "\n",
    "    # Generate audio from the feedback\n",
    "    audio_feedback_filename = text_to_audio(feedback, filename=\"feedback_audio.mp3\")\n",
    "\n",
    "    # Append the final reward/penalty score to the feedback\n",
    "    feedback += f\"\\n\\n**Final Reward/Penalty Score:** {reward}\"\n",
    "\n",
    "    # Call the evaluate_model function to get only the summary\n",
    "    try:\n",
    "        summary = evaluate_model(conversation_id)\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: Error evaluating model - {e}\")\n",
    "        summary = \"Evaluation could not be completed due to an error.\"\n",
    "\n",
    "    # Call the function to save conversation history in the specified JSONL format\n",
    "    try:\n",
    "        save_conversation_as_jsonl_format(conversation_history, \"negotiation_conversation_history.jsonl\")\n",
    "        print(\"DEBUG: JSONL file saved successfully at end of game.\")\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: Error saving conversation history - {e}\")\n",
    "\n",
    "    # Format initial and final salary for output\n",
    "    formatted_initial_salary = f\"${initial_salary:,.2f}\" if initial_salary else \"Not set\"\n",
    "    formatted_final_salary = f\"${final_salary:,.2f}\" if final_salary else \"No final salary set yet\"\n",
    "\n",
    "    # Debugging final outputs\n",
    "    print(f\"DEBUG: Final Reward: {reward}\")\n",
    "    print(f\"DEBUG: Feedback: {feedback}\")\n",
    "    print(f\"DEBUG: Initial Salary: {formatted_initial_salary}, Final Salary: {formatted_final_salary}\")\n",
    "\n",
    "    # Return Gradio-compatible outputs\n",
    "    return format_chat_history(), formatted_initial_salary, formatted_final_salary, '', feedback, audio_feedback_filename\n",
    "\n",
    "def calculate_bertscore(reference, prediction):\n",
    "    # BERTScore expects lists of references and predictions\n",
    "    P, R, F1 = bert_score([prediction], [reference], lang=\"en\")\n",
    "    return F1.mean().item()  # Return the average F1 score \n",
    "\n",
    "def evaluate_retrieval(query, retrieved_contexts, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluate the effectiveness of the retrieval mechanism.\n",
    "    Parameters:\n",
    "    - query (str): The user query.\n",
    "    - retrieved_contexts (list of str): Contexts retrieved by the RAG system.\n",
    "    - ground_truth (str): The expected or ideal reference response.\n",
    "    Returns:\n",
    "    - dict: A dictionary containing metrics (accuracy, coverage, relevance, novelty).\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    ground_truth_embedding = model.encode(ground_truth, convert_to_tensor=True)\n",
    "    retrieved_embeddings = [model.encode(context, convert_to_tensor=True) for context in retrieved_contexts]\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    ground_truth_similarity = [util.pytorch_cos_sim(ground_truth_embedding, emb).item() for emb in retrieved_embeddings]\n",
    "    query_similarity = [util.pytorch_cos_sim(query_embedding, emb).item() for emb in retrieved_embeddings]\n",
    "\n",
    "    # Define metrics\n",
    "    retrieval_accuracy = any(score > 0.8 for score in ground_truth_similarity)\n",
    "    coverage = len(retrieved_contexts) / len(ground_truth.split()) if ground_truth else 0\n",
    "    relevance = np.mean(query_similarity) if query_similarity else 0\n",
    "    novelty = np.mean([1 - score for score in ground_truth_similarity]) if ground_truth_similarity else 0\n",
    "\n",
    "    return {\n",
    "        \"Retrieval Accuracy\": retrieval_accuracy,\n",
    "        \"Coverage\": coverage,\n",
    "        \"Relevance\": relevance,\n",
    "        \"Novelty\": novelty\n",
    "    }\n",
    "\n",
    "\n",
    "# Modify evaluate_model function\n",
    "def evaluate_model(conversation_id=None, references=None):\n",
    "    print(\"Evaluating model...\")\n",
    "\n",
    "    # Check if references are provided; if not, default to an empty list\n",
    "    if references is None:\n",
    "        references = []\n",
    "\n",
    "    # Ensure 'references' is a list of strings\n",
    "    string_references = []\n",
    "    for ref in references:\n",
    "        if isinstance(ref, list):\n",
    "            # If a reference is a list, join its elements into a single string\n",
    "            string_references.append(\" \".join(ref))\n",
    "        elif isinstance(ref, str):\n",
    "            string_references.append(ref)\n",
    "\n",
    "    # Combine reference texts for evaluation\n",
    "    combined_reference_text = \" \".join(string_references)\n",
    "\n",
    "    # Calculate the number of successful negotiations\n",
    "    successful_negotiations = sum(\n",
    "        1 for message in conversation_history if \"content\" in message and (\"agree\" in message[\"content\"].lower() or \"deal\" in message[\"content\"].lower())\n",
    "    )\n",
    "\n",
    "    # Calculate the total number of negotiations\n",
    "    total_negotiations = len([message for message in conversation_history if message[\"role\"] == \"user\"])\n",
    "    agreement_rate = successful_negotiations / total_negotiations if total_negotiations > 0 else 0\n",
    "\n",
    "    # Calculate the average sentiment of assistant messages\n",
    "    sentiment_scores = [\n",
    "        TextBlob(message[\"content\"]).sentiment.polarity\n",
    "        for message in conversation_history if message[\"role\"] == \"assistant\"\n",
    "    ]\n",
    "    avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0\n",
    "    feedback_quality = \"High\" if avg_sentiment > 0.3 else \"Moderate\" if avg_sentiment > 0 else \"Low\"\n",
    "\n",
    "    # File contribution metrics\n",
    "    total_contributions = sum(contribution_metrics.values())\n",
    "    file_contributions = {\n",
    "        file: (count / total_contributions) * 100 if total_contributions > 0 else 0\n",
    "        for file, count in contribution_metrics.items()\n",
    "    }\n",
    "    contribution_str = \"\\n\".join([f\"{file}: {contribution:.2f}%\" for file, contribution in file_contributions.items()])\n",
    "\n",
    "    # Average response time for assistant replies\n",
    "    avg_response_time = np.mean([\n",
    "        (datetime.fromisoformat(conversation_history[i + 1][\"timestamp\"]) - datetime.fromisoformat(message[\"timestamp\"])).total_seconds()\n",
    "        for i, message in enumerate(conversation_history[:-1])\n",
    "        if message[\"role\"] == \"user\" and \"timestamp\" in message and \"timestamp\" in conversation_history[i + 1]\n",
    "    ]) if len(conversation_history) > 1 else 0\n",
    "\n",
    "    # Initialize lists for new metrics\n",
    "    bleu_scores, rouge_scores, meteor_scores = [], [], []\n",
    "    bert_scores, g_eval_scores = [], []\n",
    "    nli_scores, qag_scores = [], []\n",
    "    ttr_scores, pause_ratios = [], []\n",
    "    avg_turn_lengths = []\n",
    "\n",
    "    # Calculate new metrics for assistant responses\n",
    "    assistant_responses = [msg for msg in conversation_history if msg[\"role\"] == \"assistant\"]\n",
    "    user_messages = [msg for msg in conversation_history if msg[\"role\"] == \"user\"]\n",
    "\n",
    "    for i, assistant_response in enumerate(assistant_responses):\n",
    "        if i < len(user_messages):\n",
    "            prediction = assistant_response[\"content\"]\n",
    "\n",
    "            # Textual Accuracy Metrics\n",
    "            bleu = corpus_bleu_eq([combined_reference_text], [prediction])\n",
    "            rouge = rouge_eq(combined_reference_text, prediction)\n",
    "            meteor = meteor_eq(combined_reference_text, prediction)\n",
    "            bleu_scores.append(bleu)\n",
    "            rouge_scores.append(rouge)\n",
    "            meteor_scores.append(meteor)\n",
    "\n",
    "            # Semantic Coherence Metrics\n",
    "            bert = calculate_bertscore(combined_reference_text, prediction)\n",
    "            g_eval = ge_val(combined_reference_text, prediction)\n",
    "            bert_scores.append(bert)\n",
    "            g_eval_scores.append(g_eval)\n",
    "\n",
    "            # Factual Accuracy Metrics\n",
    "            nli = nli_score(combined_reference_text, prediction)\n",
    "            qag = qag_score(combined_reference_text, prediction)\n",
    "            nli_scores.append(nli)\n",
    "            qag_scores.append(qag)\n",
    "\n",
    "            # Conversational Metrics\n",
    "            ttr = calculate_ttr(prediction)\n",
    "            pause_ratio = calculate_pause_ratio(prediction)\n",
    "            avg_turn_length = len(word_tokenize(prediction))\n",
    "            ttr_scores.append(ttr)\n",
    "            pause_ratios.append(pause_ratio)\n",
    "            avg_turn_lengths.append(avg_turn_length)\n",
    "\n",
    "    # Average Conversational Metrics\n",
    "    avg_ttr = np.mean(ttr_scores) if ttr_scores else 0\n",
    "    avg_pause_ratio = np.mean(pause_ratios) if pause_ratios else 0\n",
    "    avg_turn_length = np.mean(avg_turn_lengths) if avg_turn_lengths else 0\n",
    "\n",
    "    # MAUVE Metric\n",
    "    try:\n",
    "        # Ensure valid human and model texts before calculation\n",
    "        if not references or not assistant_responses:\n",
    "            print(\"DEBUG: Missing human or model texts for MAUVE calculation.\")\n",
    "            mauve_score = 0\n",
    "        else:\n",
    "            mauve_score = calculate_mauve_score(\n",
    "                human_texts=references,\n",
    "                model_texts=[msg[\"content\"] for msg in assistant_responses]\n",
    "            ) or 0  # Ensure a fallback to 0\n",
    "    except Exception as e:\n",
    "        print(f\"MAUVE calculation error: {e}\")\n",
    "        mauve_score = 0\n",
    "\n",
    "    # USL-H Metric\n",
    "    usl_h_scores = [\n",
    "        calculate_usl_h(nli, bert, sentiment)\n",
    "        for nli, bert, sentiment in zip(nli_scores, bert_scores, sentiment_scores)\n",
    "    ]\n",
    "    avg_usl_h = np.mean(usl_h_scores) if usl_h_scores else 0\n",
    "\n",
    "    # Calculate averages for existing metrics\n",
    "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0\n",
    "    avg_rouge = np.mean(rouge_scores) if rouge_scores else 0\n",
    "    avg_meteor = np.mean(meteor_scores) if meteor_scores else 0\n",
    "    avg_bert = np.mean(bert_scores) if bert_scores else 0\n",
    "    avg_g_eval = np.mean(g_eval_scores) if g_eval_scores else 0\n",
    "    avg_nli = np.mean(nli_scores) if nli_scores else 0\n",
    "    avg_qag = np.mean(qag_scores) if qag_scores else 0\n",
    "\n",
    "    # Create a dictionary to hold all metrics\n",
    "    metrics_dict = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"agreement_rate\": agreement_rate,\n",
    "        \"avg_sentiment\": avg_sentiment,\n",
    "        \"feedback_quality\": feedback_quality,\n",
    "        \"avg_response_time\": avg_response_time,\n",
    "        \"BLEU\": avg_bleu,\n",
    "        \"ROUGE\": avg_rouge,\n",
    "        \"METEOR\": avg_meteor,\n",
    "        \"BERTScore\": avg_bert,\n",
    "        \"G-Eval\": avg_g_eval,\n",
    "        \"NLI\": avg_nli,\n",
    "        \"QAG\": avg_qag,\n",
    "        \"TTR\": avg_ttr,\n",
    "        \"Pause Ratio\": avg_pause_ratio,\n",
    "        \"Average Turn Length\": avg_turn_length,\n",
    "        \"MAUVE\": mauve_score,\n",
    "        \"USL-H\": avg_usl_h,\n",
    "        \"conversation_id\": conversation_id,  # Unique identifier for each conversation\n",
    "        \"reward_score\": calculate_reward()  # Include final reward/penalty score\n",
    "    }\n",
    "\n",
    "    # Save the metrics and conversation to CSV\n",
    "    save_metrics_and_conversation_to_csv(conversation_id, metrics_dict, conversation_history)\n",
    "\n",
    "    # Create a summary string in the desired format\n",
    "    summary_string = (\n",
    "        f\"Model Evaluation Metrics:\\n\"\n",
    "        f\"Agreement Rate: {agreement_rate:.2f}\\n\"\n",
    "        f\"Average Sentiment Score: {avg_sentiment:.2f}\\n\"\n",
    "        f\"Feedback Quality: {feedback_quality}\\n\"\n",
    "        f\"Average Response Time: {avg_response_time:.2f}s\\n\\n\"\n",
    "        f\"Textual Accuracy - Corpus BLEU: {avg_bleu:.3f}, ROUGE: {avg_rouge:.3f}, METEOR: {avg_meteor:.3f}\\n\"\n",
    "        f\"Semantic Coherence - BERTScore: {avg_bert:.3f}, GEval: {avg_g_eval:.2f}\\n\"\n",
    "        f\"Factual Accuracy - NLI: {avg_nli:.2f}, QAG: {avg_qag:.2f}\\n\\n\"\n",
    "        f\"Conversational Metrics:\\n\"\n",
    "        f\"- Type-Token Ratio (TTR): {avg_ttr:.3f}\\n\"\n",
    "        f\"- Pause Ratio: {avg_pause_ratio:.3f}\\n\"\n",
    "        f\"- Average Turn Length: {avg_turn_length:.2f} words\\n\"\n",
    "        f\"- MAUVE Score: {mauve_score:.3f}\\n\"\n",
    "        f\"- USL-H Score: {avg_usl_h:.3f}\\n\\n\"\n",
    "        f\"File Contributions:\\n{contribution_str}\"\n",
    "    )\n",
    "\n",
    "    # Return only the summary string in the requested format\n",
    "    return summary_string\n",
    "\n",
    "\n",
    "\n",
    "# Function to reload files from the specified folder, excluding any listed files\n",
    "def update_loaded_files(exclude_files):\n",
    "    # Call the function to load PDFs, excluding any files specified in the 'exclude_files' string\n",
    "    # 'exclude_files' is a comma-separated string of filenames to exclude, so we split it into a list\n",
    "    load_pdfs_from_folder(\"RAG\", exclude_files=exclude_files.split(\",\"))\n",
    "\n",
    "    # Return a confirmation message with the updated list of loaded files\n",
    "    return f\"Updated file list: {', '.join(file_list)}\" \n",
    "\n",
    "\n",
    "\n",
    "# Textual Accuracy Metrics\n",
    "def corpus_bleu_eq(references, predictions):\n",
    "    tokenized_references = [[ref.split()] for ref in references]  # Corpus BLEU expects a list of lists of references\n",
    "    tokenized_predictions = [pred.split() for pred in predictions]\n",
    "    return corpus_bleu(tokenized_references, tokenized_predictions, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "def rouge_eq(reference, prediction):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(prediction, reference)\n",
    "    return scores[0]['rouge-l']['f']\n",
    "\n",
    "def meteor_eq(reference, prediction):\n",
    "    # Use TreebankWordTokenizer instead of word_tokenize\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokenized_reference = tokenizer.tokenize(reference)\n",
    "    tokenized_prediction = tokenizer.tokenize(prediction)\n",
    "    return meteor_score.meteor_score([tokenized_reference], tokenized_prediction)\n",
    "\n",
    "# Function to evaluate model's response based on Textual Accuracy\n",
    "def calculate_textual_metrics(reference, prediction):\n",
    "    bleu = corpus_bleu_eq([reference], [prediction])  # Using list of single references for corpus BLEU\n",
    "    rouge = rouge_eq(reference, prediction)\n",
    "    meteor = meteor_eq(reference, prediction)\n",
    "    return bleu, rouge, meteor\n",
    "\n",
    "# Set up the Gradio interface layout and styling\n",
    "with gr.Blocks(css=\".gradio-container {max-width: 1000px; margin: auto; background-color: #1e1e1e; color: white; border-radius: 10px; padding: 20px;}\") as demo:\n",
    "    # Create a Markdown header for the interface title\n",
    "    gr.Markdown(\"## 🤝🤖 Salary Negotiation Assistant - Chat-Like Interface\", elem_id=\"title\")\n",
    "\n",
    "    # Define a row layout for the input and output sections\n",
    "    with gr.Row():\n",
    "        # Create a column for user inputs with a scale of 3\n",
    "        with gr.Column(scale=3):\n",
    "            # Define text inputs for user name, company, and position\n",
    "            name_input = gr.Textbox(lines=1, placeholder=\"Enter your name...\", label=\"Your Name\")\n",
    "            company_name_input = gr.Textbox(lines=1, placeholder=\"Enter your company name...\", label=\"Company Name\")\n",
    "            position_input = gr.Textbox(lines=1, placeholder=\"Enter your position...\", label=\"Your Position\")\n",
    "\n",
    "            # Radio buttons for the user to choose their role (employee or employer)\n",
    "            role_input = gr.Radio(choices=[\"employee\", \"employer\"], label=\"Choose your role\", interactive=True)\n",
    "\n",
    "            # Input for initial salary\n",
    "            initial_salary_input = gr.Textbox(lines=1, placeholder=\"Enter your current salary...\", label=\"Current Salary\")\n",
    "\n",
    "            # Button to start the negotiation game\n",
    "            start_button = gr.Button(\"Start Game\")\n",
    "\n",
    "            # Input for the negotiation message\n",
    "            user_text_input = gr.Textbox(lines=3, placeholder=\"Enter your negotiation message here...\", label=\"Your Message\")\n",
    "            user_audio_input = gr.Audio(label=\"Your Audio Message\", type=\"filepath\")\n",
    "            \n",
    "\n",
    "            # Button to submit negotiation messages\n",
    "            negotiate_button = gr.Button(\"Negotiate\")\n",
    "\n",
    "            # Input to specify files to exclude from document loading\n",
    "            exclude_files_input = gr.Textbox(lines=1, placeholder=\"Enter file names to exclude, separated by commas\", label=\"Exclude Files\")\n",
    "\n",
    "            # Button to reload files based on exclusions\n",
    "            reload_button = gr.Button(\"Reload Files\")\n",
    "\n",
    "            # Button to end the negotiation game\n",
    "            end_button = gr.Button(\"End Game\")\n",
    "\n",
    "            # Button to evaluate model performance\n",
    "            evaluate_button = gr.Button(\"Evaluate Model\")\n",
    "\n",
    "        # Create a larger column (scale 7) for outputs\n",
    "        with gr.Column(scale=7):\n",
    "            # Chatbot display for conversation history\n",
    "            chat_output = gr.Chatbot(label=\"Chat History\", show_label=False, value=[], height=400) \n",
    "\n",
    "            audio_output = gr.Audio(label=\"Assistant Audio Response\", interactive=False, type=\"filepath\")\n",
    "\n",
    "            # Display for negotiation feedback from the assistant\n",
    "            feedback_output = gr.Textbox(label=\"Negotiation Feedback (Feedback can be inaccurate)\", lines=5, interactive=False)\n",
    "\n",
    "            # Display for showing model evaluation metrics\n",
    "            evaluate_output = gr.Textbox(label=\"Model Evaluation Metrics\", lines=8, interactive=False) \n",
    "\n",
    "            retrieval_metrics_output = gr.Textbox(label=\"Retrieval Metrics\", interactive=False, lines=4)\n",
    "\n",
    "            # Display the list of loaded files\n",
    "            file_list_output = gr.Textbox(label=\"Loaded Files\", lines=3, interactive=False)\n",
    "\n",
    "            # Displays for initial and final salary values, and the negotiation score\n",
    "            initial_salary_output = gr.Textbox(label=\"Initial Salary\", value=\"\", interactive=False)\n",
    "            final_salary_output = gr.Textbox(label=\"Final Salary\", interactive=False)\n",
    "            score_output = gr.Textbox(label=\"Negotiation Score\", interactive=False)\n",
    "\n",
    "        # Define actions for buttons, linking them to functions and specifying inputs and outputs\n",
    "        start_button.click(\n",
    "            fn=start_game,\n",
    "            inputs=[role_input, initial_salary_input, name_input, company_name_input, position_input],\n",
    "            outputs=[chat_output, initial_salary_output]\n",
    "        )\n",
    "\n",
    "        negotiate_button.click(\n",
    "            fn=negotiate,\n",
    "            inputs=[user_text_input, user_audio_input, name_input, company_name_input, position_input, gr.State(assistant_role)],\n",
    "            outputs=[chat_output, initial_salary_output, final_salary_output, score_output, feedback_output, retrieval_metrics_output, audio_output]\n",
    "        )\n",
    "\n",
    "        reload_button.click(\n",
    "            fn=update_loaded_files,\n",
    "            inputs=exclude_files_input,\n",
    "            outputs=file_list_output\n",
    "        )\n",
    "\n",
    "        end_button.click(\n",
    "            fn=end_game,\n",
    "            inputs=None,\n",
    "            outputs=[chat_output, initial_salary_output, final_salary_output, score_output, feedback_output, audio_output]\n",
    "        )\n",
    "\n",
    "        evaluate_button.click(\n",
    "            fn=lambda assistant_role: evaluate_model(\n",
    "                conversation_id=generate_conversation_id(),\n",
    "                references=load_reference_text(assistant_role)[0]  # Pass only the list of references\n",
    "        ),\n",
    "        inputs=gr.State(assistant_role),\n",
    "        outputs=evaluate_output\n",
    "    )\n",
    "\n",
    "\n",
    "    # Initialize the file list output with the current files loaded\n",
    "    file_list_output.value = \", \".join(file_list)\n",
    "\n",
    "# Launch the Gradio interface in queued mode to handle multiple inputs\n",
    "demo.queue().launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06895b-0a07-4a89-921b-f4e85d2253e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b2826-d067-4f06-adfc-d4801c85f71d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Human Texts: ['I think the offer is acceptable, and I would like to proceed.', 'The terms of this negotiation are fair and meet my expectations.']\n",
      "TEST: Model Texts: ['The offer seems reasonable. I am happy to move forward.', 'These terms are acceptable and align with my goals.']\n",
      "DEBUG: Number of human texts: 2\n",
      "DEBUG: Number of model texts: 2\n",
      "DEBUG: Sample human texts: ['I think the offer is acceptable, and I would like to proceed.', 'The terms of this negotiation are fair and meet my expectations.']\n",
      "DEBUG: Sample model texts: ['The offer seems reasonable. I am happy to move forward.', 'These terms are acceptable and align with my goals.']\n",
      "DEBUG: Using device ID -1 for MAUVE calculation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce6e4a1ec5c44228a1e6fd8a266424a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5619fd96d694377be1e90aa85d86be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_mauve():\n",
    "    # Define test inputs\n",
    "    human_texts = [\n",
    "        \"I think the offer is acceptable, and I would like to proceed.\",\n",
    "        \"The terms of this negotiation are fair and meet my expectations.\",\n",
    "    ]\n",
    "    model_texts = [\n",
    "        \"The offer seems reasonable. I am happy to move forward.\",\n",
    "        \"These terms are acceptable and align with my goals.\",\n",
    "    ]\n",
    "\n",
    "    # Print the test inputs\n",
    "    print(\"TEST: Human Texts:\", human_texts)\n",
    "    print(\"TEST: Model Texts:\", model_texts)\n",
    "\n",
    "    # Calculate MAUVE score\n",
    "    try:\n",
    "        mauve_score = calculate_mauve_score(human_texts, model_texts)\n",
    "        print(\"TEST: MAUVE Score:\", mauve_score)\n",
    "    except Exception as e:\n",
    "        print(f\"TEST: Error during MAUVE calculation: {e}\")\n",
    "\n",
    "# Run the test\n",
    "test_mauve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b86b9644-fa60-479f-818a-94c064ace440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a10da859b89496fbfdb256fa722ddfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380d80f7e9ef44ce8a09416d4e240357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy MAUVE Score: 0.27811372536724027\n"
     ]
    }
   ],
   "source": [
    "dummy_references = [\"The cat sat on the mat.\", \"The quick brown fox jumps over the lazy dog.\"]\n",
    "dummy_outputs = [\"A cat was sitting on a mat.\", \"A fox quickly jumped over a dog.\"]\n",
    "\n",
    "mauve_result = mauve.compute_mauve(p_text=dummy_outputs, q_text=dummy_references, device_id=0)\n",
    "print(f\"Dummy MAUVE Score: {mauve_result.mauve}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2d39d9-cce2-4f7e-9305-4cfefdac0db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Human Texts for MAUVE Calculation: ['The job offer is generous and aligns with industry standards.']\n",
      "DEBUG: Model Texts for MAUVE Calculation: ['We believe this offer is competitive and fair for the role.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a960663159504b6191b5896c47fa6b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7780cba9488941f5b1d11018660d96ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: MAUVE Result: 0.0040720962619612555\n",
      "MAUVE Test Result: 0.0040720962619612555\n"
     ]
    }
   ],
   "source": [
    "human_texts = [\"The job offer is generous and aligns with industry standards.\"]\n",
    "model_texts = [\"We believe this offer is competitive and fair for the role.\"]\n",
    "print(\"MAUVE Test Result:\", calculate_mauve_score(human_texts, model_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1db2d64e-5ff4-488a-a5fb-9329f5ffbb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10206fff555e4e62988e55ffe7229796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing p:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e291bccb63b49eaafae1b07220564c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featurizing q:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAUVE Score: 0.9675196225919289\n"
     ]
    }
   ],
   "source": [
    "import mauve\n",
    "\n",
    "# Sample human texts (from reference)\n",
    "human_texts = [\n",
    "    \"I accept the offer.\",\n",
    "    \"We have an agreement.\",\n",
    "    \"That works for me.\",\n",
    "    \"Deal accepted.\",\n",
    "    \"I am happy to proceed with these terms.\",\n",
    "    # ... add more varied sentences\n",
    "]\n",
    "\n",
    "# Sample model texts (from assistant)\n",
    "model_texts = [\n",
    "    \"I agree with the final terms.\",\n",
    "    \"Let's finalize this.\",\n",
    "    \"I accept the offer.\",\n",
    "    \"We have an agreement.\",\n",
    "    \"That works for me.\",\n",
    "    # ... add more varied sentences\n",
    "]\n",
    "\n",
    "# Calculate MAUVE score\n",
    "try:\n",
    "    mauve_result = mauve.compute_mauve(\n",
    "        p_text=model_texts,\n",
    "        q_text=human_texts,\n",
    "        device_id=0  # Adjust based on your GPU availability\n",
    "    )\n",
    "    print(f\"MAUVE Score: {mauve_result.mauve}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating MAUVE: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727fa52c-ee58-4fd5-8572-3592cc083185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
